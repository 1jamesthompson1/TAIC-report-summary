---
title: TAIC Report Sumarizer
subtitle: Final Report for 159333 Massey University
date: 24/10/2023
author: James Thompson (21011195) - 1jamesthompson1@gmail.com
toc: "true"
documentclass: report
bibliography: 159333_Final_report
geometry: margin=3cm
---

<!--
You need to run these two lines in cli to render this document.

cd C:\Users\James\OneDrive\Documents\Notes\Massey Uni BsC\2023 (third year)\Semester 2\159333\Final Report
pandoc --citeproc --bibliography 159333_Final_report.bib -F mermaid-filter.cmd -o 21011195_159333_Final_Report.pdf -s 21011195_159333_Final_Report.md
-->
# Introduction

As this is my last project I will do for my Bachelor Degree I wanted to do something hard, novel and interesting. It was these ideas that lead me to do this project. The Transport Accident Investigation Commission (TAIC) is a Crown funded organisation who's sole purpose is to do "blame free" investigations into transport accidents across Marine, Rail and Aviation. TAIC conducts about 15 investigation a year at a cost of about $450,000 each. All of these report are publicly available online [@InvestigationsTAIC]. This mass of information and knowledge is supposed to go out to the industry and regulators and help prevent the same sort of accident from happening again. The process of taking insight form the accidents and giving it to the industry is something that TAIC had identified and has recently invested in upgrading their Knowledge Transfer System.

I identified that this was somewhere that AI could play a big role. My idea involed using LLMs "agents" to read the reports and help summarize them in a way to make the insights and learning not only more accessible but also comparable. The goal is not just to summarize a single report but help understand the trends and insights that can only be found from all of that publication combined.

With such a vague goal for this project there was quite a simple mission for the project "The goal is to create a proof of concept that can be used to jump start further work in this area" [@jamesthompsonTAICReportSummarizer2023]. Turned into more practical features it was to have an engine that uses gpt 3.5 to summarize and extract the theme present in each accident report. This has been achieved by creating theme summaries for each report and then weightings for each report over the global themes. Another addition to the goal was added about half way through and that is a web app to give a user friendly way to search the outputs. One way or another these goals were meet with a bit of change along the way.



# What I did

## Plan (What I wanted to do)

Given the scope of my idea I had to narrow it down quite a lot to make it feasible for a single semester project. I laid out my initial plan in my project proposal [@jamesthompsonTAICReportSummarizer2023]. The project more or less stayed within the same intent of the proposals. As work begun more items were added to the "todo list". These were:  
- A web app that would allow a user to easily view and search the output from the engine.  
This was added to allow the project to have a obvious application value and allow effective demonstration to non technical people.
- Validation  

There however were a few things that never happened and was cut from the final project:  
- Adding in more variable extraction from the reports in the breadth phase. This was removed due to time constraints and other additions were going to be more useful.  
- No time was spent looking at training/fine-tuning our own model.  
- The final two example reports were not made as they no longer were needed. This was because presentation to higher up stakeholders was not going to be done until the project was in a complete final stage.  



## Execution (How I did it)

### Tools

#### Engine

As I working on a prototype I wanted to use a language and framework with minimal overhead to allow me to get some up and running as quickly as possible. In particular I didn't need really need performance in the engine it self. As the openAI api only have an official package for JS and Python [@OpenAIPlatform] it was limited to these two languages. I choose Python due to its ease of use, scalability and industry de facto standard status. As work began I was thankful of my choice in my language due to pythons rich eco system of packages. This relates to one of my [lessons](#eco-systems-are-important). 

#### Viewer

When the requirement of a web app was added I needed to decide on what tooling to use for it. As I had experience with R and Rmd, I was interested in using Shiny [@Shiny]. I started using this locally however I realised quite quickly that working within Shiny's framework was too restrictive. I believe that Shiny would excel at creating dashboards and such, yet was I was trying to create had more custom features that didn't fit into Shiny's framework. Rather than find a hacky solution I looked elsewhere. I came up with flask [@WelcomeFlaskFlask]. This was minimal enough yet flexible so that I could add the custom things I wanted. To deploy the app I used Heroku [@HerokuDevCenter] as it was simple and easy to use compared to AWS.

#### Dev tools
As I was dealing with many packages I wanted to make it as portable as possible so I used Poetry [@PoetryPythonDependency] as my dependency manger. This is because from my experience in Rust and using Crate [@CratesIoRust] I wanted to have a more powerful project manager than pip [@developersPipPyPARecommended]. Lastly I used R [@rcoreteamLanguageEnvironmentStatistical2023] and Rmd [@grolemundMarkdownDefinitiveGuidea] for the example reports. 


### Components

Before talking about the components it will be best to visualize the structure of the project as it is.

~~~mermaid
graph LR
    subgraph Python
		direction TB
        subgraph Engine
            A((Gather and Wrangle))
            B((Extract and Analyze))
            F((Verify))
        end

        subgraph openAI API
            D((ChatCompletion))
        end


        subgraph Flask
            E((Viewer App))
        end
    end

	G((Validation set))

    subgraph R/Rmd
        C((Report Making))
    end

	
    A -->|clean text files| B
    B <-->|summarising| D
    B -->|data table| C
    Engine --->|output folder| E
    B --> |output folder| F
    F --> |validation| G
    F <--> |comparing| D
~~~

This should be thought of less as a dependency graph but just a general project structure.

#### Engine

The engine is what this project is about. As seen in the graph above I have separated it out into three distinct folders. This was to help keep the ideas separate. You can see that some modules are still imported from other folders and there are a few general modules that are used.

The output of it is effectively a cli program. It is run from CLI and outputs both to the console and to an output file. There are four functions that the CLI can do. These are download, themes, summarize and verify. The first 3 of these run in chronological order and together complete the main engine "function". Which is getting all the PDF reports from the website and then analysing and summarising them.

Furthermore this project has alot of scaffolding which is explained fully here. However the novetly of this project is to be found in the ___ and ____ modules

##### Gather and Wrangle

The main goal of this section is to simply download all the reports from [@InvestigationsTAIC] and prepare them into text reports. This required two distinct steps. Downloading and PDF text extraction. 

###### Downloading
As there is no API available on the TAIC website I needed to use more traditional web scraping techniques. Due to it being such a common task in python there is a library called bs4 [@richardsonBeautifulsoup4ScreenscrapingLibrary]. This made the implementation of downloading a trivial task without any hiccups

###### PDF text extraction

A PDF file is something that can be unreasonably complicated to extract the text from. This is due to a few reasons. As described in this article [@stavrakisExtractingTextPDF2023] PDFs can be separated into three categories, Programmatically generated, Image, and OCR images. Luckily we are dealing with firsxt type which is "which are all searchable and easy to edit". Due to this and pyPDF [@fenniakPypdfPurepythonPDF] it was a trivial implementation. 

The only unusual formatting that my module would do is that of adding structure around the page numbers. This helped later modules actually extract the pages they need so that they can read the reports for their various purposes.

##### Extract and Analyse

This is where much of the work has been spent. It is this section that does the heavy lifting of novel programming work. As there are a few modules in this section I wont be providing a break down of all of them just the two important ones. These are Summarizer and ThemeGenerator.

###### ThemeGenerator

**What**

This module does as it says on the packet it generates themes that it finds in the collection of reports. At the onset it was not on the list of features in the engine. The original plan was to use human generated themes and then do the summarising on that. However I believed that the theme generation was actually something of great value and gave the engine alot more use and freedom. This is due to human generated themes would have the biases present meaning any summary across the themes will be greatly influenced by what the human biases are. Generating everything from the reports directly means that the output of the engine is completely clean of *a priori* biases.

The output of the theme generation step will be 3-6 themes for each report and 3-6 themes that cover all the themes present in the reports. It are these global themes that are passed onto the next step of summarizing.

Here is an example of what the themes might look like after a run through

>themes:
>
>  - title: Human Factors and Safety Management
>
>    description: "This theme encompasses various human factors and safety management issues that contributed to the accidents. These include lack of awareness, distraction, inadequate training, poor communication, and non-compliance with procedures. The theme underscores the importance of addressing human error and behavior, enhancing safety culture and leadership, and implementing effective risk management practices."
>
>
>  
>
>  - title: Equipment and Maintenance Issues
>
>    description: "This theme focuses on the equipment and maintenance issues that played a role in the accidents. These include mechanical failures, inadequate maintenance practices, and deficiencies in equipment design. The theme emphasizes the need for proper equipment inspection, maintenance, and adherence to manufacturer's specifications to prevent failures and ensure safe operations."
>
>  
>
>  - title: Environmental Conditions and Hazards
>
>    description: "This theme highlights the impact of environmental conditions and hazards on the accidents. These include adverse weather conditions, challenging navigational environments, and the presence of hazards such as sand bars and rocks. The theme emphasizes the need for proper risk assessment, planning, and adherence to safety guidelines when operating in challenging environments."
>
>  
>
>  - title: Lack of Training and Familiarity
>
>    description: "This theme focuses on the lack of training and familiarity with procedures and equipment that contributed to the accidents. These include insufficient training on safety procedures, inadequate familiarization with equipment, and lack of understanding of risks and hazards. The theme highlights the importance of comprehensive training programs and ongoing education to ensure crew members are equipped with the necessary skills and knowledge for safe operations."
>
>  
>
>  - title: Inadequate Regulations and Oversight
>
>    description: "This theme addresses the inadequacies in regulations and oversight that contributed to the accidents. These include lack of clear regulations, inconsistencies in safety standards, and insufficient oversight of compliance. The theme emphasizes the need for robust regulations, standardized safety standards, and effective oversight to ensure the safety of marine operations."

*Note that there is still some variation in the output from the engine so this shouldn't be treated as the 'final' output.*

**How**

The end goal is to have a list of 3- 6 themes with names and descriptions of each theme. These themes should be inclusive of all the general undertones present in the reports.

Ideally we could just take all the reports through them at GPT and it responds back with the general themes it thinks it have found. However this is not possible because of the context limit in the gpt 3.5 and 4.0. Each report has about 6.5 thousand tokens. This means that to read all the reports at the same time would take about 330,000 tokens. With the current limits being 16 thousand and 8 thousand respectively [@Pricing] it is not possible to do it this way. Instead each report is summarised with this prompt:

>I am trying to find the themes present across a collection of about 50 marine accident investigation reports.<br><br>For this, I need your help by reading this report and telling me the 3-6 themes/causes that are present in the report.<br><br>Your response should have a short paragraph (<= 30 words) for each theme/cause. With an empty line in between each paragraph.<br><br>Note that I want the output of this process to be consistent and repeatable. This means that I want your response to be as deterministic as possible.

Now each report will have its own themes. These themes can be collated together and sent back to gpt 3.5 with this prompt:

>I am trying to find the themes present across a collection of about 50 marine accident investigation reports.<br><br>I will give you 3-6 themes/causes summaries for each report. <br><br>Please read all of the summaries and provide 5-10 themes/causes that best cover all of the individual themes present.<br><br>Your output should have a title and description paragraph (<= 50 words) for each general theme/cause discovered.<br><br>Note that I want the output of this process to be consistent and repeatable. This means that I want your response to be as deterministic as possible.

This gives me the final output that I require. Furthermore it actually provides me theme summaries of each report which after discussion with the stakeholders might be quite valuable.

**Challenges**

Because we are dealing with LLMs non determinism is of great concern. A few techniques have been employed to try help the process to actually come up with consistent results.  Below is a graph to show what it looks likes:

~~~mermaid
graph TB

report1 -->|gpt3.5 individual summarising| report1_theme_summary
report2 -->|gpt3.5 individual summarising| report2_theme_summary
report... -->|gpt3.5 individual summarising| report..._theme_summary
reportN -->|gpt3.5 individual summarising| reportN_theme_summary

report1_theme_summary --> collection[All theme summaries added togather]
report2_theme_summary --> collection
report..._theme_summary --> collection
reportN_theme_summary --> collection

collection -->|gpt 3.5 summarise to 3-6 themes| themes1
collection -->|gpt 3.5 summarise to 3-6 themes| themes2
collection -->|gpt 3.5 summarise to 3-6 themes| themes3
collection -->|gpt 3.5 summarise to 3-6 themes| themes4

themes1 -->|Adding togather| general_themes[General themes collection]
themes2 -->|Adding togather| general_themes[General themes collection]
themes3 -->|Adding togather| general_themes[General themes collection]
themes4 -->|Adding togather| general_themes[General themes collection]

general_themes -->|gpt 4.0 summarise 4 answers into 1| final[Final output of 3 - 6 themes]
~~~

Firstly was hyper parameter tuning. This involved taking the temperature and lowering it to 0. This as per the documentation the temperature will be "between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic." [@OpenAIPlatform].

Secondly was doing some prompt engineering. I played around a little bit with various wordings and lengths. What I have settled with is by no means optimal however it did it as good as was needed. At this point it was doing a good enough job however there was still significant enough variation between runs of the theme generation I.e number of themes and theme substance.

To help resolve this I tried taking the "average" of multiple responses. This meant using the same prompt as above that takes the collection of all the reports individual summaries and rather than getting one response get multiple. Due to the nature of GPT 3.5 each of these responses are equally valid and "right" however there is just variation. Now I could use GPT 4.0 to summarise these multiple responses into one. To do this I used this prompt:
>I am trying to find the themes present across a collection of about 50 marine accident investigation reports.<br><br>I have three summaries of all of the themes.<br><br>I wanted you to take the average of all of the summaries.<br><br>Your output should have a title and description of each theme/cause.<br><br>Note that I want this to be reproducible and deterministic as possible.

The results turned out quite nicely. It reduced the variation to almost nothing. Through multiple run through it produced the same number of themes with broadly the same number of the themes. However sadly I didn't develop a robust measuring tool so I am not sure how effective and/or how stable the output really is.

###### Summarizer (Weightings generating)

**What**

This module was original the only section of the engine that used the openAI api and was really the "brains" of the project. However issue 14 [@jamesthompsonAutoGenerateThemes2023] which was resolved early on in the project the Summarizer was no longer the only brains in the project.

The goal of this section is to actually create weightings for each report for all of the themes. These are designed to help understand how much each global theme contributed to the accident.
This means that each report gets and output string that looks something like this:
> 2020_205,[16  17  18  19  20  21  22  23  24],30.0,10.0,10.0,30.0,20.0,nan,nan,nan,nan,nan,true,N/A

Now most of this is not very useful to use at first glance. However "[16  17  18  19  20  21  22  23  24]" is stating which pages were read as part of the *important text*. Next is  "30.0,10.0,10.0,30.0,20.0" which are the the weightings for the five theme found above (This is all from the same output so the number of themes could change). Lastly is a bunch of error and logging information that is not important in this discussion.

All of these output lines are collated together in a csv which is then can be used for statistical analysis and the viewer web app.

**How**

In comparison to the ThemeGeneration the summarizing of weights is actually quite simple. It works by reading reading each of the reports one by one. For each reports it sends off the *important text* to gpt 3.5 with the prompt:

>Please read this report and determine what themes had the most contribution. Can you please provide a paragraph for each theme with how much you think it contributed to the accident? You should provide percentages for each of the {number_of_themes} themes, with all the percentages adding up to 100.<br><br>Here is a summary of the {number_of_themes} themes:<br>{self.theme_reader.get_theme_description_str()}<br><br>---<br>Note that I want this to be repeatable and deterministic as possible.

Note that every thing inside curly braces are replaced with actually values at runtime. With the `get_theme_description_str()` returning a somewhat formatted string made up of what was seen above.

**Challenges**

Like with the theme generation there were similar challenges with constancy and convergence of output. As it has already been spoken about above I want explain the hyper parameter tuning and prompt engineering steps again. However the last step taken was a bit different in this case. This is because of the numeric nature in the responses.

Because of the numeric nature of the weightings I could get multiple responses and take the average of them. This is what I first tried to do and it proved to be mostly effective. However when coming back and working on making the weightings more consitant I found that it was not needed as the repeated outputs were identical. This is quite plausible due to the output in this case being only a handle of numbers rather than a few paragraphs (citation needed). 


##### Verify

It became apparent that the engine would be producing output themes and weightings which were at least at face value reasonable. This meant for it to be any use we would need something to validate that it is actually coming up with something that is close what a human. As discussed in hte lessons I should really of started developing a good test set and framework before I started work.

**How**

To do the verifying I opted to use a traditional statistics technique of having a validation set. This meant I would need to have the human answers to compare to the engine output and see how close they are. Ideally for the validation set it would be from a subject expert. However due to time and resource constraints it was just an average human (me) who created the validation set.

For the weightings it is relatively simple to compare the weightings from the engine and the validation set. I did this using a Manhattan distance [@TaxicabGeometry2023] which allows me to say the statement "each weighting is on average x percent away from the validation set.".

Theme validation is a bit more complicated due to needing to compare two bodies of text. Gpt 4.0 is quite sufficient at comparing two bodies of text for semantic matching (citation needed). This meant that using similar techniques of stabilizing output as for the engine output I could get a text comparer.

**Challenges**

The weightings comparison is deterministic which is what you want for the validation. However the theme validation is not as much. As it is using gpt 4 to compare the text one would have to test it to make sure that it similarity percentages it produces are accurate. However due to times constraints this has not happened.

#### Viewer

##### Why

After a somewhat unsuccessful demonstration to Robert (TAIC stakeholder) it was decided that having a user interface for the engine was going to be quite important [@jamesthompsonApplicationExampleSearch2023]. This web app initial vision was to simply give a more user friendly way to understand and search the output of the engine. Which makes the apps main function searching.

##### What

As discussed in the tools sections the web app is a flask [@WelcomeFlaskFlask] app and deployed with Heroku [@HerokuDevCenter]. The web app can be viewed live at (https://taic-viewer-72e8675c1c03.herokuapp.com)[https://taic-viewer-72e8675c1c03.herokuapp.com/] (note that this url will likely change if this project continues see github for up to date information). There is currently a lack of UI design however it is all functional as it should be

Here are some pictures showing what the app looks like

>Home page
![](images/webapp_homepage.png)

*Note that as mentioned the UI design is non-existent so the home page is mostly blank*

>Search result

![](images/webapp_search_result.png)

#### How

After initially starting the viewer in a separate repository I decided to treat it simply as a front end and remerged the repo quickly. Instead I created a new package in the project directory. This was the `viewer` package which contained all the html templates, js, css and  flask server code.

**search**
This module is the most important of them all and is where the heart of the server side code is. It is worth noting that this module has a few dpendcy on modules in the engine package. This close coupling makes the viewer less portable by does allow alot of code reuse. There are three classes with the `searcher` class performing the search.

Like with the engine modules, when dealing with the output the `OutputFolderReader` is used. Each report is searched one by one. All of the reports have their theme summary (engine generated) and complete report text searched. To increase the yield of a search a "fuzzification" method is used. This involves using nlt words corpus and adding all the semantically similar words to the search query. In theory this should mean that when you search for a word like "fatigue" it will also show reports that mention being "tired" or "exhausted". The usefulness of the fuzzification has yet to be properly tested and would likely involve user feedback. 

The actual `serach_report` function uses regex statements to find all of the search queries and highlight them yellow. The highlighting is done by adding in and html `span` tag in. Which when displayed by the viewer makes the text highlighted.


## Results (What I actually did)

For brevity sake I will only discuss the engine and its validation. The two main parts of the engine are the report themes and the report weightings. The validation set was made by me which means we are comparing to an average humans interpretation. It is worth noting I have no idea whether having an expert level validation set would be closer too or further from the engines output. Therefore I can only say how close it is to a humans answer but cannot say that it is better or worse than a human. Furthermore due to the extensive time it takes a human to process a report only 4 were completed.

### Theme generation

The validation of the theme generation involves comparing each reports theme summary with the validation set. Here is the table the validation results.

|Report ID|Percentage|
|---|---|
|2015_203|67%|
|2017_201|90%|
|2018_206|70%|
|2021_203|70%|

We see that there that there is an average of $75$%. Interpretation of this is quite simple as it translates to about $\frac{3}{4}$ of the listed themes for each report match up. The problem comes that we don't have any benchmark to know if this is very good or not. In some ways this is quite successful as the themes are vastly the same. However on the other hand because the global theme generation is a bit of a black box (i.e uses a LLM) I am not really sure how the difference is compounded. For instance it could be plausible that it acts something like probability, thus the global themes will have the chance of being the same as the validation set global like $\text{averageDifference}^{\text{numberReports}}$. So in our case with about 50 reports we would need a average individual similarity score greater than $99$% for our global themes to be at least $60$% similar to validation.

This complication and expanse of unknown is why there is considerable further work to be done in the validation section alone. However as this is not part of the primary goal it was not prioritzed for exploration.



### Weighting generation

The validation of the weightings are much simpler. The validation set was created with a parituclar set of global themes that has to match the global themes used by the engine when doing the weightings.

|Report ID|Weightings Manhatten distance|Pages read Jaccard Similarity|
|---|---|---|
|2015_203|140.0|1.0|
|2017_201|50.0|1.0|
|2018_206|50.0|1.0|
|2021_203|70.0|1.0|

Firstly we shall disregard the pages read. As the weightings generation only reads the "important" parts it was important to check that the important parts was considered the same. However as this was only reading the content page it is not surprising that it is perfectly accurate.

Second is the weightings Manhattan distance. It creates an average distance per report of $77.5$. This works out to $15.5$ percent difference in each themes weighting. Once again just as with the theme generation there is little to compare to. On one side the difference between $10$ and $25$ is quite substantial where as $70$ to $85$ does not feel as pronounced. This leads to the point that quite possibly there is a better metric out there beside Manhattan distance. Particularly one that will take into account the relative difference of these percentages. However my knowledge in this realm is limited and so more research and learning for me would have to be done.

# User Manual

As the previous section explains the inner workings of the system I will only give instruction on how to setup and use the project in your local environment.

## Engine

Use of the engine at the present state requires running the source code directly. For the sake brevity I will assume a basic competency in terminal use and python development. More instruction can be found on the homepage of the Github repository [@TAICreportsummaryREADMEMd].

### Setup

There are 4 things you need to run the engine:

- Python 3.11 or greater  
- Poetry [@PoetryPythonDependency]  
- Stable internet connection (This is used for the downloading of the report PDFs)
- openAI api key
- Git (Used to get a copy of the source code)

To setup you need to get the source code and set your open AI api key.

Start by navigating to the directory where you want the code to be and running `git clone https://github.com/1jamesthompson1/TAIC-report-summary.git`

Now create a file called `.env` in the root of your project directory. This is going to have your openAI key in it.
Add your key to the file like so:  
>OPENAI_API_KEY=sk-xWHey2gE3BLskFJzx33cDzRbdR66hDxAhum5voMmETL8yZwy

### Using

To use the engine you are going to run a command that looks like this

`poetry run engine -t *type*`
The type is going to be one of 5 values, download, themes, summarize, all, validate. The first 3 are the steps the engine runs as discussed above. The all command will simply do the three previous commands one after another. Lastly the validate will run the validation.

Each of these command will produce console logs to keep you up to date with what it is doing. It will use the `config.yaml` to find the output folder location and strucutre. By default it will create an `output` folder with all of the reports, themes, and weightings.

There are a few other functions available with more information being found out by running.

`poetry run engine -h`

The viewer can also be run locally by using `poetry run viewer`. However this requires you to have a local 

## Viewer

Unlike the engine using the Viewer will only require a broswer. Open up the webapp by going to the url (https://taic-viewer-72e8675c1c03.herokuapp.com)[https://taic-viewer-72e8675c1c03.herokuapp.com/].

### Searching

The apps main feature is searching. This searching feature has a few settings as seen below:

![](images/webapp_settings.png)

This settings menu can be brought up by pressing on the search settings button. The three check boxes are by default ticked. These simply narrow down the search space. The simple search search turns off the "fuzzification" step.

Be default when the search query is made it will do a "fuzzy" search.

By simply typing in your search and pressing the "show reports" button you will be able to see the results.

### Viewing results

The results of your search are displayed in a table like manner. Currently there are 5 regular columns are then a column for each global theme.

![](images/webapp_columns.png)

*Note that the 5 theme columns are from this run though and you might have a different number and/or different themes*

**Report ID**
This is the ID given to the report from TAIC. It is always in the format year_2XY. There year is the publication year, 2 stands for it being a marine report and XY is the number of the report that year. There are usually about 5-10 marine reports a year.

**NoMatches**
This is quite simply the number of times your search query matched something in either the theme summary or the report text. If you have changed the setting to not include the theme summary or the report text then it obviously wont include those.

*Important* if you click on the blue link number it will bring up a copy of the report text with all the matches highlighted yellow. There is no way to skip to the next match but the yellow with the scroll bar should make it easy enough to find the match locations.

**ThemeSummary**
This is the theme summary that is generated by the engine in the `ThemeGenerator` module.

**ErrorMessage**
Whenever the engine runs into a problem in the theme generation or weightings step it will give an error. If there is an error then it will be listed here. It is useful to see why for example a report was not properly evaluated by the engine.

**ThemeWeightings**
These columns (5 in the example picture) are the weightings for this report. It should be interpreted as "this theme contributed XY% towards the accident". All of the weightings will sum to 100.

**PDF**
It is badly named but this is a link to the webpage for the report on the TAIC website. This webpage includes information about the accident as well as the PDF report itself.

# Conclusion

Goals listed in the initial proposal included creating "a proof of concept that can be used to jumpstart further work in this area" [@jamesthompsonTAICReportSummarizer2023].  In a broad sense the project was success.

In particular it was a success in creating a base to "jumpstart further work" this means that [Future Work](#future-work) discuses a lot of avenue for further work. The complete basic pipeline as been made from gathering all the reports, summarizing them, validation as well as viewing the results. Thus any further work will either be improving the engine or working on application and usability.

As seen by looking at the validation results in [results](#results) we have similarity to an average human benchmark that at face values looks reasonable. This has shown that it is theoretically possible to use off the shelf pre trained LLMs models to complete the sort of summarizing and interpretation that TAIC are looking for. Thus the proof of concept can be used to demonstrate the potential and acquire support for the further work that can be made.

The engine would be on stronger ground if it had had better validation throughout the development cycle. Regardless it does produce an output with theme summaries for each report, global themes and global theme weightings for each report. Ontop of this there is a web app viewer that can be used to search up reports and find the results.

I have thoroughly enjoyed this project. It has been tough but exciting working in an area without any guard rails or guiding lights. As this project feels uncomplete I am very eager to carry on this project and bring it to a more satisfying completion. Furthermore this has given me hope for enjoyment of further research and development in computer science and related fields as being something I would enjoy.

# Future Work

This project although successful in many way is still far from complete. Below are various avenues for further development that should be fruitful in both furthering this project and expanding into other areas. There is not any particular order of avenues.

## Viewer

The viewer is at the point of a MVP for the prototype. In other words it is in a very early stage. There are a couple of things that would need to be improved to make it acceptable for a presentation / demonstration.

### Visuals

Due to time constraints the webapp has no styling whatsoever. This means that it is mostly just raw html which to any user looks unfinished. A theme should be introduced throughout. Ideally it would follow that which is present throughout TAIC. Addition of formatting to meet standard website expectations like a menu bar up top and footer with information below.

### UI/UX

The functionality of the web app is still quite simple. However the user experience is still lacking. It would be good to have more sophisticated and user friendly screen that will give people just the right amount of information they need to make a search.

### Search

Currently the search is quite a simple matching exercise. Two things need to be improved for it to be better. Firstly the fuzzification needs to be better so that a user can easily search for the ideas that they want to be. Secondly is a better results finder. currently there is no easy was to look at the matches in the reports and you have to manually try to find the highlights.

## Engine broadening

Currently the engine only extracts the themes and weightings for each report.

### Quote extraction

Any publication of opinion from TAIC has to be signed off from the commission. This means that all of the report summaries and weightings are useless from an official perspective. However one can make "safe" summaries by using quotes as evidence for what is being said. This would make the summarises usable and "publishable" to a certain extant. The area of using references in LLMs is an ongoing challenge as it has a habit of making things up [@HallucinationArtificialIntelligence2023].

### More variables per report
The engine could also extract more factual information about the accident for example boat size or number of fatalities. This would make the statistics analysis done on the output more powerful and allow the searching of reports to be a lot easier. However TAIC is internally working on a case management software (CMS) upgrade which would make this sort of extraction pointless. Instead once could simple join the dataset together by report id as the CMS would have all the varaibles natively

## Text comparison

It was noted when creating the `verify` module that for it to be able to compare the validation set to the output we need to compare text. The solution used up until this point seems to work but is broadly untested. Further investigation could develop and create a package that would use gpt 4 to compare text. In that developing there would be the creation of a validation to make sure that it actually does a good enough job of creating a similarity percentage. Furthermore it would be beneficial to have more traditional NLP techniques to provide similarity metrics for contrast to the gpt 4 generated ones.


# Personal Learning

There were a few lessons to be learned both in the hard and soft skills sections [@HardSoftSkills].
## Hard

### Python and Poetry

At the start of this project I had never really used Python for a real project. This meant that I needed to learn the workflows and standards present in python development. The first important tool I learnt to use was Poetry [@PoetryPythonDependency]. It is by no means the industry standard but from the Rust experience I wanted to find something that was most like Cargo which is Rust's Package manager. Learning to use this has been useful and I have already put these skills to use in other projects.

### Deployment and web apps

This project was my first time is my first time deploying a web application. I have made various website but nothing with much functionality. This was a good experience in learning the basics of not just Flask but also how deployment can be done with Heroku and a Github repo. The actual solution that I came up with is quite simple but figuring it out differently took some trial and error and learning about the python build process.

## Soft

### Don't forget the basics

At the outset of this project I was very much unaware on how to solve this problem. So to make it achievable I started with the bits I new and worked from there. This involved programming the various structures required to solve this problem. The programming of PDf downloading, looping through folders etc. It meant that I forgot/avoided some of the basics like CI and proper testing. Overall I forgot some of the basics in not just software development but also statistics. The lack of validation early on is mentioned further [here](#what-you-can't-measure-you-can't-improve)

### Eco systems are important

There was some thought given to the language that I would use for the project. As mentioned I choose Python based on it beign the defacto standard in this area. However one thing I did not think about properly is the eco system that surrounds the language. Thankfully I was quite lucky with Python having a great eco system. But if I had chosen Rust for example I might of struggled the whole way through trying to do simple tasks which in Python are easy with the use of established packages. IN the engine along I ended up having many dependencies. If any other language didn't have a  single one of these it would of made it all that much harder to make progress in this project.


### What you can't measure you can't improve
This follows on from the lessons in [Dont forget the basics](#don't-forget-the-basics). But right throughout the development cycle I failed to recognize the importance of automated validation. As I am dealing with LLMs which are really quite unstable it would of bee beneficial to start off creating the validation set. Then whenever I would make changes the the prompt and such I would be able to see if it made it closer or further from the validation set. This would introduce a risk of overfitting to the validation set however at least this is an known risk.

# Bibliography
:::::{.references}
:::::