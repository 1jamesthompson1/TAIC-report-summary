{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "This notebook will be where the work for #165 will take place.\n",
    "\n",
    "As per the issues there are three steps, getting embedding dataset, creating the vector database and then building the RAg on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine\n",
    "from engine.OpenAICaller import openAICaller\n",
    "from viewer import Searching\n",
    "\n",
    "# Third Party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "import openai\n",
    "\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "\n",
    "import voyageai\n",
    "\n",
    "import yaml\n",
    "\n",
    "# built in\n",
    "import os\n",
    "from typing import Callable\n",
    "from importlib import reload\n",
    "\n",
    "reload(Searching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting embeddings ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding(file_name):\n",
    "\n",
    "    embedding_folder = 'embeddings'\n",
    "\n",
    "    return pd.read_pickle(os.path.join(embedding_folder, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_issue_embeddings = read_embedding('voyageai_si_embeddings.pkl')\n",
    "safety_issue_embeddings.rename(columns={'si_embedding': 'vector'}, inplace=True)\n",
    "display(safety_issue_embeddings)\n",
    "print(f\"Number of unique reports: {len(set(safety_issue_embeddings['report_id']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking safety issues distrubitions\n",
    "\n",
    "I have done a rerun of the safety issues and included all since 2000. This means I have alot more safety issues. I need to do a recheck to see if the safety issue extraction is good enough. I think it is quite variable as there seems to be 660ish where the last run through only had 560. More so I didn't think I was doing a run through where it would redo the safety issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_issue_embeddings['year'] = safety_issue_embeddings['report_id'].apply(lambda x: int(x[0:4]))\n",
    "\n",
    "safety_issue_embeddings['year'].hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_text_embeddings = read_embedding('voyageai_important_text_embeddings.pkl')\n",
    "\n",
    "important_text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_text_embeddings = read_embedding('voyageai_section_embeddings.pkl')\n",
    "\n",
    "print(f\"Number of unique reports: {len(set(section_text_embeddings['report_id']))}\")\n",
    "section_text_embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_searches = yaml.safe_load(open('data/evaluation_searches.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 'databases/safety_issue_rag-lancedb'\n",
    "# uri = 'az://vectordb/lancedb'\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = voyageai.Client()\n",
    "\n",
    "def embed_query(text):\n",
    "\n",
    "    return vo.embed(text, model=\"voyage-large-2-instruct\", input_type=\"query\", truncation=False).embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_search(table, query, limit = 100, type: str = ['hybrid', 'fts', 'vector']) -> pd.DataFrame:\n",
    "    if type == 'hybrid':\n",
    "        results = table.search((embed_query(query), query),  query_type='hybrid') \\\n",
    "            .metric(\"cosine\") \\\n",
    "            .limit(limit) \\\n",
    "            .to_pandas()\n",
    "        results.rename(columns={'_relevance_score': 'section_relevance_score'}, inplace = True)\n",
    "    elif type == 'fts':\n",
    "        results = table.search(query,  query_type='fts') \\\n",
    "            .limit(limit) \\\n",
    "            .to_pandas()\n",
    "        results.rename(columns={'score': 'section_relevance_score'}, inplace = True)\n",
    "    else: # type == 'vector'\n",
    "        results = table.search(embed_query(query),  query_type='vector') \\\n",
    "            .metric(\"cosine\") \\\n",
    "            .limit(limit) \\\n",
    "            .to_pandas()\n",
    "        results.rename(columns={'_distance': 'section_relevance_score'}, inplace = True)\n",
    "        results['section_relevance_score'] = 1 - results['section_relevance_score']\n",
    "\n",
    "    results['section_relevance_score'] = (results['section_relevance_score'] - results['section_relevance_score'].min()) / (results['section_relevance_score'].max() - results['section_relevance_score'].min())\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic safety issues\n",
    "\n",
    "I am noticing a problem of it not really getting the safety issues which are relevant. One big thing is that it isnt even finding the exact same safety issue! This is a bit of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_table = db.create_table('safety_issue_embeddings', data = safety_issue_embeddings, mode='overwrite')\n",
    "\n",
    "si_table\n",
    "si_table.create_fts_index(\"si\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_issue_search(query, limit = 100):\n",
    "    simple_search = si_table.search(embed_query(query)) \\\n",
    "        .metric(\"cosine\") \\\n",
    "        .where(\"year >= 2006 \", prefilter=True) \\\n",
    "        .limit(limit) \\\n",
    "        .to_pandas()\n",
    "\n",
    "    return simple_search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Close proximity events at unattended aerodromes\"\n",
    "\n",
    "safety_issue_search(query).loc[0, 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking of Safety issue search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query, results, n_results = 10):\n",
    "    reranked_results = pd.DataFrame(vo.rerank(query, results['si'].tolist(), model = \"rerank-1\", truncation = False, top_k=n_results).results)\n",
    "\n",
    "    merged_df = reranked_results.merge(results, left_on='document', right_on='si')[['report_id', 'si', 'index', 'relevance_score', 'vector', 'year', '_distance']]\n",
    "    merged_df.rename(columns={'index': 'previous_rank'}, inplace=True)\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Close proximity incidents at unattended aerodromes\"\n",
    "\n",
    "results = safety_issue_search(query)\n",
    "rerank_results(query, results, n_results = 20).query('report_id == \"2008_001\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding search of reports text to help find relevant safety issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_sections_table = db.create_table('report_section_embeddings', data = section_text_embeddings.rename(columns={'section_text_embedding': 'vector'}), mode='overwrite')\n",
    "print('Making fts index')\n",
    "report_sections_table.create_fts_index('section_text', replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_sections_search = lambda query, limit = 100, type = 'vector': table_search(query = query, table = report_sections_table, limit = limit, type = type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[0, 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Vortex ring state (or 'settling with power')\"\n",
    "\n",
    "results = report_sections_search(query, limit = 50000, type = 'vector')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reports_relevance(query):\n",
    "    results = report_sections_search(query, limit = 50000, type = 'fts')\n",
    "    results.sort_values(by='section_relevance_score', ascending=False, inplace=True)\n",
    "    return results.groupby('report_id').head(50).groupby('report_id')['section_relevance_score'].mean().sort_values(ascending=False).to_dict()\n",
    "\n",
    "\n",
    "get_reports_relevance(test_searches[0]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_issue_search_with_report_relevance(query):\n",
    "    safety_issues = safety_issue_search(query, limit = 500)\n",
    "\n",
    "    reports_relevance = get_reports_relevance(query)\n",
    "\n",
    "    safety_issues['_distance'] = safety_issues.apply(lambda row: row['_distance'] * (1 -reports_relevance[row['report_id']] if row['report_id'] in reports_relevance else 1), axis = 1)\n",
    "\n",
    "    safety_issues.sort_values(by = '_distance', inplace=True)\n",
    "\n",
    "    safety_issues.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    return safety_issues\n",
    "\n",
    "safety_issue_search_with_report_relevance(test_searches[2]['query']).query('report_id == \"2015_201\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchEngine.db.open_table('safety_issue_embeddings').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Searching)\n",
    "searchEngine = Searching.SearchEngine('../../viewer/vector_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchEngineSearch = lambda query: searchEngine.search(\n",
    "        Search(query,\n",
    "               SearchSettings(Modes.all_modes, (2000, 2024))),\n",
    "        with_rag = False\n",
    "    ).getContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchEngineSearch(test_searches[0]['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database search evaluation\n",
    "\n",
    "There is a need to know how well a search is performing.\n",
    "I can do this by having some exmaples of a search query, report etc and what we would expect to see in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(results: pd.DataFrame, relevant_reports: list, at = 20):\n",
    "    '''\n",
    "    Calculates the Normalized discounted cummulative gain.\n",
    "    Arugments\n",
    "    results - this should be a dataframe of all of the safety issues. The rank of the reports will be extracted from the first occurane of it in `report_id` column\n",
    "    relevant_reports - A list of all of the relevant report ID. This is treated as binary\n",
    "    at - The number of results to consider\n",
    "    '''\n",
    "    # display(relevant_reports)\n",
    "    reports_rank = list(enumerate(results['report_id'].unique()))[:100]\n",
    "\n",
    "    # display(reports_rank)\n",
    "\n",
    "    reports_relevance = [(at/2 if (report_id in relevant_reports) else 0) for _, report_id in reports_rank]\n",
    "\n",
    "    # display(reports_relevance)\n",
    "    \n",
    "    DCG = [(pow(2,relevance) - 1) / np.log2(rank+1) for rank, relevance in zip(range(1, len(reports_relevance)+1), reports_relevance)]\n",
    "    # display(DCG)\n",
    "    DCG = sum(DCG)\n",
    "\n",
    "    IDCG = [(pow(2,(at/2))- 1)  / np.log2(rank+1) for rank in range(1, len(reports_rank)+1)]\n",
    "    # display(IDCG)\n",
    "    IDCG = sum(IDCG)\n",
    "    # print(DCG, IDCG)\n",
    "    return DCG / IDCG\n",
    "\n",
    "def evaluate_search(search: dict, search_function: Callable[[str], pd.DataFrame], loss_function: Callable[[pd.DataFrame, list, int], float], valid_size = 20, verbose = False) -> float:\n",
    "\n",
    "    search_results = search_function(search['query'])\n",
    "    \n",
    "    if not 'report_id' in search_results.columns:\n",
    "        raise ValueError(\"Search results must have a 'report_id' column\")\n",
    "\n",
    "    expected_report_ids = set(search['expected_reports'])\n",
    "    search_report_ids = set(search_results['report_id'].head(valid_size))\n",
    "\n",
    "    score = loss_function(search_results, expected_report_ids, valid_size)\n",
    "\n",
    "    percrent_present_reports = len(expected_report_ids.intersection(search_report_ids)) / len(expected_report_ids)\n",
    "    if verbose:\n",
    "        print(f\"  Percentage of expected reports present in search results: {percrent_present_reports} with score: {score}\")\n",
    "        if percrent_present_reports != 1.0:\n",
    "            misisng_reports = expected_report_ids.difference(search_report_ids)\n",
    "            print(f\"  Missing reports: {misisng_reports}\")\n",
    "            print(f\"  These are at index {[search_results.report_id.ne(report_id).idxmin() for report_id in misisng_reports]}\")\n",
    "        \n",
    "        display(search_results)\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_searches(searches, search_function, verbose=False):\n",
    "    percents = []\n",
    "    for i, search in enumerate(searches):\n",
    "        if verbose:\n",
    "            print(f\"{i} Evaluating search: '{search['query']}'\")\n",
    "        percents.append(evaluate_search(search, search_function, loss_function=NDCG, verbose = verbose))\n",
    "\n",
    "    return sum(percents) / len(percents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_functions(test_searches, search_functions):\n",
    "\n",
    "   searches = []\n",
    "\n",
    "   for search_function in (pbar := tqdm(search_functions)):\n",
    "      pbar.set_description(f\"Evaluating {search_function['name']}\")\n",
    "      searches.append({\n",
    "         'search_function': search_function['name'],\n",
    "         'score': evaluate_searches(test_searches, search_function['function'], verbose = False)\n",
    "      })\n",
    "   \n",
    "   return pd.DataFrame(searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Searching)\n",
    "search_functions = [\n",
    "    {\n",
    "        'name': 'reranked_search',\n",
    "        'function': lambda query: rerank_results(query, safety_issue_search(query, limit = 1000), n_results = 500)\n",
    "    },\n",
    "    {\n",
    "        'name': 'report_relevance_search',\n",
    "        'function': safety_issue_search_with_report_relevance\n",
    "    },\n",
    "    {\n",
    "        'name': 'search_engine',\n",
    "        'function': searchEngineSearch\n",
    "    }\n",
    "]\n",
    "\n",
    "different_models = evaluate_search_functions(test_searches, search_functions)\n",
    "\n",
    "# Get the best model\n",
    "best_search_function_name = different_models.sort_values('score', ascending = False).head(1)['search_function'].values[0]\n",
    "\n",
    "best_search_function = [search_function for search_function in search_functions if search_function['name'] == best_search_function_name][0]['function']\n",
    "\n",
    "different_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_of_report(query, report_id, search_function):\n",
    "    search_results = search_function(query)\n",
    "    display(search_results.query('report_id == @report_id'))\n",
    "    just_reports = search_results.drop_duplicates(subset = ['report_id']).reset_index(drop=True)\n",
    "    display(just_reports.query('report_id == @report_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rank_of_report(test_searches[1]['query'], '2014_005', best_search_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each evaluation highlightes these problems:\n",
    "\n",
    "- Missing older reports that might be significant as reports only go back as far as 2000\n",
    "- Missing reports that dont mention the search query in the safety safety_issue_embeddings\n",
    "- Relevant reports can get lost in the search (i.e with 2015_201 being at place 25).\n",
    "\n",
    "The first issue will be ignored for now due to not being able to reasonably get text extraction.\n",
    "The second issue can be fixed by adding more context to the search by including the whole report for a full text search of sorts.\n",
    "The last issue is ranking issue as 25 is still pretty close to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding RAG ontop of the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = lambda query, context: f\"\"\"\n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "My question is: {query}\n",
    "\n",
    "Here are relevant safety issues as context:\n",
    "{context}\n",
    "\n",
    "It is important to provide references to specifc reports and safety issues in your answer.\n",
    "\"\"\"\n",
    "\n",
    "def rag_search(query, search_function: Callable[[str], pd.DataFrame]):\n",
    "\n",
    "    print((f\"Understanding query...\"))\n",
    "\n",
    "    formatted_query = openAICaller.query(\n",
    "        system = \"\"\"\n",
    "You are a helpful agent inside a RAG system.\n",
    "\n",
    "You will recieve a query from the user and return a query that should be sent to a vector database.\n",
    "\n",
    "The database will search a dataset of safety issues from transport accident investigation reports.  It will use both embeddings and full text search.\n",
    "\"\"\",\n",
    "        user = query,\n",
    "        model=\"gpt-4\",\n",
    "        temp = 0.0\n",
    "    )\n",
    "    print(f' Going to run query: \"{formatted_query}\"')\n",
    "\n",
    "    print(f\"Getting relevant safety issues...\")\n",
    "    \n",
    "    search_results = search_function(formatted_query).head(50)\n",
    "    with pd.option_context('display.max_rows', 20):\n",
    "        display(search_results)\n",
    "\n",
    "    user_message = \"\\n\".join(f\"{id} from report {report} with relevance {rel} - {si}\" for id, report, si, rel in zip(search_results['safety_issue_id'], search_results['report_id'], search_results['si'], search_results['_distance'])) \n",
    "\n",
    "    print(f\"Summarizing relevant safety issues...\")\n",
    "    response = openAICaller.query(\n",
    "        system = \"\"\"\n",
    "You are a helpful AI that is part of a RAG system. You are going to help answer questions about transport accident investigations.\n",
    "\n",
    "The questions are from investigators and researchers from the Transport Accident Investigation Commission. The context you will be given are safety issues extracted from all of TAICs reports.\n",
    "\n",
    "A couple of useful defintions for you are:\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "• can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "• is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.  \n",
    "\"\"\",       \n",
    "    user=rag_prompt(query, user_message),\n",
    "    model=\"gpt-4\",\n",
    "    temp = 0.2\n",
    "    )\n",
    "    return {\n",
    "        'relevant_safety_issues': search_results,\n",
    "        'response': response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_searches[1]['query']\n",
    "\n",
    "results = rag_search(query, best_search_function)\n",
    "\n",
    "view = results['relevant_safety_issues']\n",
    "\n",
    "print(results['response'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
