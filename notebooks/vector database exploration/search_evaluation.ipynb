{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "As was originally worked on in #146 the vector database is being used alot moving forward.\n",
    "\n",
    "This notebook will be a easy way to evaluate the performance of the vector database.\n",
    "\n",
    "In the the two otehr nmotebooks `#165_basic_safety_issue_rag.ipynb` and `vector_db_exploration.ipynb` I developed some of this evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai.client\n",
    "import yaml\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lancedb\n",
    "import voyageai\n",
    "import importlib\n",
    "import dotenv\n",
    "\n",
    "import engine.analyze.Embedding as Embedding\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "vo = voyageai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset\n",
    "\n",
    "Inside `evaluation_searches.yaml` there is a collection of searches to test the retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = yaml.safe_load(open('evaluation_searches.yaml'))\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation functions\n",
    "\n",
    "Using what seems to be the industry norm for retrieval I am going to uses the Normalized discounted cummulative gain (NDCG) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(results: pd.DataFrame, relevant_reports: list, at = 20):\n",
    "    '''\n",
    "    Calculates the Normalized discounted cummulative gain.\n",
    "    Arugments\n",
    "    results - this should be a dataframe of all of the safety issues. The rank of the reports will be extracted from the first occurane of it in `report_id` column\n",
    "    relevant_reports - A list of all of the relevant report ID. This is treated as binary\n",
    "    at - The number of results to consider\n",
    "    '''\n",
    "    # display(relevant_reports)\n",
    "    reports_rank = list(enumerate(results['report_id'].unique()))[:100]\n",
    "\n",
    "    # display(reports_rank)\n",
    "\n",
    "    reports_relevance = [(at/2 if (report_id in relevant_reports) else 0) for _, report_id in reports_rank]\n",
    "\n",
    "    # display(reports_relevance)\n",
    "    \n",
    "    DCG = [(pow(2,relevance) - 1) / np.log2(rank+1) for rank, relevance in zip(range(1, len(reports_relevance)+1), reports_relevance)]\n",
    "    # display(DCG)\n",
    "    DCG = sum(DCG)\n",
    "\n",
    "    IDCG = [(pow(2,(at/2))- 1)  / np.log2(rank+1) for rank in range(1, len(reports_rank)+1)]\n",
    "    # display(IDCG)\n",
    "    IDCG = sum(IDCG)\n",
    "    # print(DCG, IDCG)\n",
    "    return DCG / IDCG\n",
    "\n",
    "def evaluate_search(search: dict, search_function: Callable[[str, dict], pd.DataFrame], loss_function: Callable[[pd.DataFrame, list, int], float], valid_size = 20, verbose = False) -> float:\n",
    "\n",
    "    search_results = search_function(search['query'], search[\"settings\"])\n",
    "    if search_results is None:\n",
    "        print(\"No results found therefore score is 0\")\n",
    "        return 0, 0, None\n",
    "    expected_report_ids = set(search['expected_reports'])\n",
    "    search_report_ids = set(search_results['report_id'].head(valid_size))\n",
    "\n",
    "    score = loss_function(search_results, expected_report_ids, valid_size)\n",
    "\n",
    "    percent_present_reports = len(expected_report_ids.intersection(search_report_ids)) / len(expected_report_ids)\n",
    "    if verbose:\n",
    "        print(f\"  Percentage of expected reports present in search results: {percent_present_reports} with score: {score}\")\n",
    "        if percent_present_reports != 1.0:\n",
    "            misisng_reports = list(expected_report_ids.difference(search_report_ids))\n",
    "            print(f\"  Missing reports: {misisng_reports}\")\n",
    "            print(f\"  These are at index {[search_results.report_id.ne(report_id).idxmin() for report_id in misisng_reports]}\")\n",
    "        \n",
    "        display(search_results)\n",
    "    \n",
    "    return score, percent_present_reports, search_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_searches(searches, search_function, verbose=False):\n",
    "    at = 20\n",
    "    test_results = []\n",
    "    for i, search in enumerate(searches):\n",
    "        if verbose:\n",
    "            print(f\"{i} Evaluating search: '{search['query']}'\")\n",
    "        ndcg, percent_present, results = evaluate_search(search, search_function, loss_function=NDCG, valid_size=at, verbose = verbose)\n",
    "        test_results.append({\n",
    "            \"search_id\": i,\n",
    "            \"search\": search,\n",
    "            f\"ndcg@{at}\": ndcg,\n",
    "            f\"percent_present_@_{at}\": percent_present,\n",
    "            \"search_results\": results\n",
    "        })\n",
    "\n",
    "    test_results_df = pd.DataFrame(test_results)\n",
    "    \n",
    "    print(f\"Average NDCG@{at}: {test_results_df[f'ndcg@{at}'].mean()}, with average reports_present: {test_results_df[f'percent_present_@_{at}'].mean() * 100}%\")\n",
    "\n",
    "    return test_results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testings\n",
    "\n",
    "I am going to mainly compare between two different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import viewer.Searching as Searching\n",
    "\n",
    "def search_function(query: str, setting_dict, table, embedding_model) -> pd.DataFrame:\n",
    "    setting_dict = {f\"setting_{key}\": value for key, value in setting_dict.items()}\n",
    "    settings = Searching.SearchSettings.from_dict(setting_dict)\n",
    "    searcher = Searching.SearchEngineSearcher(\n",
    "        Searching.Search(query, settings),\n",
    "        table,\n",
    "        embedding_model\n",
    "    )\n",
    "\n",
    "    results = searcher.search()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect(\"vector_db\")\n",
    "\n",
    "db.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voyage-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyage_3_table = db.open_table(\"voyage-3\")\n",
    "voyage_3_table.count_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Searching)\n",
    "evaluate_searches(\n",
    "    test_set,\n",
    "    lambda query, settings: search_function(\n",
    "        query,\n",
    "        settings,\n",
    "        voyage_3_table,\n",
    "        \"voyage-3\"\n",
    "    ),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voyage-large-2-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyage_2_table = db.open_table(\"voyage-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_searches(\n",
    "    test_set,\n",
    "    lambda query, settings: search_function(\n",
    "        query,\n",
    "        settings,\n",
    "        voyage_2_table,\n",
    "        \"voyage-large-2-instruct\"\n",
    "    ),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reankers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "\n",
    "vo = voyageai.Client()\n",
    "\n",
    "def reranked_search(query, settings):\n",
    "\n",
    "    results = search_function(query, settings, voyage_2_table).head(1_000)\n",
    "\n",
    "    reranking = vo.rerank(query, results[\"document\"].tolist(), model = \"rerank-2\")\n",
    "\n",
    "    results[\"reranked_score\"] = [r.relevance_score for r in reranking.results]\n",
    "\n",
    "    results.sort_values(\"reranked_score\", inplace=True, ascending=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_searches(\n",
    "    test_set,\n",
    "    reranked_search,\n",
    "    verbose=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
