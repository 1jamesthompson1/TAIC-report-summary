{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "As discussed in #146 it seems that a searching problem is what this will turn into.\n",
    "After the creation of #163 this branch and notebook is the base for further work.\n",
    "\n",
    "Using a vector dba nd rag could be the best way for people to search through all of the reports.\n",
    "\n",
    "Currently **this** notebook has a few things:\n",
    "- Getting and preparing the datasets which will be used by other notebooks\n",
    "- A simple test of vector database and RAg (this is now deprecated)\n",
    "\n",
    "There are other notebooks in this folder which are:\n",
    "- [#165_basic_safety_issue_rag.ipynb](#165_basic_safety_issue_rag.ipynb) which is to answer #165 and does waht hte title suggests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from me\n",
    "from engine.OpenAICaller import openAICaller\n",
    "from engine.Extract_Analyze import ReportExtracting\n",
    "import engine.Modes as Modes\n",
    "\n",
    "# third party\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import lancedb\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import voyageai\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# built in\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import random\n",
    "import regex as re\n",
    "\n",
    "load_dotenv('../../.env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data\n",
    "\n",
    "The initial idea would be to have all of the important text inside the vetor database. This means that the searching could happen with most of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '../../output'\n",
    "reports = [dir for dir in os.listdir(output_folder) if os.path.isdir(os.path.join(output_folder, dir))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text\n",
    "\n",
    "Since I am going to want to look at all aspects of the report this means that I need to get all of the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_text = []\n",
    "\n",
    "for report in reports:\n",
    "\n",
    "    report_text_path = os.path.join(output_folder, report, f'{report}.txt')\n",
    "    \n",
    "    if not os.path.exists(report_text_path):\n",
    "        continue\n",
    "\n",
    "    with open(report_text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if text == None:\n",
    "        continue\n",
    "\n",
    "    report_text.append({\n",
    "        'report_id': report,\n",
    "        'text': text\n",
    "    })\n",
    "\n",
    "report_text_df = pd.DataFrame(report_text)\n",
    "\n",
    "report_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all important text\n",
    "\n",
    "with open('../../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "important_text_extractor = ReportExtracting.ReportExtractingProcessor(output_folder, config['engine']['output']['reports'])\n",
    "\n",
    "important_texts = []\n",
    "\n",
    "for report in reports:\n",
    "\n",
    "    text = important_text_extractor.get_important_text(report, False)\n",
    "\n",
    "    if text == None:\n",
    "        continue\n",
    "\n",
    "    important_texts.append({\n",
    "        'report_id': report,\n",
    "        'important_text': text\n",
    "    })\n",
    "\n",
    "important_texts_df = pd.DataFrame(important_texts)\n",
    "\n",
    "important_texts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_texts_df['gpt-3.5_tokens'] = important_texts_df['important_text'].apply(lambda x: openAICaller.get_tokens([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_texts_df.hist(column='gpt-3.5_tokens')\n",
    "\n",
    "# Check what token cutoff is reasonable\n",
    "\n",
    "max_tokens = 16_000/1.2\n",
    "\n",
    "print(f'{important_texts_df[important_texts_df[\"gpt-3.5_tokens\"] < max_tokens].shape[0] / important_texts_df.shape[0] * 100:.2f}% are under {max_tokens:.2f} tokens. With a total sum of {important_texts_df[\"gpt-3.5_tokens\"].sum()} tokens.')\n",
    "\n",
    "important_texts_df[important_texts_df['gpt-3.5_tokens'] > max_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_issues_df = pd.read_csv('data/safety_issues.csv')[['report_id', 'safety_issue']].rename({'safety_issue': 'si'}, axis = 1)\n",
    "safety_issues_df['si'] = safety_issues_df['si'].apply(lambda x: x.strip())\n",
    "\n",
    "safety_issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_df = pd.read_pickle('data/reports_extracted_sections.pkl')\n",
    "all_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_df.query('report_id == \"2020_103\"').loc[111, 'extracted_sections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting document\n",
    "\n",
    "I think something quite usueful about the vector databases will be not just finding the reports but also the specific report sections. For this I want to get a dataset that has the report represented as a strucutred document with sections and such. Then I could embed each of these and have much more fine-grained searching.\n",
    "\n",
    "I am going to ask TAIC directly to see if they have a dataset of word documents. I have heard back and it sounds like they wont have that dataset available. I will haveto work with the report extractor and see how well that goes. think it shouldn't be too complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ReportExtracting)\n",
    "\n",
    "report_text_df['content_section'] = report_text_df.apply(lambda row: ReportExtracting.ReportExtractor(row['text'], row['report_id']).extract_contents_section(), axis=1)\n",
    "\n",
    "report_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_section():\n",
    "    sections = list(map(str, range(1,15)))\n",
    "\n",
    "    subsections = [\n",
    "        [\n",
    "            section + '.' + str(subsection)\n",
    "            for subsection in\n",
    "            range(1,100)\n",
    "        ]\n",
    "        for section in \n",
    "        sections\n",
    "    ]\n",
    "\n",
    "    paragraphs = [\n",
    "        [\n",
    "            [\n",
    "                subsection + '.' + str(paragraph)\n",
    "                for paragraph in\n",
    "                range(1,100)\n",
    "            ]\n",
    "            for subsection in\n",
    "            section\n",
    "        ]\n",
    "        for section in \n",
    "        subsections\n",
    "    ]\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ReportExtracting)\n",
    "\n",
    "all_potential_sections = get_potential_section()\n",
    "\n",
    "def extract_sections(num_sections, report_text, debug = False):\n",
    "    get_parts_regex = r'(((\\d{1,2}).\\d{1,2}).\\d{1,2})'\n",
    "    \n",
    "    extreactor = ReportExtracting.ReportSectionExtractor(report_text, num_sections)\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    for section in all_potential_sections:\n",
    "        if debug: print(f\"Looking at section {re.search(get_parts_regex, section[0][0]).group(3)}\")\n",
    "\n",
    "        subsection_missing_count = 0\n",
    "        for sub_section in section:\n",
    "            sub_section_str = re.search(get_parts_regex, sub_section[0]).group(2)\n",
    "            if debug: print(f\" Looking at subsection {sub_section_str}\")\n",
    "\n",
    "            paragraph_missing_count = 0\n",
    "\n",
    "            paragraphs = []\n",
    "            for paragraph in sub_section:\n",
    "                if debug: print(f\"  Looking for paragraph {paragraph}\")\n",
    "\n",
    "                paragraph_text = extreactor.extract_section(paragraph, useLLM = False)\n",
    "\n",
    "                if paragraph_text is None and (paragraph_missing_count > 0 or paragraph[-1] == '1'):\n",
    "                    break\n",
    "                elif paragraph_text is None:\n",
    "                    paragraph_missing_count += 1\n",
    "                    continue\n",
    "\n",
    "                paragraphs.append({'section': paragraph, 'section_text': paragraph_text})\n",
    "\n",
    "            if len(paragraphs) == 0:\n",
    "                if debug: print(f\" No paragraphs found \")\n",
    "                sub_section_text = extreactor.extract_section(sub_section_str, useLLM = False)\n",
    "\n",
    "                if sub_section_text is None and subsection_missing_count > 0:\n",
    "                    if debug: print(f\" No subsection found\")\n",
    "                    break\n",
    "                elif sub_section_text is None:\n",
    "                    subsection_missing_count += 1\n",
    "                    continue\n",
    "\n",
    "                sections.append({'section': sub_section_str, 'section_text': sub_section_text})\n",
    "            else:\n",
    "                sections.extend(paragraphs)\n",
    "\n",
    "    df = pd.DataFrame(sections)\n",
    "\n",
    "    if debug and df.empty: print(f\"No sections extracted\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Parallel apply function\n",
    "def parallel_apply(df, func, num_sections, debug=False):\n",
    "    with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "        futures = [\n",
    "            executor.submit(func, num_sections, row['text'], debug) for index, row in df.iterrows()\n",
    "        ]\n",
    "        results = []\n",
    "        for future in tqdm(futures, total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n",
    "test_df = report_text_df.sample(30, random_state=42)\n",
    "\n",
    "report_text_df['extracted_sections'] = parallel_apply(report_text_df, extract_sections, 15, False)\n",
    "\n",
    "report_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the calculation of how many missing sections there are.\n",
    "\n",
    "For example if it has found 4.3 and 4.5 but not 4.4 then that counts as a missing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_sections(df):\n",
    "\n",
    "    if df.empty: return []\n",
    "\n",
    "    # Parse sections into a list of tuples of integers\n",
    "    sections = df['section'].apply(lambda x: tuple(map(int, x.split('.')))).tolist()\n",
    "    \n",
    "    missing_sections = []\n",
    "\n",
    "    def section_to_str(section_tuple):\n",
    "        return '.'.join(map(str, section_tuple))\n",
    "    \n",
    "    # Iterate through the sections to find missing sections\n",
    "    for i in range(len(sections) - 1):\n",
    "        current = sections[i]\n",
    "        next_section = sections[i + 1]\n",
    "        \n",
    "        # If the current section and next section are in the same main section\n",
    "        if current[0] == next_section[0]:\n",
    "            # Check for gaps in the subsection\n",
    "            for sub_section in range(current[1] + 1, next_section[1]):\n",
    "                missing_sections.append(f\"{current[0]}.{sub_section}\")\n",
    "        else:\n",
    "            # Handle the case where we move to the next main section\n",
    "            # Add missing subsections in the current main section\n",
    "            for sub_section in range(current[1] + 1, 10):  # Assuming subsections go up to .9\n",
    "                missing_sections.append(f\"{current[0]}.{sub_section}\")\n",
    "            \n",
    "            # Add all subsections for the main sections between current and next\n",
    "            for main_section in range(current[0] + 1, next_section[0]):\n",
    "                for sub_section in range(1, 10):  # Assuming subsections go up to .9\n",
    "                    missing_sections.append(f\"{main_section}.{sub_section}\")\n",
    "            \n",
    "            # Add missing subsections in the next main section up to the found subsection\n",
    "            for sub_section in range(1, next_section[1]):\n",
    "                missing_sections.append(f\"{next_section[0]}.{sub_section}\")\n",
    "\n",
    "    # Filter out the missing sections that do not have both previous and next sections\n",
    "    filtered_missing_sections = []\n",
    "    for section in missing_sections:\n",
    "        sec_tuple = tuple(map(int, section.split('.')))\n",
    "        prev_section = (sec_tuple[0], sec_tuple[1] - 1) if sec_tuple[1] > 1 else (sec_tuple[0] - 1, 9)\n",
    "        next_section = (sec_tuple[0], sec_tuple[1] + 1) if sec_tuple[1] < 9 else (sec_tuple[0] + 1, 1)\n",
    "        \n",
    "        if section_to_str(prev_section) in df['section'].values and section_to_str(next_section) in df['section'].values:\n",
    "            filtered_missing_sections.append(section)\n",
    "\n",
    "    return filtered_missing_sections\n",
    "\n",
    "dev_df = report_text_df.loc[0, 'extracted_sections']\n",
    "\n",
    "report_text_df['missing_sections'] = report_text_df['extracted_sections'].apply(lambda df: find_missing_sections(df))\n",
    "\n",
    "report_text_df['percent_missing'] = report_text_df.apply(lambda row: len(row['missing_sections']) / (len(row['extracted_sections']) + len(row['missing_sections'])) if not row['extracted_sections'].empty else 0, axis = 1)\n",
    "\n",
    "print(f\"There are {(report_text_df['percent_missing'] > 0).sum() } number of reports with missing sections\")\n",
    "print(f\"On average {report_text_df['percent_missing'].replace(0, np.nan).mean()*100:.2f}% of sections are missing when a report is missing sections\")\n",
    "print(f\"Total number of missing sections {sum(report_text_df['missing_sections'].apply(len))} which is {sum(report_text_df['missing_sections'].apply(len)) / sum(report_text_df['extracted_sections'].apply(len)) *100:.2f}% of total sections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is here for manually inspecting the process of section extraction.\n",
    "\n",
    "\n",
    "reload(ReportExtracting)\n",
    "\n",
    "report_id = '2014_004'\n",
    "\n",
    "with open(f'/home/james/code/TAIC-report-summary/output/{report_id}/{report_id}.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "sectionExtractor = ReportExtracting.ReportSectionExtractor(text, report_id)\n",
    "\n",
    "section_str = '4.2.5'\n",
    "\n",
    "print(sectionExtractor._get_section_start_end_regexs(section_str)[1][2])\n",
    "\n",
    "search_bounds = sectionExtractor._get_section_search_bounds(section_str,sectionExtractor._get_section_start_end_regexs(section_str)[1])\n",
    "display(text[search_bounds[0]:search_bounds[1]])\n",
    "\n",
    "sectionExtractor.extract_section(section_str, useLLM= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extraction is entierly implemented in the `ReportExtracting` modeule. I will list some of the problems that have been identified:\n",
    "\n",
    "| problem | examples |\n",
    "| --- | ---- | \n",
    "| Missing the ending of the section and getting multiple sections | 2013_107 and section 3.3 has all of the 3.4 and it is missing 3.4. |\n",
    "| References to paragraphs in previous paragraphs | 2014_005 4.6.11. |\n",
    "| Final paragraph with following appendices | 2011_203 with 8.2. It doesn't know when the end of the section is. |\n",
    "| Missing page in PDF conversion and thus missing many subsequent section, subsections etc. | 2019_204 3.18, 2019_204 2.10, 2018_001 3.16 and 2.5, 2018_005 3.37 and 2.8 |\n",
    "| End of the reports, last section | 2018_001  8.2, 2010_201 8.4, 2011_203 8.2|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge important text and sections togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_df = report_text_df.merge(important_texts_df, on='report_id', how='inner')\n",
    "\n",
    "for index, row in all_reports_df.iterrows():\n",
    "    if row['extracted_sections'].empty:\n",
    "        print(f\"Deleting report {row['report_id']}\")\n",
    "        all_reports_df.drop(index, inplace=True)\n",
    "\n",
    "all_reports_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "all_reports_df.to_pickle('data/reports_extracted_sections.pkl')\n",
    "\n",
    "all_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sections_long_df = pd.concat(all_reports_df.apply(lambda x: x['extracted_sections'].assign(report_id=x['report_id']), axis=1).tolist(), ignore_index=True)\n",
    "\n",
    "all_sections_long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sections_long_df.query('report_id == \"2020_103\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(df, embedding_function, document_column_name, embedding_column_name, batch_size=100, max_workers=10, debug = False):\n",
    "    \"\"\"\n",
    "    Given a dataframe with atleast the document column and embedding column name it will generate embeddins for all of the documents that dont have embeddings.pkl\n",
    "    It does this by calling the embedding_function on batches of the documents.\n",
    "    There is multithreading to speed up the process.\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe\n",
    "        embedding_function: The function that will be called to generate embeddings for documents. It must be`f([embedding]) -> [embedding]`\n",
    "        document_column_name: The name of the column that contains the documents\n",
    "        embedding_column_name: The name of the column that will contain the embeddings\n",
    "    Returns the dataframe with the missing embeddings filled in.\n",
    "    \"\"\"\n",
    "\n",
    "    # check which new columns needs to be computed, i.e \n",
    "\n",
    "    missing_embeddings = df[df[embedding_column_name].isna()]\n",
    "\n",
    "    if len(missing_embeddings) == 0:\n",
    "        if debug: print(f\"No missing embeddings\")\n",
    "        return df\n",
    "\n",
    "    section_texts = missing_embeddings[document_column_name].tolist()\n",
    "\n",
    "    print(f\"There are {len(section_texts)} missing embeddings with {len(df)} number of documents\")\n",
    "    \n",
    "    # Split section_texts into batches of size `batch_size`\n",
    "    batches = [section_texts[i:i+batch_size] for i in range(0, len(section_texts), batch_size)]\n",
    "\n",
    "    if debug: \n",
    "        for batch in batches: print(len(batch))\n",
    "\n",
    "    embeddings = [None] * len(missing_embeddings)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(embedding_function, batch): i for i, batch in enumerate(batches)}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            batch_embeddings = future.result()\n",
    "            batch_index = futures[future]\n",
    "            if debug:\n",
    "                print(f'Looking at batch {batch_index}, with return length {len(batch_embeddings)}')\n",
    "            # Place embeddings in the correct positions\n",
    "            start_index = batch_index * batch_size\n",
    "            end_index = start_index + len(batch_embeddings)\n",
    "            if debug: print(f\"Setting embeddings {start_index} to {end_index}\")\n",
    "            embeddings[start_index:end_index] = batch_embeddings\n",
    "\n",
    "    embeddings = pd.Series(embeddings, index=missing_embeddings.index) \n",
    "    # Update the dataframe with the computed embeddings\n",
    "    df_embedding_column = df.loc[missing_embeddings.index, embedding_column_name]\n",
    "    if debug:\n",
    "        display(df_embedding_column)\n",
    "        print(f'Finished embedding the documents there are {len(embeddings)} embeddings, which look like:')\n",
    "        display(embeddings)\n",
    "        print(f'Each embedding is in {len(embeddings[missing_embeddings.index[0]])} dimensions')\n",
    "\n",
    "    df.loc[missing_embeddings.index, embedding_column_name] = embeddings \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sentence transformers\n",
    "\n",
    "I have started looking at this but using sentence transformers won't work locally with models of a large enough input size to ahndle these large documents.\n",
    "\n",
    "There are two options.\n",
    "\n",
    "Firstly using hugging faces dedicated inference API which has been done for the safety theme genration.  \n",
    "Secondly splitting the document into smaller parts so that only specific sections are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sections_sentence_transformers(df, model):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    embeddings = model.encode(df['section_text'], show_progress_bar=True)\n",
    "\n",
    "    list_of_embeddings = list(embeddings)\n",
    "\n",
    "    return df.assign(extracted_section_embeddings = list_of_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alibaba-NLP/gte-large-en-v1.5\n",
    "\n",
    "This has been choosen as it has the highest retrival score on MTEB. However as they use custom code it was too difficult to get setup so it will be put on hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_df['extracted_sections'] = all_reports_df['extracted_sections'].progress_apply(embed_sections_sentence_transformers, args = (model,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using voyageai\n",
    "\n",
    "Voyage AI seems to have the largest embedding model that can take the input up to 16,000 tokens. This is much better than offered by openai embeddings models. With this extra size it allows me to vector search the whole document and not have to split it up into chunks. Alternatively I could start experimenting with Anthropic and their model offerings.\n",
    "\n",
    "However I have an idea of splitting the documents up into chunks and using it to provide more accurate and usual information to the LLM prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = voyageai.Client(max_retries = 10)\n",
    "@retry(wait=wait_random_exponential(multiplier=1, min = 1, max=60))  \n",
    "def embed_with_backoff(**kwargs):\n",
    "    print('trying')\n",
    "    return vo.embed(**kwargs)\n",
    "\n",
    "\n",
    "def embed_batch(batch):\n",
    "    return vo.embed(texts = batch, model=\"voyage-large-2-instruct\", input_type=\"document\", truncation=False).embeddings\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('voyageai/voyage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings on long form dataframe of individual sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_embeddings_file_name = os.path.join('embeddings', 'voyageai_section_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sections_long_df['num_tokens'] = all_sections_long_df['section_text'].apply(lambda x: len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sections_long_df = all_sections_long_df.query('num_tokens < 15000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyageai_loaded = pd.read_pickle(section_embeddings_file_name)\n",
    "voyageai_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_section_embeddings = pd.merge(voyageai_loaded, all_sections_long_df, on=['report_id', 'section', 'section_text', 'num_tokens'], how='right')\n",
    "merged_section_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyageai_embeddings = embed_documents(merged_section_embeddings, embed_batch, 'section_text', 'section_text_embedding', batch_size=50, debug=False)\n",
    "\n",
    "voyageai_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyageai_embeddings['year'] = voyageai_embeddings['report_id'].apply(lambda x: int(x[0:4]))\n",
    "voyageai_embeddings['mode'] = voyageai_embeddings['report_id'].apply(lambda x: Modes.get_report_mode_from_id(x).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyageai_embeddings.to_pickle(section_embeddings_file_name)\n",
    "voyageai_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding of important texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_text_embedding_file_name = os.path.join('embeddings', 'voyageai_important_text_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    important_text_voyageai_embedding_loaded = pd.read_pickle(important_text_embedding_file_name)\n",
    "except:\n",
    "    important_text_voyageai_embedding_loaded = pd.DataFrame(columns=['report_id', 'important_text', 'important_text_embedding'])\n",
    "\n",
    "important_text_voyageai_embedding_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_important_text_embeddings = important_text_voyageai_embedding_loaded.merge(all_reports_df[['report_id', 'important_text']], on=['report_id','important_text'], how='right')\n",
    "merged_important_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_important_text_embeddings['important_text_token_length'] = merged_important_text_embeddings['important_text'].apply(lambda text: len(tokenizer.tokenize(text)))\n",
    "print(\"These are the rpeorts that will be truncated. I will wait for larger embedding models to aleviate this problem\")\n",
    "      \n",
    "display(merged_important_text_embeddings[merged_important_text_embeddings['important_text_token_length'] > 16000])\n",
    "\n",
    "print(f\"There are a total of {merged_important_text_embeddings['important_text_token_length'].sum()} token in these reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_important_text(important_text_batch):\n",
    "    return vo.embed(important_text_batch, model=\"voyage-large-2\", input_type=\"document\", truncation=True).embeddings\n",
    "\n",
    "max_tokens_per_batch = 120_000\n",
    "\n",
    "average_token_length = merged_important_text_embeddings['important_text_token_length'].mean()\n",
    "\n",
    "batch_size = int(max_tokens_per_batch // (average_token_length*1.5))\n",
    "\n",
    "print(f\"Batch size will be {batch_size}\")\n",
    "\n",
    "voyageai_embeddings_important_text = embed_documents(merged_important_text_embeddings, embed_important_text, 'important_text', 'important_text_embedding', batch_size, debug=True)\n",
    "voyageai_embeddings_important_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyageai_embeddings_important_text.to_pickle(important_text_embedding_file_name)\n",
    "voyageai_embeddings_important_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding safety issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file_path = 'embeddings/voyageai_si_embeddings.pkl'\n",
    "\n",
    "if os.path.exists(embeddings_file_path):\n",
    "    embeddings = pd.read_pickle(embeddings_file_path)\n",
    "else:\n",
    "    embeddings = pd.DataFrame(columns=['report_id', 'si', 'si_embedding'])\n",
    "\n",
    "display(embeddings)\n",
    "\n",
    "\n",
    "embeddings = embeddings.merge(safety_issues_df, on=['report_id', 'si'], how ='outer')\n",
    "embeddings.rename(columns={'vector': 'si_embedding'}, inplace=True)\n",
    "\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_documents(embeddings, embed_batch, 'si', 'si_embedding', debug = True)\n",
    "\n",
    "embeddings['mode'] = embeddings.report_id.apply(lambda x: Modes.get_report_mode_from_id(x).value)\n",
    "embeddings.to_pickle(embeddings_file_path)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector database\n",
    "\n",
    "I will now test out importing the embeddings into a vector database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting dataset ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three things that we need for this data set\n",
    "    - ids\n",
    "    - payload\n",
    "    - embeddings\n",
    "\n",
    "The ids will be converted from the report_ids\n",
    "\n",
    "payload will include all the data we are interested in:\n",
    "\n",
    "Embedding have been calculated above using voyageAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting embeddings and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_pickle('embeddings.pkl')\n",
    "\n",
    "embeddings['id'] = embeddings['report_id'].apply(lambda x: int(x.replace('_','')))\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting payload data\n",
    "\n",
    "I will do this by using the search function from the viewer to turn the whole output folder into a dataframe.\n",
    "\n",
    "Then I can join it in with the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am just going to load it in from using the webapp and export button.\n",
    "\n",
    "search_results = pd.read_csv('search_results.csv')\n",
    "\n",
    "payload_df = search_results[search_results.columns.drop(['NoMatches', 'linksVisual', 'ErrorMessage'])]\n",
    "\n",
    "payload_df.rename(columns={'ReportID': 'report_id'}, inplace=True)\n",
    "\n",
    "payload_df = payload_df.join(important_texts_df.set_index('report_id'), on='report_id')\n",
    "\n",
    "payload_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging payload with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(embeddings, payload_df, on='report_id', how='left')\n",
    "\n",
    "merged_df.rename(columns={'embedding': 'vector'}, inplace=True)\n",
    "\n",
    "merged_df['mode'] = merged_df['report_id'].apply(lambda x: Modes.Mode.as_char(Modes.get_report_mode_from_id(x)))\n",
    "merged_df['year'] = merged_df['report_id'].apply(lambda x: int(x[0:4]))\n",
    "\n",
    "merged_df.to_csv('merged_df.csv')\n",
    "\n",
    "merged_df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up lanceDB\n",
    "\n",
    "!poetry add lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create db\n",
    "\n",
    "uri = 'vector_db'\n",
    "\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new table\n",
    "tbl_name = 'taic-reports'\n",
    "\n",
    "if tbl_name in db.table_names(tbl_name):\n",
    "    db.drop_table(tbl_name)\n",
    "\n",
    "tbl = db.create_table(\n",
    "    tbl_name,\n",
    "    data = merged_df\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = voyageai.Client()\n",
    "\n",
    "def search_vectorDB(text, tbl, results = 10):\n",
    "    search_text = text\n",
    "\n",
    "    search_text_embeded = vo.embed(search_text, model=\"voyage-large-2\", input_type=\"document\").embeddings[0]\n",
    "\n",
    "    return tbl.search(search_text_embeded) \\\n",
    "    .limit(results) \\\n",
    "    .to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Example below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vectorDB('fire onboard a boat', tbl, results = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag\n",
    "\n",
    "To setup a rag I need to have a returned documents that are smaller than the context limit of 128k.\n",
    "\n",
    "Given each report is about 10 tokens this only allows for about 10 reports in the context. It might be better to have smaller chunks that then link to the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add --group dev langchain langchain-community langchainhub langchain-openai langchain_voyageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain.vectorstores import LanceDB\n",
    "# from langchain_community.document_loaders import DataFrameLoader\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_voyageai import VoyageAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "voyageai_embedder = VoyageAIEmbeddings(model=\"voyage-large-2\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_vector_store = merged_df[['report_id', 'CompleteThemeSummary', 'text']]\n",
    "\n",
    "df_for_vector_store['text'] = df_for_vector_store.apply(\n",
    "    lambda x: \n",
    "    f\"Report {x['report_id']} \\n\\n{x['text']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df_for_vector_store)\n",
    "docs = loader.load()\n",
    "\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = LanceDB.from_documents(\n",
    "    documents = docs,\n",
    "    embedding = voyageai_embedder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Fire onboard marine vessel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to know that it can atleast retrieve relevent reports. I think that this will involve and require more work so that specfic reports can be quiered. Then I can work on the rag and see how that goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchan_core.runnables import RunnableParallel\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain_with_source.invoke(\"What are some examples of safety issues that came from fire onboard a marine vessel?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
