{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "As discussed in https://github.com/1jamesthompson1/TAIC-report-summary/issues/130 there is a requirement for recommendations to be extracted and linked. Due to the discovering of a dataset the extraction is no longer needed and focus on linking can be made.\n",
    "\n",
    "## How to do it\n",
    "\n",
    "I have two datasets to work with.\n",
    "\n",
    "Firstly is the recommendation dataset from TAIC this dataset can be considered trustworthy and complete.\n",
    "Secondly I have the safety issue extracted from the reports. This has a problem that as it is from the engine it cannot be fully trusted. What I will do instead will start with just the ones extracted using regexes so that I know they are true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the modules needed\n",
    "\n",
    "To keep things as transparent as possible I will add all of the dependencies at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# From the engine\n",
    "import engine.Extract_Analyze.ReportExtracting as ReportExtracting\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "\n",
    "# Third party\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import yaml\n",
    "\n",
    "# Built in\n",
    "import os\n",
    "import re\n",
    "import importlib\n",
    "import textwrap\n",
    "from typing import Literal\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting datasets\n",
    "\n",
    "My goal here is to have a single dataset that has 3 columns.\n",
    "\n",
    "These are report_id, safety_issue and recommendation. *Note that there will be more columns but this is the idea of the three things conveyed by each row*\n",
    "\n",
    "This means that there will be a row for all of the safety issues in each report and for each safety issue row there will also be a row for each recommendation from the report. Therefore I will be making a very long dataset\n",
    "\n",
    "In the next section I will work on adding the fourth column of whether they are linked and from their I can easily filter out the nonexistant connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# There are a few reports that have problems for various reasons that are just not worth the effort to include. These will be excluded from this work\n",
    "\n",
    "reports_to_exclude = [\n",
    "    '2022_203', # stevedore joint investigation that doesn't follow usual format\n",
    "    '2022_202', # \"\"\n",
    "    '2023_010', # This is not a report and instead just a protection order\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAIC dataset\n",
    "\n",
    "This data set just comes from a xlsx files but needs to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "original_TAIC_recommendations_df = pd.read_excel('TAIC_recommendations_04_04_2024.xlsx')\n",
    "\n",
    "cleaned_TAIC_recommendations_df = original_TAIC_recommendations_df.copy()\n",
    "\n",
    "cleaned_TAIC_recommendations_df['recommendation_id'] = cleaned_TAIC_recommendations_df['Number']\n",
    "\n",
    "cleaned_TAIC_recommendations_df['recommendation_text'] = cleaned_TAIC_recommendations_df['Recommendation']\n",
    "\n",
    "cleaned_TAIC_recommendations_df.dropna(subset=['recommendation_text', 'Inquiry'], inplace = True)\n",
    "\n",
    "rows_deleted = len(original_TAIC_recommendations_df) - len(cleaned_TAIC_recommendations_df)\n",
    "print(f\"Deleting {rows_deleted} rows with NAs in either recommendation_text or Inquiry\")\n",
    "\n",
    "\n",
    "# find all rows that dont match regex on column 'Inquiry'\n",
    "inquiry_regex = r'^(((AO)|(MO)|(RO))-[12][09][987012]\\d-[012]\\d{2})$'\n",
    "cleaned_TAIC_recommendations_df = cleaned_TAIC_recommendations_df[cleaned_TAIC_recommendations_df['Inquiry'].str.match(inquiry_regex)]\n",
    "\n",
    "# printout how many rows were dropped\n",
    "print(f\"Dropped {len(original_TAIC_recommendations_df) - len(cleaned_TAIC_recommendations_df) - rows_deleted} rows that didn't match regex\")\n",
    "\n",
    "# Show the rows that were dropped\n",
    "# display(original_TAIC_recommendations_df[~original_TAIC_recommendations_df.index.isin(cleaned_TAIC_recommendations_df.index)])\n",
    "\n",
    "cleaned_TAIC_recommendations_df['report_id'] = cleaned_TAIC_recommendations_df['Inquiry'].apply(lambda x: \"_\".join(x.split('-')[1:3]))\n",
    "\n",
    "# Some of the recommendations have more than just hte recommendation itself. I will use regex to rid of these and can store the extra context in another column\n",
    "cleaned_TAIC_recommendations_df['has_extra_context'] = cleaned_TAIC_recommendations_df['recommendation_text'].apply(lambda x: re.search(r'[\\s\\S]{35,}the commission recommend[\\s\\S]{100,}', x, re.IGNORECASE) is not None)\n",
    "cleaned_TAIC_recommendations_df['extra_recommendation_context'] = cleaned_TAIC_recommendations_df.apply(lambda x: re.search(r'([\\s\\S]*)the commission recommend', x['recommendation_text'], re.IGNORECASE).group(1) if x['has_extra_context'] else None, axis=1) # type: ignore\n",
    "cleaned_TAIC_recommendations_df['recommendation'] = cleaned_TAIC_recommendations_df.apply(lambda x: x['recommendation_text'].replace(x['extra_recommendation_context'], '') if x['has_extra_context'] else x['recommendation_text'], axis=1)  \n",
    "\n",
    "# There are recommendations in 2011_104 that are duplicated once all of the context is removed.\n",
    "# This should be dropped now to not cause any more confusion\n",
    "cleaned_TAIC_recommendations_df.drop_duplicates(subset=['report_id', 'recommendation'], inplace=True)\n",
    "\n",
    "\n",
    "# To make it simpler later on all other columns are being removed.\n",
    "\n",
    "cleaned_TAIC_recommendations_df = cleaned_TAIC_recommendations_df[['report_id', 'recommendation_id', 'recommendation', 'extra_recommendation_context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('search_results.csv')\n",
    "\n",
    "print(f\"Total number of recommendations: {cleaned_TAIC_recommendations_df.shape[0]}\")\n",
    "print(f\"Average number of recommendations per report: {all_data['Recommendations'].mean():.2f}%\")\n",
    "print(f\"Number of reports with no recommendations: {all_data[all_data['Recommendations'] == 0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety extraction dataset\n",
    "\n",
    "I will need to extract all of the direct safety issues from the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "reports_path = 'output'\n",
    "\n",
    "reports = [name for name in os.listdir(reports_path) if os.path.isdir(os.path.join(reports_path, name))]\n",
    "\n",
    "reports = [report for report in reports if report not in reports_to_exclude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old method\n",
    "\n",
    "It was previously done by reading them with regex.\n",
    "However now I have a dataset that was run with the LLM. This is completely reliable but it is larger and better than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Clean the outputs folder\n",
    "\n",
    "# for report in reports:\n",
    "#     files = os.listdir(os.path.join(reports_path, report))\n",
    "\n",
    "#     for file in files:\n",
    "#         if not re.match(fr\"{report}((.pdf)|(.txt))\", file):\n",
    "#             print(os.path.join(reports_path, report, file))\n",
    "#             os.remove(os.path.join(reports_path, report, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function copied from 'safety_issue_extraction.ipynb'\n",
    "def  read_or_create_important_text_file(path, report_id, text):\n",
    "    important_text_path = os.path.join(path, report_id, f'{report_id}_important_text.txt')\n",
    "\n",
    "    if not os.path.isfile(important_text_path):\n",
    "        important_text, pages = ReportExtracting.ReportExtractor(text, report_id).extract_important_text()\n",
    "\n",
    "        important_text = \"\" if important_text == None else important_text\n",
    "\n",
    "        with open(important_text_path, 'w') as stream:\n",
    "            stream.write(important_text)\n",
    "\n",
    "        return important_text\n",
    "    \n",
    "    with open(important_text_path, 'r') as stream:\n",
    "        important_text = stream.read()\n",
    "        \n",
    "    important_text = None if important_text == \"\" else important_text\n",
    "\n",
    "    return important_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get safety issues extracted from all reports\n",
    "\n",
    "\n",
    "# importlib.reload(ReportExtracting)\n",
    "\n",
    "# safety_issues = []\n",
    "\n",
    "# for report in reports:\n",
    "    \n",
    "#     with open(os.path.join(reports_path, report, f'{report}.txt'), 'r') as stream:\n",
    "#         text = stream.read()\n",
    "\n",
    "#     important_text = read_or_create_important_text_file(reports_path, report, text)\n",
    "\n",
    "#     if important_text == None:\n",
    "#         print(\" No safety issues from \" + report + \" as important text could not be found\")\n",
    "#         continue\n",
    "\n",
    "#     extracted_safety_issues = ReportExtracting.SafetyIssueExtractor(text, report)._extract_safety_issues_with_regex(important_text)\n",
    "\n",
    "#     safety_issues.append({\n",
    "#         'report_id': report,\n",
    "#         'safety_issues': extracted_safety_issues\n",
    "#     })\n",
    "\n",
    "# safety_issues_df = pd.DataFrame(safety_issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lengthen it so that each safety issue has its own row\n",
    "safety_issues_df = safety_issues_df.explode('safety_issues')\n",
    "\n",
    "# Remove reports without explicit safety issues\n",
    "safety_issues_df.dropna(subset=['safety_issues'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the safety issues that have non standard characters\n",
    "\n",
    "standard_characters_regex = r'''^[\\w,.\\s()'\"/\\-%]+$'''\n",
    "\n",
    "non_standard_safety_issues = safety_issues_df[~safety_issues_df['safety_issues'].str.match(standard_characters_regex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_issues_df = safety_issues_df[safety_issues_df['safety_issues'].str.match(standard_characters_regex)]\n",
    "print(f\"  Removed {len(non_standard_safety_issues)} non standard safety issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method with LLM\n",
    "\n",
    "As I have the update data set from https://github.com/1jamesthompson1/TAIC-report-summary/pull/141\n",
    "\n",
    "Therefore I can simple read that datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "safety_issues = []\n",
    "\n",
    "for report in reports:\n",
    "    \n",
    "    SI_file_path = os.path.join(reports_path, report, f\"{report}_safety_issues.yaml\")\n",
    "\n",
    "    if os.path.exists(SI_file_path):\n",
    "        with open(SI_file_path, \"r\") as f:\n",
    "            safety_issues.append({\n",
    "                \"report_id\": report,\n",
    "                \"safety_issue\": yaml.safe_load(f)\n",
    "                })\n",
    "    else:\n",
    "        print(f\"Could not find safety issues for {SI_file_path}\")\n",
    "\n",
    "safety_issues_df = pd.DataFrame(safety_issues)\n",
    "safety_issues_df = safety_issues_df.explode(\"safety_issue\")\n",
    "\n",
    "safety_issues_df['safety_issue'] = safety_issues_df['safety_issue'].apply(lambda x: x['safety_issue'])\n",
    "\n",
    "display(safety_issues_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the two datasets\n",
    "\n",
    "Now that I have the two data sets `safety_issues_df` and `cleaned_TAIC_recommendations_df` I can combine them together to be compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# List the safety issues and the recommendations for each report\n",
    "\n",
    "combined_df = []\n",
    "\n",
    "for report in safety_issues_df['report_id'].unique():\n",
    "\n",
    "    report_safety_issues = safety_issues_df[safety_issues_df['report_id'] == report]['safety_issue']\n",
    "\n",
    "    report_recommendations = cleaned_TAIC_recommendations_df[cleaned_TAIC_recommendations_df['report_id'] == report]['recommendation']\n",
    "\n",
    "    for safety_issue in report_safety_issues:\n",
    "        for recommendation in report_recommendations:\n",
    "            combined_df.append({\n",
    "                'report_id': report,\n",
    "                'safety_issue': safety_issue,\n",
    "                'recommendation': recommendation\n",
    "            })\n",
    "\n",
    "combined_df = pd.DataFrame(combined_df)\n",
    "\n",
    "print(f\"  There are {len(combined_df)} safety issues and recommendations.\")\n",
    "\n",
    "test = combined_df.loc[combined_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link analysis functions\n",
    "\n",
    "Now that I have a big dataset I need to compare all the possible connections and see which ones stick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many tokens am I dealing with\n",
    "\n",
    "As I have a large data set with 500~ rows I need to know roughly how many tokens. Because this will give me a rough cost guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Count up the tokens in the safety issues and recommendations\n",
    "\n",
    "def how_many_tokens(df):\n",
    "\n",
    "    df['safety_issue_tokens'] = df['safety_issue'].apply(lambda x: sum(openAICaller.get_tokens(x)))\n",
    "    df['recommendation_tokens'] = df['recommendation'].apply(lambda x: sum(openAICaller.get_tokens(x)))\n",
    "\n",
    "    total_number_of_tokens = sum(df['safety_issue_tokens']) + sum(df['recommendation_tokens'])\n",
    "\n",
    "    return total_number_of_tokens\n",
    "\n",
    "def rough_cost(df):\n",
    "\n",
    "    tokens = how_many_tokens(df)\n",
    "\n",
    "    return round(tokens/1000 * 0.01, 2)\n",
    "\n",
    "print(f\"I am dealing with {len(combined_df)} possible combinations. The cost to read with gpt 4 turbo is ${rough_cost(combined_df)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating link functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# This collection of functions will be used to evaluate the recommendations using the all method. That is the LLM is given all the SI and recommendations at the same time\n",
    "\n",
    "def create_numbered_list_string(list):\n",
    "    return \"\\n\".join([f\"{i}. {item}\" for i, item in enumerate(list)])\n",
    "\n",
    "def issues_recommendations_to_string(SI, recs):\n",
    "    return \"\"\"\n",
    "\n",
    "Here are the safety issues:\n",
    "{SI}\n",
    "\n",
    "Here are the recommendations:\n",
    "{recs}\n",
    "\n",
    "\"\"\".format(SI=create_numbered_list_string(SI), recs=create_numbered_list_string(recs))\n",
    "\n",
    "def evaluate_all_SI_recommendations(SI, recommendations):\n",
    "  response = openAICaller.query(\n",
    "        system = f\"\"\"\n",
    "You are going to help me find find links between recommendations and safety issues identified in transport accident investigation reports.\n",
    "\n",
    "Each transport accident investigation report will identify safety issues. These reports will then issue recommendation that will address one or more of the safety issues identfied in the report.\n",
    "\n",
    "You will be given the list of safety issues and recommendations from the report. You will need to identify which safety issues the recommendation was meant to solve.\n",
    "\n",
    "There are two types of links that you can use:\n",
    "- Possible (The recommendation is reasonably likely to directly address the safety issue)\n",
    "- Confirmed (The recommendation explicitly mention that safety issue that it is trying address)\n",
    "\n",
    "Note that a recommendation will always be issues to address at least one safety issue but can solve multiple safety issues. However not all safety issues will have a recommendations. There there should be a linked safety issue for each recommendation.\n",
    "\n",
    "Your response should be a yaml list of all links\n",
    "\n",
    "- recommendation_index: 0\n",
    "  SI: [1, ... ]\n",
    "  SI_quality: ['Confirmed', ... ]\n",
    "...\n",
    "\n",
    "Your yaml response never needs opening or closing code blocks.\n",
    "        \"\"\",\n",
    "        user = issues_recommendations_to_string(SI, recommendations),\n",
    "        model = \"gpt-4\",\n",
    "        temp = 0\n",
    "    )\n",
    "\n",
    "  try:\n",
    "    response_yaml = yaml.safe_load(response)\n",
    "  except yaml.YAMLError as exc:\n",
    "    print(f\"Response was incorrect \\n{response} error is \\n{exc}\")\n",
    "\n",
    "  # check to make sure there are the right amount of recommendations\n",
    "  if len(response_yaml) != len(recommendations):\n",
    "    print(f\"Model response is incorrect there are the right  amount of recommendations\")\n",
    "\n",
    "  if not isinstance(response_yaml, list):\n",
    "    response_yaml = [response_yaml]\n",
    "\n",
    "  return response_yaml\n",
    "\n",
    "def find_recommendation_links_all(df):\n",
    "  \"\"\"\n",
    "  Take a expanded df (that is a row for each potential safety issue and recommendation match) and find all recommendation matches. \n",
    "  \"\"\"\n",
    "\n",
    "  # Convert to collapsed df with one row per report\n",
    "  collapsed_df = df.groupby('report_id').agg({'safety_issue': list, 'recommendation': list}).reset_index()\n",
    "\n",
    "  collapsed_df['safety_issue'] = collapsed_df['safety_issue'].apply(lambda x: list(set(x)))\n",
    "  collapsed_df['recommendation'] = collapsed_df['recommendation'].apply(lambda x: list(set(x)))\n",
    "\n",
    "  # Perform evaluation\n",
    "  collapsed_df['links'] = collapsed_df.apply(lambda row: evaluate_all_SI_recommendations(row['safety_issue'], row['recommendation']), axis=1)\n",
    "\n",
    "  # Expand back into all possible links\n",
    "\n",
    "  expanded_df = collapsed_df.explode('links')\n",
    "\n",
    "\n",
    "  expanded_df['recommendation'] = expanded_df.apply(lambda x: x['recommendation'][x['links']['recommendation_index']], axis=1)\n",
    "  expanded_df['targeted_SI'] = expanded_df['links'].apply(lambda x: x['SI'])\n",
    "\n",
    "  expanded_df = expanded_df.explode('targeted_SI')\n",
    "\n",
    "  expanded_df.dropna(subset=['targeted_SI'], inplace=True)\n",
    "\n",
    "  expanded_df['safety_issue'] = expanded_df.apply(lambda x: x['safety_issue'][x['targeted_SI']], axis=1)\n",
    "  expanded_df['link'] = expanded_df.apply(lambda x: x['links']['SI_quality'][x['links']['SI'].index(x['targeted_SI'])], axis=1)\n",
    "\n",
    "  # Join the expanded links as well as original to make sure that the none links are present.\n",
    "  combined_df = df.drop(labels = 'link', axis = 1, errors = 'ignore').merge(expanded_df, how = 'outer')[['report_id', 'safety_issue', 'recommendation', 'link']]\n",
    "\n",
    "  combined_df.fillna(\"None\", axis =1, inplace = True)\n",
    "\n",
    "  return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Function is used to evaluate a given recommendation and return either None, Possible, or Confirmed\n",
    "\n",
    "\n",
    "def evaluate_SI_recommendation_link(SI, recommendation):\n",
    "    response =openAICaller.query(\n",
    "        system = f\"\"\"\n",
    "You are going to help me find find links between recommendations and safety issues identified in transport accident investigation reports.\n",
    "\n",
    "Each transport accident investigation report will identify safety issues. These reports will then issue recommendation that will address one or more of the safety issues identfied in the report.\n",
    "\n",
    "For each pair given you need to respond with one of three answers.\n",
    "\n",
    "- None (The recommendation is not directly related to the safety issue)\n",
    "- Possible (The recommendation is reasonably likely to directly address the safety issue)\n",
    "- Confirmed (The recommendation explicitly mention that safety issue that it is trying address)\n",
    "\"\"\",\n",
    "        user = f\"\"\"\n",
    "Here is the safety issue:\n",
    "\n",
    "{SI}\n",
    "\n",
    "\n",
    "Here is the recommendation:\n",
    "\n",
    "{recommendation}\n",
    "\n",
    "Now can you please respond with one of three options\n",
    "\n",
    "- None (The recommendation is not directly related to the safety issue)\n",
    "- Possible (The recommendation is reasonably likely to directly address the safety issue)\n",
    "- Confirmed (The recommendation explicitly mention that safety issue that it is trying address)\n",
    "\"\"\",\n",
    "        model = \"gpt-4\",\n",
    "        temp = 0)\n",
    "    \n",
    "    if response in ['None', 'Possible', 'Confirmed']:\n",
    "        return response\n",
    "    else:\n",
    "        print(f\"Model response is incorrect and is {response}\")\n",
    "        return 'undetermined'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Look for pkl file and if not found remake it\n",
    "_EVALUATION_TYPES = Literal['single', 'all']\n",
    "\n",
    "def perform_link_evaluation(df, _file_name = 'links', link_evaluation_method:_EVALUATION_TYPES = 'single'):\n",
    "    \"\"\"\n",
    "    This function will do a the link evaluation with the llm. It will however save it to a file at the end. Therefore next time it is run it will just retrieve the file if it thinks it is up to date.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = f\"{_file_name}.pkl\"\n",
    "\n",
    "    redo = False\n",
    "    if os.path.exists(file_name):\n",
    "\n",
    "        print(f\" Found pkl file {file_name}\")\n",
    "        found_df = pd.read_pickle(file_name)\n",
    "\n",
    "        if found_df.drop(columns='link').equals(df.drop(columns='link', errors='ignore')):\n",
    "            print(\"  Pkl file is up to date\")\n",
    "            df = found_df\n",
    "        else:\n",
    "            redo =True\n",
    "    else:\n",
    "        redo = True\n",
    "\n",
    "    if not redo:\n",
    "        return df\n",
    "    \n",
    "    print(f\"Remaking the file\")\n",
    "\n",
    "    match link_evaluation_method:\n",
    "        case 'single':\n",
    "            df['link'] = df.apply(lambda x: evaluate_SI_recommendation_link(x['safety_issue'], x['recommendation']), axis=1)\n",
    "\n",
    "        case 'all':\n",
    "            df = find_recommendation_links_all(df)\n",
    "\n",
    "    print(f\" saving file: {file_name}\")\n",
    "    df.to_pickle(file_name)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link_visualization(df, report_id, folder = 'visualization_of_links'):\n",
    "    '''\n",
    "    Create a picture that shows both the recommendations and the safety issues fro a report. There will be arrows showing the link between the two.\n",
    "    '''\n",
    "    # Create a directed graph\n",
    "    print(\"Creating visualization\")\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    NODE_WIDTH = 50\n",
    "\n",
    "    # Preemptively wrap the text\n",
    "    df['recommendation'] = df['recommendation'].apply(lambda x: \"\\n\".join(textwrap.wrap(x, width=NODE_WIDTH)))\n",
    "    df['safety_issue'] = df['safety_issue'].apply(lambda x: \"\\n\".join(textwrap.wrap(x, width=NODE_WIDTH)))\n",
    "\n",
    "    # Add nodes for the 'extracted' and 'inferred' indices\n",
    "    for i, text in enumerate(df['recommendation'].unique()):\n",
    "        G.add_node(text, pos=(0, i))\n",
    "\n",
    "    for i, issue in enumerate(df['safety_issue'].unique()):\n",
    "        G.add_node(issue, pos=(3, i))\n",
    "\n",
    "\n",
    "    # Add edges between the matched indices\n",
    "    for _, row in df.iterrows():\n",
    "        # Add solid arrow\n",
    "        if row['link'] == 'Confirmed':\n",
    "            G.add_edge(row['recommendation'], row['safety_issue'], color='red', style='solid', alpha =1)\n",
    "\n",
    "        # Add dotted arrow\n",
    "        elif row['link'] == 'Possible':\n",
    "            G.add_edge(row['recommendation'], row['safety_issue'], color='blue', style='dashed', alpha = 0.5)\n",
    "\n",
    "\n",
    "    max_num_of_nodes_column = max(\n",
    "        len(df['recommendation'].unique()),\n",
    "        len(df['safety_issue'].unique())\n",
    "        )\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    plt.figure(figsize=(10, max_num_of_nodes_column * 5))  \n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=[len(node) * NODE_WIDTH for node in G.nodes()])\n",
    "    nx.draw_networkx_labels(G, pos, font_size=7)\n",
    "    nx.draw_networkx_edges(G, pos, arrows=True, edge_color=nx.get_edge_attributes(G, 'color').values(), style=list(nx.get_edge_attributes(G, 'style').values()), alpha = list(nx.get_edge_attributes(G, 'alpha').values()))\n",
    "\n",
    "    plt.xlim(-1, 4.5)  # Add buffer to the outside side edges\n",
    "    plt.ylim(-1, max_num_of_nodes_column*1)  # Add buffer to the outside top and bottom edges\n",
    "\n",
    "    # Add report ID and headers\n",
    "    plt.title(f'Report ID: {report_id}')\n",
    "    plt.text(0, max_num_of_nodes_column-0.2, 'Recommendations', fontsize=12, ha='center')\n",
    "    plt.text(3, max_num_of_nodes_column-0.2, 'Safety issue', fontsize=12, ha='center')\n",
    "    plt.text(1.5, max_num_of_nodes_column-0.5, 'Arrow indicates that the recommendation is indicated to solve the safety issue by the LLM', fontsize=6, ha='center')\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    plt.savefig(os.path.join(folder, f'{report_id}_comparison_visualization.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lots of d which just have one sort of report_id. THen send each individual one to the make visualization function\n",
    "def make_link_visualization_for_df(df, output_folder = 'visualization_of_links'):\n",
    "    for report_id in df['report_id'].unique():\n",
    "        d = df[df['report_id'] == report_id]\n",
    "\n",
    "        make_link_visualization(d, report_id, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing link analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out linking with sample\n",
    "\n",
    "I am going to simply look at 10 random reports and see how to do some comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the play dataset\n",
    "\n",
    "playset_reports = combined_df['report_id'].sample(10, random_state=42)\n",
    "\n",
    "combined_df_playset = combined_df[combined_df['report_id'].isin(playset_reports)]\n",
    "\n",
    "print(f\"  There are {len(combined_df_playset)} safety issues and recommendations in the playset.\\nIt will cost roughly ${rough_cost(combined_df_playset)} to run through.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform analysis on playset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playset_links = perform_link_evaluation(combined_df_playset, _file_name = 'playset_links', link_evaluation_method = 'single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform link analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_TAIC_recommendations_df.to_csv('cleaned_TAIC_recommendations_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df_SI_first = perform_link_evaluation(combined_df, _file_name = 'all_links_SI_first', link_evaluation_method = 'single')\n",
    "links_df_SI_second = pd.read_pickle('all_links_SI_second.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the linking results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_link_visualization_for_df(links_df_SI_first, 'visualization_of_links_SI_first')\n",
    "make_link_visualization_for_df(links_df_SI_second, 'visualization_of_links_SI_second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_stats_links(df):\n",
    "    \"\"\"\n",
    "    This function will return a string that has a nice printout that summaries the links with all useful statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # Check to make sure that there are the 4 required columns\n",
    "    if not {'report_id', 'safety_issue', 'recommendation', 'link'}.issubset(df.columns):\n",
    "        raise ValueError(\"df must have columns 'report_id', 'safety_issue', 'recommendation', and 'link'\")\n",
    "    \n",
    "    number_of_reports = df['report_id'].unique().shape[0]\n",
    "    \n",
    "    return_string = f\"\"\"\n",
    "== Here are some summary stats for the df ==\n",
    "\n",
    "There are {df.shape[0]} links compared.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Stats about recommendations\n",
    "\n",
    "    number_of_recommendations = df['recommendation'].unique().shape[0]\n",
    "\n",
    "    avg_recs_per_report = number_of_recommendations/number_of_reports\n",
    "\n",
    "    return_string += f\"\"\"\n",
    "There are {number_of_recommendations} unique recommendations and {avg_recs_per_report:.2f} recommendations per report.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Stats about safety issues\n",
    "\n",
    "    number_of_safety_issues = df['safety_issue'].unique().shape[0]\n",
    "\n",
    "    avg_safety_issues_per_report = number_of_safety_issues/number_of_reports\n",
    "\n",
    "    return_string += f\"\"\"\n",
    "There are {number_of_safety_issues} unique safety issues and {avg_safety_issues_per_report:.2f} safety issues per report.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Stats about links\n",
    "\n",
    "    number_of_links = df.shape[0]\n",
    "\n",
    "    avg_links_per_report = number_of_links/number_of_reports\n",
    "\n",
    "    avg_confirmed_links_per_report = df[df['link'] == 'Confirmed'].shape[0]/number_of_reports\n",
    "\n",
    "    avg_total_links_per_report = df[df['link'].isin(['Confirmed', 'Possible'])].shape[0]/number_of_reports\n",
    "\n",
    "    link_type_count = df['link'].value_counts()\n",
    "\n",
    "    return_string += f\"\"\"\n",
    "There are {number_of_links} unique links and {avg_confirmed_links_per_report:.2f} confirmed links per report.\n",
    "The link type breakdown is as follows:\n",
    "None: {link_type_count['None']} ({link_type_count['None']/number_of_links*100:.2f}% of all links)\n",
    "Possible: {link_type_count['Possible']} ({link_type_count['Possible']/number_of_links*100:.2f}% of all links)\n",
    "Confirmed: {link_type_count['Confirmed']} ({link_type_count['Confirmed']/number_of_links*100:.2f}% of all links)\n",
    "    \"\"\"\n",
    "\n",
    "    ### Stats about links per recommendations\n",
    "\n",
    "    possible_or_greater_links_per_rec = avg_total_links_per_report/avg_recs_per_report\n",
    "\n",
    "    confirmed_links_per_rec = avg_confirmed_links_per_report/avg_recs_per_report\n",
    "\n",
    "    potential_links_per_rec = avg_links_per_report/avg_recs_per_report\n",
    "\n",
    "    return_string += f\"\"\"\n",
    "There are about {possible_or_greater_links_per_rec:.2f} links per recommendation.\n",
    "These are {confirmed_links_per_rec:.2f} confirmed per recommendation.\n",
    "Note that there were on average {potential_links_per_rec:.2f} potential links per recommendation. So only about {confirmed_links_per_rec/potential_links_per_rec*100:.2f}% of links were confirmed.\n",
    "\"\"\"\n",
    "    \n",
    "    return_string += f\"\"\"\n",
    "== End of summary ==\n",
    "    \"\"\"\n",
    "\n",
    "    print(return_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_summary_stats_links(links_df_SI_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_summary_stats_links(links_df_SI_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of two prompts\n",
    "\n",
    "I have now split up and have two link datasets one with SI given first and one with SI given second.\n",
    "\n",
    "the SI first solves problems that SI_second had. However I need to confirm that it doesnt change to much.\n",
    "\n",
    "The summary stats show that they are quite similar but I could actaully compare row by row and find the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.merge(links_df_SI_second, links_df_SI_first, on = ['report_id', 'safety_issue', 'recommendation'], suffixes=('_SI_second', '_SI_first'))\n",
    "\n",
    "comparison_df['link_changed'] = comparison_df['link_SI_first'] != comparison_df['link_SI_second']\n",
    "\n",
    "changed_df = comparison_df[comparison_df['link_changed'] == True]\n",
    "\n",
    "changed_df['change'] = changed_df['link_SI_second'] + ' -> ' + changed_df['link_SI_first']\n",
    "\n",
    "display(changed_df['change'].value_counts())\n",
    "\n",
    "# What are the rows where the link was downgraded\n",
    "\n",
    "print(\"Links going from Confirmed -> None\")\n",
    "display(changed_df[changed_df['change'] == 'Confirmed -> None'])\n",
    "\n",
    "print(\"Links going from Confirmed -> Possible\")\n",
    "display(changed_df[changed_df['change'] == 'Confirmed -> Possible'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2011_102*\n",
    "The recommendation is a summary of all of the other recommendations. This means that it has no relevant context\n",
    "\n",
    "*2018_001*\n",
    "Even though a connection could be made. It was not really necessary.\n",
    "\n",
    "The rest of these are maybe not perfect but it is not that different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out analysis\n",
    "\n",
    "\n",
    "The two identified problems are \n",
    "- Too many links we only need one link type this will be fixed by upgrading the possible however they will be mostly be deleted.\n",
    "- There are recommendations without any links\n",
    "\n",
    "Therefor the main problem which needs to be focused on here is the **recommendations without any links**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which ones don't have links\n",
    "\n",
    "I need to focus on the problem that some done have links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find recommendations from combined_df_playset that dont have a linked safety issue\n",
    "\n",
    "def find_unlinked_recommendations(df):\n",
    "    all_recommendations = df.drop_duplicates(['report_id', 'recommendation', 'safety_issue'])\n",
    "\n",
    "    linked_recommendations = df[df['link'] == \"Confirmed\"]\n",
    "\n",
    "    unlinked_recommendations = all_recommendations[~all_recommendations['recommendation'].isin(linked_recommendations['recommendation'])]\n",
    "\n",
    "    return unlinked_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SI second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_unlinked_recommendations(links_df_SI_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will look through the 5 reports that have unlinked recommendations.\n",
    "\n",
    "**2010_009**\n",
    "This report has really short safety issues. This can make it hard for it to find anything to link to.\n",
    "\n",
    "<em>Hmm, yes, I think you’re right for the most part. Some of it is to do with the way we are writing our reports. Re the unlinked rec on drug and alcohol: it is mentioned in 4.1.3 as an ‘issue’ (although not a ‘safety issue’) and then turns up in a ‘findings’ box. Also, the text talks about cannabis and performance-impairing substances but then the rec refers to ‘drug and alcohol’ – but maybe that wouldn’t flummox your model!\n",
    "\n",
    "For the other unlinked recs, they are about monitoring, checking, ensuring standards, which I would all link to the safety issue ‘ CAA’s oversight of the parachuting industry’. So I think the report has identified several safety issues and broadly lumped them under one ‘theme’; and then written some recs on that topic. So, yes, I agree that this is about safety rec extraction. But one of them – the one about monitoring the outcome of a FAA/US Parachute Assoc report -- refers to something in the report that isn’t actually a safety issue. IMHO. I don’t think it really should be a recommendation (assuming that a rec is about system-level safety issues).  \n",
    "\n",
    "Some very vague wording and passive sentence construction doesn’t help.</em>\n",
    "\n",
    "**2016_206**\n",
    "One is missing a red link but does have a blue link. \n",
    "This link could be upgrades to red if needed.\n",
    "\n",
    "<em> Ditto last sentence above. The rec refers to ‘people’ – but which people? If it’s recreational boaties, then the link is to the third rec,  I think the safety issue links to the third rec only; otherwise both blue links should be red. On reading the report, I think it’s the latter.</em>\n",
    "\n",
    "**2013_005**\n",
    "Vague wordings that don't make it clear that they are addressing each other.\n",
    "The recommendation is specific where both of the safety issues are more general.\n",
    "\n",
    "<em>One of the safety issues here is identified as such in the report (the only one identified) and the other is a ‘finding’ (see 1.4). But the unlinked rec also relates to a finding listed in 1.4 – so not sure why the model would have picked up one of these as a safety issue, but not the other….?</em>\n",
    "\n",
    "**2014_005**\n",
    "The first missing recommendation about seat belts does not seem to have an associated safety issue.\n",
    "Recommendations regarding the vortex effect has a blue link that seems reasonable.\n",
    "\n",
    "<em>This seems to me another example of making recs on the basis of things other than safety issues – in this case key lessons. I think all three safety issues are linked to the middle rec on ‘safety culture’, which is about the CAA doing something in its oversight role – reviewing and analysing safety culture. The other two recs are about its educational role and using the key lessons from the inquiry to educate pilots about stuff – so (in my opinion anyway) not really a rec aimed at making change at the system level to mitigate a risk.</em>\n",
    "\n",
    "**2011_204**\n",
    "There are not many direct links here but the two blue links will work.\n",
    "\n",
    "<em>This is a bit like 2010-009 – a potential safety issue is identified at 4.1.19 and discussed at 4.8. There was not enough evidence to say that it was, so the rec was about collecting data to see whether action was justified.</em>\n",
    "\n",
    "_I am awaiting a response from Ingrid_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SI first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_unlinked_recommendations(links_df_SI_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upgrading possible to confirmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_unlinked_recommendations(df):\n",
    "\n",
    "    # Find unlinked recommendations\n",
    "    unlinked_recommendations = find_unlinked_recommendations(df)\n",
    "\n",
    "    print(f\"There are {unlinked_recommendations.drop_duplicates(['report_id', 'recommendation']).shape[0]} unlinked recommendations. This is {unlinked_recommendations.drop_duplicates(['report_id', 'recommendation']).shape[0]/df.drop_duplicates(['report_id', 'recommendation']).shape[0]*100:.2f}% of all recommendations.\")\n",
    "\n",
    "    # Find which links should be upgraded\n",
    "    link_to_upgrade = unlinked_recommendations[unlinked_recommendations['link'] == \"Possible\"]\n",
    "    link_to_upgrade['link'] = \"Confirmed\"\n",
    "\n",
    "    # Upgrade links in original df\n",
    "    upgraded_df = df.merge(link_to_upgrade, how = 'outer')\n",
    "\n",
    "    upgraded_df.drop_duplicates(subset=['report_id', 'safety_issue', 'recommendation'], keep = 'first', inplace = True)\n",
    "\n",
    "    num_upgraded_links = upgraded_df.value_counts('link')['Confirmed'] - df.value_counts('link')['Confirmed']\n",
    "\n",
    "    print(f\"{num_upgraded_links} links were upgraded.\\n This represents {num_upgraded_links/df.shape[0]*100:.2f}% of all links and {num_upgraded_links/df[df['link'] == 'Possible'].shape[0]*100:.2f}% of possible links.\")\n",
    "\n",
    "    still_unlinked_recommendations = find_unlinked_recommendations(upgraded_df).drop_duplicates(['report_id', 'recommendation'])[[\"report_id\", \"recommendation\"]]\n",
    "    print(f\"After performing the upgrading there are still {still_unlinked_recommendations.shape[0]} unlinked recommendations which is {still_unlinked_recommendations.shape[0]/df.drop_duplicates(['report_id', 'recommendation']).shape[0]*100:.2f}% of all recommendations.\")\n",
    "    if still_unlinked_recommendations.shape[0] < 10 and still_unlinked_recommendations.shape[0] > 0:\n",
    "        display(still_unlinked_recommendations)\n",
    "\n",
    "    return upgraded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing upgrading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at second\n",
    "\n",
    "links_df_SI_second_upgraded = upgrade_unlinked_recommendations(links_df_SI_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at SI first in prompt\n",
    "\n",
    "links_df_SI_first_upgraded = upgrade_unlinked_recommendations(links_df_SI_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_visuals_for_upgraded(df):\n",
    "    remove_possible_df = df.where(df['link'] != 'Possible', \"None\")\n",
    "    make_link_visualization_for_df(remove_possible_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remaining unlinked\n",
    "\n",
    "There are 5 recommendations that still aren't linked.\n",
    "\n",
    "I will go through each of them here.\n",
    "\n",
    "_After discussion with TAIC these were all deemed to be non important and it would be reasonable to leave them unlinked. The only caveat to this was 2013_002 as it was a simple erorr._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2010_009\n",
    "\n",
    "Drug and Alcohol related recommendation where the safety issues are really short and non descript. This means that even though it could be argued to be linked to the last safety issue. This is regarding the operator and owner use of the aircraft.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2011_003\n",
    "\n",
    "Flight tracking devices are not relevant at all to any of the safety issues identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2013_002\n",
    "\n",
    "This is a problem where there is a clear link and it is just missing it.\n",
    "\n",
    "Not sure why but will do some tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_clear_link = combined_df[combined_df['report_id'] == \"2013_002\"]\n",
    "\n",
    "missing_clear_link['link'] = missing_clear_link.apply(lambda x: evaluate_SI_recommendation_link(x['safety_issue'], x['recommendation']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have had a look and the problem is that it doesn't think a helicopter is a type of aircraft.\n",
    "\n",
    "I could resolve this by adding some information in the system message about helicopters being a type of aricraft.\n",
    "\n",
    "However I noticed that just changing the order around fixes it. I have worked on it above and now have two link datasets. I will have to compare these and see how really different they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2013_005\n",
    "\n",
    "This is the problem where the reccommenation relaate to a fiding but not a safety issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2014_005\n",
    "\n",
    "Missing a direct link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have run this on a complete runthrough with two types of prompts eithe SI first or second.\n",
    "\n",
    "This was to resolve the 3013_005 problem of missing a direct link.\n",
    "\n",
    "There are no unlinked recommendation problem anymore.\n",
    "\n",
    "I will have to confirm that flipping the prompt is not too problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving all issues and recommendations at same time and asking for a link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Perform evaluation\n",
    "combined_df_playset_alltogather = find_recommendation_links_all(combined_df_playset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for report_id in combined_df_playset_alltogather['report_id'].unique():\n",
    "    d = combined_df_playset_alltogather[combined_df_playset_alltogather['report_id'] == report_id]\n",
    "\n",
    "    make_link_visualization(d, report_id, 'visualization_of_links_alltogather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having a look at 2019_006\n",
    "single_df = find_recommendation_links_all(combined_df[combined_df['report_id'] == '2019_006'])\n",
    "\n",
    "make_link_visualization(single_df, '2019_006', 'visualization_of_links_alltogather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
