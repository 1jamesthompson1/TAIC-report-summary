{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "As discussed in https://github.com/1jamesthompson1/TAIC-report-summary/issues/130 there is a requirement for recommendations to be extracted and linked. Due to the discovering of a dataset the extraction is no longer needed and focus on linking can be made.\n",
    "\n",
    "## How to do it\n",
    "\n",
    "I have two datasets to work with.\n",
    "\n",
    "Firstly is the recommendation dataset from TAIC this dataset can be considered trustworthy and complete.\n",
    "Secondly I have the safety issue extracted from the reports. This has a problem that as it is from the engine it cannot be fully trusted. What I will do instead will start with just the ones extracted using regexes so that I know they are true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the modules needed\n",
    "\n",
    "To keep things as transparent as possible I will add all of the dependencies at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the engine\n",
    "import engine.Extract_Analyze.ReportExtracting as ReportExtracting\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "\n",
    "# Third party\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Built in\n",
    "import os\n",
    "import re\n",
    "import importlib\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting datasets\n",
    "\n",
    "My goal here is to have a single dataset that has 3 columns.\n",
    "\n",
    "These are report_id, safety_issue and recommendation. *Note that there will be more columns but this is the idea of the three things conveyed by each row*\n",
    "\n",
    "This means that there will be a row for all of the safety issues in each report and for each safety issue row there will also be a row for each recommendation from the report. Therefore I will be making a very long dataset\n",
    "\n",
    "In the next section I will work on adding the fourth column of whether they are linked and from their I can easily filter out the nonexistant connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few reports that have problems for various reasons that are just not worth the effort to include. These will be excluded from this work\n",
    "\n",
    "reports_to_exclude = [\n",
    "    '2022_203', # stevedore joint investigation that doesnt follow usual format\n",
    "    '2022_202', # \"\"\n",
    "    '2023_010', # This is not a report and instead just a protection order\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAIC dataset\n",
    "\n",
    "This data set just comes from a xlsx files but needs to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 165 rows with NAs in either recommendation_text or Inquiry\n",
      "Dropped 37 rows that didn't match regex\n"
     ]
    }
   ],
   "source": [
    "original_TAIC_recommendations_df = pd.read_excel('TAIC_recommendations_04_04_2024.xlsx')\n",
    "\n",
    "cleaned_TAIC_recommendations_df = original_TAIC_recommendations_df.copy()\n",
    "\n",
    "cleaned_TAIC_recommendations_df['recommendation_id'] = cleaned_TAIC_recommendations_df['Number']\n",
    "\n",
    "cleaned_TAIC_recommendations_df['recommendation_text'] = cleaned_TAIC_recommendations_df['Recommendation']\n",
    "\n",
    "cleaned_TAIC_recommendations_df.dropna(subset=['recommendation_text', 'Inquiry'], inplace = True)\n",
    "\n",
    "rows_deleted = len(original_TAIC_recommendations_df) - len(cleaned_TAIC_recommendations_df)\n",
    "print(f\"Deleting {rows_deleted} rows with NAs in either recommendation_text or Inquiry\")\n",
    "\n",
    "\n",
    "# find all rows that dont match regex on column 'Inquiry'\n",
    "inquiry_regex = r'^(((AO)|(MO)|(RO))-[12][09][987012]\\d-[012]\\d{2})$'\n",
    "cleaned_TAIC_recommendations_df = cleaned_TAIC_recommendations_df[cleaned_TAIC_recommendations_df['Inquiry'].str.match(inquiry_regex)]\n",
    "\n",
    "# printout how many rows were dropped\n",
    "print(f\"Dropped {len(original_TAIC_recommendations_df) - len(cleaned_TAIC_recommendations_df) - rows_deleted} rows that didn't match regex\")\n",
    "\n",
    "# Show the rows that were dropped\n",
    "# display(original_TAIC_recommendations_df[~original_TAIC_recommendations_df.index.isin(cleaned_TAIC_recommendations_df.index)])\n",
    "\n",
    "cleaned_TAIC_recommendations_df['report_id'] = cleaned_TAIC_recommendations_df['Inquiry'].apply(lambda x: \"_\".join(x.split('-')[1:3]))\n",
    "\n",
    "cleaned_TAIC_recommendations_df = cleaned_TAIC_recommendations_df[['report_id', 'recommendation_id', 'recommendation_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety extraction dataset\n",
    "\n",
    "I will need to extract all of the direct safety issues from the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reports_path = 'output'\n",
    "\n",
    "reports = [name for name in os.listdir(reports_path) if os.path.isdir(os.path.join(reports_path, name))]\n",
    "\n",
    "reports = [report for report in reports if report not in reports_to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/2019_106/2019_106_important_text.txt\n",
      "output/2013_107/2013_107_important_text.txt\n",
      "output/2020_102/2020_102_important_text.txt\n",
      "output/2011_003/2011_003_important_text.txt\n",
      "output/2012_105/2012_105_important_text.txt\n",
      "output/2019_202/2019_202_important_text.txt\n",
      "output/2022_102/2022_102_important_text.txt\n",
      "output/2010_010/2010_010_important_text.txt\n",
      "output/2013_011/2013_011_important_text.txt\n",
      "output/2015_103/2015_103_important_text.txt\n",
      "output/2019_201/2019_201_important_text.txt\n",
      "output/2013_006/2013_006_important_text.txt\n",
      "output/2010_009/2010_009_important_text.txt\n",
      "output/2019_204/2019_204_important_text.txt\n",
      "output/2010_007/2010_007_important_text.txt\n",
      "output/2015_202/2015_202_important_text.txt\n",
      "output/2010_101/2010_101_important_text.txt\n",
      "output/2016_206/2016_206_important_text.txt\n",
      "output/2021_101/2021_101_important_text.txt\n",
      "output/2010_202/2010_202_important_text.txt\n",
      "output/2021_204/2021_204_important_text.txt\n",
      "output/2011_001/2011_001_important_text.txt\n",
      "output/2012_001/2012_001_important_text.txt\n",
      "output/2016_006/2016_006_important_text.txt\n",
      "output/2019_203/2019_203_important_text.txt\n",
      "output/2020_204/2020_204_important_text.txt\n",
      "output/2021_103/2021_103_important_text.txt\n",
      "output/2020_205/2020_205_important_text.txt\n",
      "output/2011_103/2011_103_important_text.txt\n",
      "output/2019_001/2019_001_important_text.txt\n",
      "output/2011_203/2011_203_important_text.txt\n",
      "output/2015_003/2015_003_important_text.txt\n",
      "output/2017_201/2017_201_important_text.txt\n",
      "output/2013_002/2013_002_important_text.txt\n",
      "output/2010_011/2010_011_important_text.txt\n",
      "output/2017_102/2017_102_important_text.txt\n",
      "output/2021_003/2021_003_important_text.txt\n",
      "output/2010_206/2010_206_important_text.txt\n",
      "output/2011_004/2011_004_important_text.txt\n",
      "output/2010_207/2010_207_important_text.txt\n",
      "output/2012_202/2012_202_important_text.txt\n",
      "output/2013_202/2013_202_important_text.txt\n",
      "output/2011_202/2011_202_important_text.txt\n",
      "output/2019_105/2019_105_important_text.txt\n",
      "output/2010_006/2010_006_important_text.txt\n",
      "output/2020_103/2020_103_important_text.txt\n",
      "output/2012_102/2012_102_important_text.txt\n",
      "output/2012_104/2012_104_important_text.txt\n",
      "output/2011_106/2011_106_important_text.txt\n",
      "output/2022_104/2022_104_important_text.txt\n",
      "output/2016_203/2016_203_important_text.txt\n",
      "output/2012_101/2012_101_important_text.txt\n",
      "output/2016_004/2016_004_important_text.txt\n",
      "output/2019_107/2019_107_important_text.txt\n",
      "output/2015_201/2015_201_important_text.txt\n",
      "output/2016_205/2016_205_important_text.txt\n",
      "output/2010_003/2010_003_important_text.txt\n",
      "output/2010_205/2010_205_important_text.txt\n",
      "output/2019_102/2019_102_important_text.txt\n",
      "output/2022_103/2022_103_important_text.txt\n",
      "output/2021_205/2021_205_important_text.txt\n",
      "output/2013_005/2013_005_important_text.txt\n",
      "output/2022_201/2022_201_important_text.txt\n",
      "output/2018_005/2018_005_important_text.txt\n",
      "output/2019_108/2019_108_important_text.txt\n",
      "output/2013_009/2013_009_important_text.txt\n",
      "output/2018_102/2018_102_important_text.txt\n",
      "output/2019_104/2019_104_important_text.txt\n",
      "output/2021_201/2021_201_important_text.txt\n",
      "output/2017_205/2017_205_important_text.txt\n",
      "output/2016_102/2016_102_important_text.txt\n",
      "output/2020_002/2020_002_important_text.txt\n",
      "output/2014_002/2014_002_important_text.txt\n",
      "output/2014_006/2014_006_important_text.txt\n",
      "output/2014_005/2014_005_important_text.txt\n",
      "output/2014_101/2014_101_important_text.txt\n",
      "output/2015_102/2015_102_important_text.txt\n",
      "output/2021_202/2021_202_important_text.txt\n",
      "output/2015_009/2015_009_important_text.txt\n",
      "output/2015_005/2015_005_important_text.txt\n",
      "output/2013_101/2013_101_important_text.txt\n",
      "output/2017_103/2017_103_important_text.txt\n",
      "output/2013_203/2013_203_important_text.txt\n",
      "output/2017_002/2017_002_important_text.txt\n",
      "output/2012_103/2012_103_important_text.txt\n",
      "output/2020_003/2020_003_important_text.txt\n",
      "output/2014_105/2014_105_important_text.txt\n",
      "output/2023_103/2023_103_important_text.txt\n",
      "output/2021_203/2021_203_important_text.txt\n",
      "output/2018_203/2018_203_important_text.txt\n",
      "output/2022_206/2022_206_important_text.txt\n",
      "output/2014_202/2014_202_important_text.txt\n",
      "output/2010_004/2010_004_important_text.txt\n",
      "output/2013_010/2013_010_important_text.txt\n",
      "output/2021_001/2021_001_important_text.txt\n",
      "output/2012_201/2012_201_important_text.txt\n",
      "output/2017_105/2017_105_important_text.txt\n",
      "output/2022_101/2022_101_important_text.txt\n",
      "output/2010_201/2010_201_important_text.txt\n",
      "output/2018_206/2018_206_important_text.txt\n",
      "output/2016_002/2016_002_important_text.txt\n",
      "output/2011_105/2011_105_important_text.txt\n",
      "output/2011_002/2011_002_important_text.txt\n",
      "output/2011_101/2011_101_important_text.txt\n",
      "output/2020_201/2020_201_important_text.txt\n",
      "output/2015_002/2015_002_important_text.txt\n",
      "output/2014_102/2014_102_important_text.txt\n",
      "output/2014_103/2014_103_important_text.txt\n",
      "output/2010_204/2010_204_important_text.txt\n",
      "output/2014_004/2014_004_important_text.txt\n",
      "output/2018_204/2018_204_important_text.txt\n",
      "output/2020_001/2020_001_important_text.txt\n",
      "output/2014_203/2014_203_important_text.txt\n",
      "output/2019_005/2019_005_important_text.txt\n",
      "output/2013_105/2013_105_important_text.txt\n",
      "output/2016_101/2016_101_important_text.txt\n",
      "output/2019_007/2019_007_important_text.txt\n",
      "output/2010_203/2010_203_important_text.txt\n",
      "output/2020_202/2020_202_important_text.txt\n",
      "output/2016_008/2016_008_important_text.txt\n",
      "output/2011_104/2011_104_important_text.txt\n",
      "output/2011_005/2011_005_important_text.txt\n",
      "output/2019_101/2019_101_important_text.txt\n",
      "output/2011_007/2011_007_important_text.txt\n",
      "output/2017_202/2017_202_important_text.txt\n",
      "output/2013_007/2013_007_important_text.txt\n",
      "output/2021_105/2021_105_important_text.txt\n",
      "output/2019_103/2019_103_important_text.txt\n",
      "output/2015_101/2015_101_important_text.txt\n",
      "output/2010_001/2010_001_important_text.txt\n",
      "output/2013_103/2013_103_important_text.txt\n",
      "output/2013_008/2013_008_important_text.txt\n",
      "output/2018_101/2018_101_important_text.txt\n",
      "output/2018_205/2018_205_important_text.txt\n",
      "output/2017_106/2017_106_important_text.txt\n",
      "output/2012_002/2012_002_important_text.txt\n",
      "output/2023_201/2023_201_important_text.txt\n",
      "output/2017_204/2017_204_important_text.txt\n",
      "output/2011_102/2011_102_important_text.txt\n",
      "output/2019_003/2019_003_important_text.txt\n",
      "output/2012_203/2012_203_important_text.txt\n",
      "output/2013_003/2013_003_important_text.txt\n",
      "output/2015_203/2015_203_important_text.txt\n",
      "output/2010_102/2010_102_important_text.txt\n",
      "output/2013_104/2013_104_important_text.txt\n",
      "output/2010_005/2010_005_important_text.txt\n",
      "output/2016_201/2016_201_important_text.txt\n",
      "output/2015_007/2015_007_important_text.txt\n",
      "output/2019_002/2019_002_important_text.txt\n",
      "output/2016_204/2016_204_important_text.txt\n",
      "output/2022_002/2022_002_important_text.txt\n",
      "output/2018_001/2018_001_important_text.txt\n",
      "output/2018_202/2018_202_important_text.txt\n",
      "output/2013_102/2013_102_important_text.txt\n",
      "output/2019_006/2019_006_important_text.txt\n",
      "output/2011_204/2011_204_important_text.txt\n",
      "output/2014_104/2014_104_important_text.txt\n",
      "output/2022_207/2022_207_important_text.txt\n",
      "output/2013_108/2013_108_important_text.txt\n",
      "output/2017_203/2017_203_important_text.txt\n",
      "output/2020_101/2020_101_important_text.txt\n",
      "output/2021_102/2021_102_important_text.txt\n",
      "output/2013_201/2013_201_important_text.txt\n",
      "output/2021_106/2021_106_important_text.txt\n",
      "output/2017_007/2017_007_important_text.txt\n",
      "output/2014_201/2014_201_important_text.txt\n",
      "output/2015_001/2015_001_important_text.txt\n",
      "output/2013_106/2013_106_important_text.txt\n",
      "output/2023_206/2023_206_important_text.txt\n",
      "output/2017_101/2017_101_important_text.txt\n",
      "output/2011_006/2011_006_important_text.txt\n",
      "output/2017_003/2017_003_important_text.txt\n",
      "output/2017_104/2017_104_important_text.txt\n",
      "output/2020_104/2020_104_important_text.txt\n"
     ]
    }
   ],
   "source": [
    "## Clean the outputs folder\n",
    "\n",
    "for report in reports:\n",
    "    files = os.listdir(os.path.join(reports_path, report))\n",
    "\n",
    "    for file in files:\n",
    "        if not re.match(fr\"{report}((.pdf)|(.txt))\", file):\n",
    "            print(os.path.join(reports_path, report, file))\n",
    "            os.remove(os.path.join(reports_path, report, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function copied from 'safety_issue_extraction.ipynb'\n",
    "def  read_or_create_important_text_file(path, report_id, text):\n",
    "    important_text_path = os.path.join(path, report_id, f'{report_id}_important_text.txt')\n",
    "\n",
    "    if not os.path.isfile(important_text_path):\n",
    "        important_text, pages = ReportExtracting.ReportExtractor(text, report_id).extract_important_text()\n",
    "\n",
    "        important_text = \"\" if important_text == None else important_text\n",
    "\n",
    "        with open(important_text_path, 'w') as stream:\n",
    "            stream.write(important_text)\n",
    "\n",
    "        return important_text\n",
    "    \n",
    "    with open(important_text_path, 'r') as stream:\n",
    "        important_text = stream.read()\n",
    "        \n",
    "    important_text = None if important_text == \"\" else important_text\n",
    "\n",
    "    return important_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No safety issues from 2010_011 as important text could not be found\n",
      " No safety issues from 2010_207 as important text could not be found\n",
      " No safety issues from 2010_205 as important text could not be found\n",
      " No safety issues from 2022_206 as important text could not be found\n",
      " No safety issues from 2016_002 as important text could not be found\n",
      " No safety issues from 2023_201 as important text could not be found\n",
      " No safety issues from 2023_206 as important text could not be found\n"
     ]
    }
   ],
   "source": [
    "# Get safety issues extracted from all reports\n",
    "importlib.reload(ReportExtracting)\n",
    "\n",
    "safety_issues = []\n",
    "\n",
    "for report in reports:\n",
    "    \n",
    "    with open(os.path.join(reports_path, report, f'{report}.txt'), 'r') as stream:\n",
    "        text = stream.read()\n",
    "\n",
    "    important_text = read_or_create_important_text_file(reports_path, report, text)\n",
    "\n",
    "    if important_text == None:\n",
    "        print(\" No safety issues from \" + report + \" as important text could not be found\")\n",
    "        continue\n",
    "\n",
    "    extracted_safety_issues = ReportExtracting.SafetyIssueExtractor(text, report)._extract_safety_issues_with_regex(important_text)\n",
    "\n",
    "    safety_issues.append({\n",
    "        'report_id': report,\n",
    "        'safety_issues': extracted_safety_issues\n",
    "    })\n",
    "\n",
    "safety_issues_df = pd.DataFrame(safety_issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reports without explicit safety issues\n",
    "safety_issues_df.dropna(subset=['safety_issues'], inplace = True)\n",
    "\n",
    "# Lengthen it so that each safety issue has its own row\n",
    "safety_issues_df = safety_issues_df.explode('safety_issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the safety issues that have non standard characters\n",
    "\n",
    "standard_characters_regex = r'''^[\\w,.\\s()'\"/\\-%]+$'''\n",
    "\n",
    "non_standard_safety_issues = safety_issues_df[~safety_issues_df['safety_issues'].str.match(standard_characters_regex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 19 non standard safety issues\n"
     ]
    }
   ],
   "source": [
    "safety_issues_df = safety_issues_df[safety_issues_df['safety_issues'].str.match(standard_characters_regex)]\n",
    "print(f\"  Removed {len(non_standard_safety_issues)} non standard safety issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the two datasets\n",
    "\n",
    "Now that I have the two data sets `safety_issues_df` and `cleaned_TAIC_recommendations_df` I can combine them together to be compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  There are 522 safety issues and recommendations.\n"
     ]
    }
   ],
   "source": [
    "# List the safety issues and the recommendations for each report\n",
    "\n",
    "combined_df = []\n",
    "\n",
    "for report in safety_issues_df['report_id'].unique():\n",
    "\n",
    "    report_safety_issues = safety_issues_df[safety_issues_df['report_id'] == report]['safety_issues']\n",
    "\n",
    "    report_recommendations = cleaned_TAIC_recommendations_df[cleaned_TAIC_recommendations_df['report_id'] == report]['recommendation_text']\n",
    "\n",
    "    for safety_issue in report_safety_issues:\n",
    "        for recommendation in report_recommendations:\n",
    "            combined_df.append({\n",
    "                'report_id': report,\n",
    "                'safety_issue': safety_issue,\n",
    "                'recommendation': recommendation\n",
    "            })\n",
    "\n",
    "combined_df = pd.DataFrame(combined_df)\n",
    "\n",
    "print(f\"  There are {len(combined_df)} safety issues and recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing link analysis\n",
    "\n",
    "Now that I have a big dataset I need to compare all the possible connections and see which ones stick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many tokens am I dealing with\n",
    "\n",
    "As I have a large data set with 500~ rows I need to know roughly how many tokens. Because this will give me a rough cost guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are about 355989 tokens in the safety issues and recommendations. \n",
      "Cost for reading all tokens is $3.56\n"
     ]
    }
   ],
   "source": [
    "# Count up the tokens in the safety issues and recommendations\n",
    "\n",
    "combined_df['safety_issue_tokens'] = combined_df['safety_issue'].apply(lambda x: sum(openAICaller.get_tokens(x)))\n",
    "combined_df['recommendation_tokens'] = combined_df['recommendation'].apply(lambda x: sum(openAICaller.get_tokens(x)))\n",
    "\n",
    "\n",
    "total_number_of_tokens = sum(combined_df['safety_issue_tokens']) + sum(combined_df['recommendation_tokens'])\n",
    "\n",
    "print(f\" There are about {total_number_of_tokens} tokens in the safety issues and recommendations. \\nCost for reading all tokens is ${round(total_number_of_tokens/1000 * 0.01, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out linking with sample\n",
    "\n",
    "I am going to simply look at 15 random reports and see how to do some comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  There are 162 safety issues and recommendations in the playset.\n"
     ]
    }
   ],
   "source": [
    "# Get the play dataset\n",
    "\n",
    "playset_reports = combined_df['report_id'].sample(10, random_state=42)\n",
    "\n",
    "combined_df_playset = combined_df[combined_df['report_id'].isin(playset_reports)]\n",
    "\n",
    "print(f\"  There are {len(combined_df_playset)} safety issues and recommendations in the playset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SI_recommendation_link(SI, recommendation):\n",
    "    response =openAICaller.query(\n",
    "        system = f\"\"\"\n",
    "You are going to help me find find links between recommendations and safety issues identified in transport accident investigation reports.\n",
    "\n",
    "Each transport accident investigation report will identify safety issues. These reports will then issue recommendation that will address one or more of the safety issues identfied in the report.\n",
    "\n",
    "For each pair given you need to respond with one of three answers.\n",
    "\n",
    "- None (The recommendation has nothing to do with the safety issue)\n",
    "- Possible (The engine thinks that it might be linked to the safety issue i.e it has inferred it.)\n",
    "- Confirmed (The safety issue and/or recommendation explicitly mention each other as being connected)\n",
    "\"\"\",\n",
    "        user = f\"\"\"\n",
    "Here is the recommendation:\n",
    "\n",
    "{recommendation}\n",
    "\n",
    "Here is the safety issue:\n",
    "\n",
    "{SI}\n",
    "\"\"\",\n",
    "        model = \"gpt-3.5\",\n",
    "        temp = 0)\n",
    "    \n",
    "    if response in ['None', 'Possible', 'Confirmed']:\n",
    "        return response\n",
    "    else:\n",
    "        print(f\"Model response is incorrect and is {response}\")\n",
    "        return 'undetermined'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5753/4160376780.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_df_playset['link'] = combined_df_playset.apply(lambda x: evaluate_SI_recommendation_link(x['safety_issue'], x['recommendation']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "combined_df_playset['link'] = combined_df_playset.apply(lambda x: evaluate_SI_recommendation_link(x['safety_issue'], x['recommendation']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link_visualization(df, report_id):\n",
    "    '''\n",
    "    Create a picture that shows both the recommendations and the safety issues fro a report. There will be arrows showing the link between the two.\n",
    "    '''\n",
    "    # Create a directed graph\n",
    "    print(\"Creating visualization\")\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    NODE_WIDTH = 50\n",
    "\n",
    "    # Preemptively wrap the text\n",
    "    df['recommendation'] = df['recommendation'].apply(lambda x: \"\\n\".join(textwrap.wrap(x, width=NODE_WIDTH)))\n",
    "    df['safety_issue'] = df['safety_issue'].apply(lambda x: \"\\n\".join(textwrap.wrap(x, width=NODE_WIDTH)))\n",
    "\n",
    "    # Add nodes for the 'extracted' and 'inferred' indices\n",
    "    for i, text in enumerate(df['recommendation'].unique()):\n",
    "        G.add_node(text, pos=(0, i))\n",
    "\n",
    "    for i, issue in enumerate(df['safety_issue'].unique()):\n",
    "        G.add_node(issue, pos=(3, i))\n",
    "\n",
    "    # Add edges between the matched indices\n",
    "    for _, row in df.iterrows():\n",
    "        # Add solid arrow\n",
    "        if row['link'] == 'Confirmed':\n",
    "            G.add_edge(row['recommendation'], row['safety_issue'], color='red', style='solid', alpha =1)\n",
    "\n",
    "        # Add dotted arrow\n",
    "        elif row['link'] == 'Possible':\n",
    "            G.add_edge(row['recommendation'], row['safety_issue'], color='blue', style='dashed', alpha = 0.5)\n",
    "\n",
    "\n",
    "    max_num_of_nodes_column = max(\n",
    "        len(df['recommendation'].unique()),\n",
    "        len(df['safety_issue'].unique())\n",
    "        )\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    plt.figure(figsize=(8, max_num_of_nodes_column * 2.5))  \n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=[len(node) * NODE_WIDTH for node in G.nodes()])\n",
    "    nx.draw_networkx_labels(G, pos, font_size=7)\n",
    "    nx.draw_networkx_edges(G, pos, arrows=True, edge_color=nx.get_edge_attributes(G, 'color').values(), style=list(nx.get_edge_attributes(G, 'style').values()), alpha = list(nx.get_edge_attributes(G, 'alpha').values()))\n",
    "\n",
    "    plt.xlim(-1, 4.5)  # Add buffer to the outside side edges\n",
    "    plt.ylim(-1, max_num_of_nodes_column*1)  # Add buffer to the outside top and bottom edges\n",
    "\n",
    "    # Add report ID and headers\n",
    "    plt.title(f'Report ID: {report_id}')\n",
    "    plt.text(0, max_num_of_nodes_column-0.2, 'Real', fontsize=12, ha='center')\n",
    "    plt.text(3, max_num_of_nodes_column-0.2, 'Inferred', fontsize=12, ha='center')\n",
    "    plt.text(1.5, max_num_of_nodes_column-0.5, 'Arrow indicates that real has been \"matched\" by LLM with inferred', fontsize=6, ha='center')\n",
    "\n",
    "    if not os.path.exists('visualization_of_links'):\n",
    "        os.mkdir('visualization_of_links')\n",
    "\n",
    "    plt.savefig(os.path.join('visualization_of_links', f'{report_id}_comparison_visualization.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualization\n",
      "Creating visualization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5753/493083936.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['recommendation'] = df['recommendation'].apply(lambda x: \"\\n\".join(textwrap.wrap(x, width=NODE_WIDTH)))\n",
      "/tmp/ipykernel_5753/493083936.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['safety_issue'] = df['safety_issue'].apply(lambda x: \"\\n\".join(textwrap.wrap(x, width=NODE_WIDTH)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualization\n",
      "Creating visualization\n",
      "Creating visualization\n",
      "Creating visualization\n",
      "Creating visualization\n",
      "Creating visualization\n",
      "Creating visualization\n"
     ]
    }
   ],
   "source": [
    "# Make lots of d which just have one sort of report_id. THen send each individual one to the make visualization function\n",
    "\n",
    "for report_id in combined_df_playset['report_id'].unique():\n",
    "    d = combined_df_playset[combined_df_playset['report_id'] == report_id]\n",
    "\n",
    "    make_link_visualization(d, report_id)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
