{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "This is going to add the content extraction of the reports form atsb and tsbi nto the engine.\n",
    "\n",
    "It carries on the work from #257 of adding pdf text extraction to the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import ggplot, geom_histogram, aes, facet_wrap, geom_bar\n",
    "\n",
    "import engine.extract.ReportExtracting as ReportExtracting\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "importlib.reload(ReportExtracting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will setup some of the global values needed for this workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '../../output'\n",
    "report_text = pd.read_pickle(os.path.join('converted_with_headers.pkl'))\n",
    "report_text['year'] = report_text['report_id'].str.extract('(\\d{4})').astype(int)\n",
    "report_text['mode'] = report_text['report_id'].str.extract('_([amr])_')\n",
    "report_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "atsb_reports = report_text[report_text['report_id'].str.contains('ATSB')]\n",
    "atsb_reports['content_section'] = atsb_reports.apply(\n",
    "    lambda x: ReportExtracting.ReportExtractor(x['text'], x['report_id']).extract_contents_section(), axis=1\n",
    ")\n",
    "atsb_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(atsb_reports['headers'].isnull() & atsb_reports['content_section'].isnull()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((atsb_reports['headers'].notna() | atsb_reports['content_section'].notna()).value_counts())\n",
    "\n",
    "failed = atsb_reports[atsb_reports['content_section'].isnull() & atsb_reports['headers'].isnull()]\n",
    "\n",
    "(\n",
    "    ggplot(atsb_reports.assign(\n",
    "        has_content_section=atsb_reports['content_section'].notna(),\n",
    "    ), aes(x='year'))\n",
    "    + geom_histogram()\n",
    "    + facet_wrap(['mode', 'has_content_section',], ncol=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATSB has two sorts of reports. Full investigation and short ones. For the short ones I will not extract the content section. The content section is only used to find the important text to instead I will just provide the wohle text if it is not too long. Most f the short rpeorts are only about 2-5k token which is really not much.\n",
    "\n",
    "For ATSB full we have 1000 that have been extracted alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atsb_reports['content_section_len'] = atsb_reports['content_section'].map(lambda x: len(x) if x is not None else 0)\n",
    "atsb_reports.sort_values('content_section_len', ascending=False, inplace=True)\n",
    "\n",
    "atsb_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at this is seems that the content section extraction is accurate. I will add some tests in. Furthermore I will look at what is missed out and see if it is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to me that it is reasonable to ignore the failed reports. Most of them are small so the whole report can just be used. For the 50 or so large failed rpeort it can be left for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSB is currently running into the issue that it doesn't have content sections for mosts of its reports. I will do ATSB first and will hopefully have a more robust content section extractor. Otherwise I will look into having itgenerate a content sections based on regex searches of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_reports = report_text[report_text['report_id'].str.startswith('TSB')]\n",
    "\n",
    "importlib.reload(ReportExtracting)\n",
    "tsb_reports['content_section'] = tsb_reports.apply(\n",
    "    lambda x: ReportExtracting.ReportExtractor(x['text'], x['report_id']).extract_contents_section(), axis=1\n",
    ")\n",
    "tsb_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = tsb_reports['content_section'].isnull() & tsb_reports['headers'].isnull()\n",
    "print(failures.value_counts())\n",
    "# Make histogram of years with two colours for content section exist and not\n",
    "failed = tsb_reports[failures]\n",
    "(\n",
    "    ggplot(tsb_reports.assign(has_content_section=tsb_reports['content_section'].notna()), aes(x='year'))\n",
    "    + geom_histogram()\n",
    "    + facet_wrap('has_content_section', ncol=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing with ATSb and we have 1077 False and only 251 true. I will work on this a bit to hopefully get the low hanging fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_reports['content_section_len'] = tsb_reports['content_section'].map(lambda x: len(x) if x is not None else 0)\n",
    "\n",
    "tsb_reports.sort_values('content_section_len', ascending=False, inplace=True)\n",
    "\n",
    "tsb_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAIC\n",
    "\n",
    "Even though it is already supported I should know the full information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taic_reports = report_text[report_text['report_id'].str.startswith('TAIC')]\n",
    "\n",
    "importlib.reload(ReportExtracting)\n",
    "taic_reports['content_section'] = taic_reports.apply(\n",
    "    lambda x: ReportExtracting.ReportExtractor(x['text'], x['report_id']).extract_contents_section(), axis=1\n",
    ")\n",
    "taic_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taic_reports['content_section'].notna().value_counts())\n",
    "\n",
    "failed = taic_reports[taic_reports['content_section'].isnull()]\n",
    "\n",
    "(\n",
    "    ggplot(taic_reports.assign(has_content_section=taic_reports['content_section'].notna()), aes(x='year'))\n",
    "    + geom_histogram()\n",
    "    + facet_wrap('has_content_section', ncol=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 87 failed and 432 true before i start making changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first test there is about 10% failed for TAIC, 50% for ATSB and 16% for TSB.\n",
    "That is just using the regular expression as it currently is.\n",
    "\n",
    "We can see that TAIC is only failing once it gets back before 2005 so that can be ignored for now. However both TSB and ATSB have a lot of missing content sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_id = \"ATSB_a_2019_030\"\n",
    "pdf_link = os.path.join(output_folder, 'report_pdfs', f\"{report_id}.pdf\")\n",
    "shutil.copy(pdf_link, f\"{report_id}.pdf\")\n",
    "text = report_text.query(f'report_id == \"{report_id}\"').text.values[0]\n",
    "with open(\"individual.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "importlib.reload(ReportExtracting)\n",
    "parsed_text = ReportExtracting.ReportExtractor(text, report_id).extract_contents_section()\n",
    "\n",
    "with open(\"individual-content-section.txt\", 'w') as f:\n",
    "    f.write(parsed_text if parsed_text is not None else '')\n",
    "\n",
    "\n",
    "for pdf in [pdf for pdf in os.listdir() if pdf.endswith('.pdf')]:\n",
    "    os.remove(pdf)\n",
    "\n",
    "\n",
    "display(parsed_text[:15])\n",
    "display(parsed_text[-15:])\n",
    "print(len(parsed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to text files parsed files to let me inspect them\n",
    "shutil.rmtree('parsed_reports/', ignore_errors=True)\n",
    "\n",
    "os.makedirs('parsed_reports/', exist_ok=True)\n",
    "for index, text, content_section in failed[['report_id', 'text', 'content_section']].sample(10, random_state=42).to_records(index=False):\n",
    "    with open(os.path.join('parsed_reports', f'{index}.txt'), 'w') as f:\n",
    "        f.write(text)\n",
    "    with open(os.path.join('parsed_reports', f'{index}_content_section.txt'), 'w') as f:\n",
    "        f.write(content_section if content_section is not None else '')\n",
    "    shutil.copy(os.path.join(output_folder, 'report_pdfs', f\"{index}.pdf\"), os.path.join('parsed_reports', f\"{index}.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed['text_len'] = failed['text'].map(len)\n",
    "failed.sort_values('text_len', ascending=False, inplace=True)\n",
    "print(failed['text_len'].describe())\n",
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(\"../../output/important_text.pkl\")['important_text'].map(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at PDF outlines\n",
    "\n",
    "As we can see about 50% of the reports dont have a dedicated content section.\n",
    "\n",
    "However we can look at more of the pdf details and the outline THis oultline coulh\\d help us aas about 2/3 of the rpeorts have an outline.\n",
    "\n",
    "This has been good and has made ATSB pretty much solved. For TSB we will just go with the default of reading the whole report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outline(outline, reader, level =1):\n",
    "    headers = []\n",
    "    if not outline:\n",
    "        return headers\n",
    "    for item in outline:\n",
    "        if isinstance(item, list):\n",
    "            headers.extend(process_outline(item, reader, level + 1))\n",
    "        else:\n",
    "            if isinstance(item['/Page'], dict) and '/StructParents' in item['/Page']:\n",
    "                headers.append({\"Title\": item[\"/Title\"], \"Level\": level, \"Page\": reader.get_destination_page_number(item)})\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "documents = []\n",
    "for pdf in [pdf for pdf in os.listdir('../../output/report_pdfs') if pdf.endswith('.pdf')]:\n",
    "    try:\n",
    "        reader = PdfReader(os.path.join('../../output/report_pdfs', pdf))\n",
    "    except:\n",
    "        continue\n",
    "    documents.append(\n",
    "        {\"report_id\": pdf[:-4], \"outline\": pd.DataFrame(process_outline(reader.outline, reader))}\n",
    "    )\n",
    "\n",
    "outline_df = pd.DataFrame(documents)\n",
    "\n",
    "outline_df.set_index('report_id', inplace=True)\n",
    "\n",
    "outline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_df['agency'] = outline_df.index.str.split('_').map(lambda x: x[0])\n",
    "\n",
    "outline_df['has_outline'] = outline_df['outline'].map(len) > 0\n",
    "\n",
    "(\n",
    "    ggplot(outline_df, aes(x='agency', fill='has_outline'))\n",
    "    + geom_bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = outline_df.loc[\"ATSB_a_2019_030\"]['outline']\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    (0, ''),\n",
    "    (1, ''),\n",
    "    (2, ''),\n",
    "    (3, '1'),\n",
    "    (4, '2'),\n",
    "    (5, '3'),\n",
    "    (6, '4'),\n",
    "    (7, '5'),\n",
    "    (8, '6'),\n",
    "]\n",
    "\n",
    "\n",
    "test = pd.DataFrame(test).set_index(0)\n",
    "\n",
    "headers['Page'] = headers['Page'].map(lambda x: test.loc[x][1])\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out full pdf parsing\n",
    "\n",
    "import engine.gather.PDFParser as PDFParser\n",
    "importlib.reload(PDFParser)\n",
    "\n",
    "PDFParser.convertPDFToText('../../output/report_pdfs', 'converted_with_headers.pkl', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_headers(row):\n",
    "    shutil.copy(os.path.join('../../output/report_pdfs', f\"{row['report_id']}.pdf\"), os.path.join('headers', f\"{row['report_id']}.pdf\"))\n",
    "    print(row['report_id'])\n",
    "    display(row['headers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_text = pd.read_pickle('../../output/parsed_reports.pkl')\n",
    "display(converted_text)\n",
    "converted_text[converted_text['headers'].notnull()].sample(10, random_state=42).apply(inspect_headers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "converted_text['content_section'] = [\n",
    "    ReportExtracting.ReportExtractor(x[1]['text'], x[1]['report_id'], x[1]['headers']).extract_contents_section()\n",
    "    for x\n",
    "    in converted_text.iterrows()\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_text[converted_text['content_section'].map(lambda x: isinstance(x, pd.DataFrame))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the content section is only used to extract the important text. This important text is then used to extract the safety issues.\n",
    "Moving forward we can just use the entire report as most of them are not that long. Given that TAIC average important length is 22222 characters we can just do the same for the failed ones as they are on average only 24000 characters."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
