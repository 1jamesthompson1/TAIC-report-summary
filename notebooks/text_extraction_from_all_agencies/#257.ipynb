{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in PDF parsing of text for other agencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "from engine.gather import PDFParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_file_name = 'parsed_reports.pkl'\n",
    "report_dir = '../../output/report_pdfs'\n",
    "importlib.reload(PDFParser)\n",
    "reports = []\n",
    "parsed_pdfs = pd.read_pickle(parsed_file_name)\n",
    "for pdf in os.listdir(report_dir):\n",
    "    report_id = pdf[:-4]\n",
    "    if not pdf.endswith('.pdf'):\n",
    "        continue\n",
    "    if not parsed_pdfs.query('report_id == @report_id').empty:\n",
    "        continue\n",
    "    try:\n",
    "        text = PDFParser.extractTextFromPDF(os.join(report_dir, pdf))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {pdf}: {e}\")\n",
    "        continue\n",
    "    reports.append({\n",
    "        'report_id': report_id,\n",
    "        'text': text\n",
    "    })\n",
    "\n",
    "parsed_pdfs = pd.concat([parsed_pdfs, pd.DataFrame(reports)], ignore_index=True)\n",
    "parsed_pdfs.to_pickle('parsed_reports.pkl')\n",
    "print(f\"There are {len(parsed_pdfs)} reports in total out of {len(os.listdir(report_dir))}\")\n",
    "parsed_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pdfs = pd.read_pickle(parsed_file_name)\n",
    "parsed_pdfs.sort_values(by='report_id', inplace=True)\n",
    "parsed_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first problem is that ATSb and TAIC have some duplicated reports. We dont want to have duplicated doucments inside the searcher. So for now it will be best if we just through away all of the occurance reports and just keep the regualr reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = parsed_pdfs['text'].value_counts()\n",
    "deduped_pdfs = parsed_pdfs[~parsed_pdfs['text'].isin(counts[counts > 3].index)]\n",
    "\n",
    "deduped_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATSB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the parsing the PDFs need to have readable text as well as identifiable page numbers.\n",
    "\n",
    "I will start by having a look at the different strcutures that are present\n",
    "\n",
    "**aviation**\n",
    "2000 - ATSB_a_2001_710\n",
    "Really basic without a content section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running new parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all report IDs that start with ATSB*\n",
    "importlib.reload(PDFParser)\n",
    "atsb_text = parsed_pdfs[parsed_pdfs['report_id'].str.startswith('ATSB')].reset_index(drop=True)\n",
    "atsb_text[['text', 'valid_page_numbers']] = [\n",
    "    PDFParser.formatText(PDFParser.cleanText(text), report_id) for report_id, text, _ in atsb_text.to_records(index=False)]\n",
    "atsb_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to see if page numbers are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = atsb_text[atsb_text['valid_page_numbers'] == False]\n",
    "failed['year'] = failed['report_id'].map(lambda x: int(x[7:11]))\n",
    "\n",
    "atsb_text.valid_page_numbers.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "65/len(atsb_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(PDFParser)\n",
    "tsb_text = parsed_pdfs[parsed_pdfs['report_id'].str.startswith('TSB')].reset_index(drop=True)\n",
    "tsb_text[['text', 'valid_page_numbers']] = [\n",
    "    PDFParser.formatText(PDFParser.cleanText(text), report_id) for report_id, text, _ in tsb_text.to_records(index=False)]\n",
    "tsb_text['year'] = tsb_text['report_id'].map(lambda x: int(x[6:10]))\n",
    "tsb_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = tsb_text[tsb_text['valid_page_numbers'] == False]\n",
    "\n",
    "tsb_text.valid_page_numbers.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = parsed_pdfs.query('report_id == \"TSB_a_2011_F0012\"').text.values[0]\n",
    "with open(\"individual.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "importlib.reload(PDFParser)\n",
    "parsed_text, _ = PDFParser.formatTSBText(text, \"test\")\n",
    "\n",
    "with open(\"individual-parsed.txt\", 'w') as f:\n",
    "    f.write(parsed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAIC\n",
    "\n",
    "I should also move over taic to this system then I can merge them togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(PDFParser)\n",
    "taic_text = parsed_pdfs[parsed_pdfs['report_id'].str.startswith('TAIC')].reset_index(drop=True)\n",
    "taic_text[['text', 'valid_page_numbers']] = [\n",
    "    PDFParser.formatText(PDFParser.cleanText(text), report_id) for report_id, text in taic_text.to_records(index=False)]\n",
    "taic_text['year'] = taic_text['report_id'].map(lambda x: int(x[7:11]))\n",
    "taic_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = taic_text[taic_text['valid_page_numbers'] == False]\n",
    "\n",
    "taic_text.valid_page_numbers.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to text files parsed files to let me inspect them\n",
    "shutil.rmtree('parsed_reports/', ignore_errors=True)\n",
    "\n",
    "os.makedirs('parsed_reports/', exist_ok=True)\n",
    "for index, text in failed[['report_id', 'text']].sample(10, random_state=42).to_records(index=False):\n",
    "    with open(os.path.join('parsed_reports', f'{index}.txt'), 'w') as f:\n",
    "        f.write(text)\n",
    "    shutil.copy(os.path.join(report_dir, f\"{index}.pdf\"), os.path.join('parsed_reports', f\"{index}.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_id = \"TAIC_r_2004_103\"\n",
    "text = parsed_pdfs.query(f'report_id == \"{report_id}\"').text.values[0]\n",
    "with open(\"individual.txt\", \"w\") as f:\n",
    "    f.write(text)\n",
    "importlib.reload(PDFParser)\n",
    "parsed_text, _ = PDFParser.formatText(text, report_id)\n",
    "\n",
    "with open(\"individual-parsed.txt\", 'w') as f:\n",
    "    f.write(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFParser.convertPDFToText(report_dir, '../../output/parsed_reports.pkl', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_pdfs = pd.read_pickle('../../output/parsed_reports.pkl')\n",
    "value_counts = all_processed_pdfs.valid.value_counts()\n",
    "print(f\"{value_counts.iloc[1] / (value_counts.iloc[0] + value_counts.iloc[1]) * 100:.2f}% of the pdfs are invalid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test pdfs\n",
    "\n",
    "test_pdfs = [\n",
    "    \"ATSB_r_2021_010\",\n",
    "    \"ATSB_r_2021_004\",    \n",
    "    \"ATSB_a_2007_030\",\n",
    "    \"ATSB_a_2002_646\",\n",
    "    \"TSB_a_2022_O0118\",\n",
    "    \"TSB_m_2021_A0041\",\n",
    "    \"TSB_a_2011_F0012\",\n",
    "    \"TAIC_r_2014_103\",\n",
    "    \"TAIC_r_2004_121\",\n",
    "    \"TAIC_a_2019_006\",\n",
    "]\n",
    "\n",
    "test_report_dfs = '../../tests/data/output/report_pdfs/' \n",
    "\n",
    "shutil.rmtree(test_report_dfs, ignore_errors=True)\n",
    "os.mkdir(test_report_dfs)\n",
    "for report_id in test_pdfs:\n",
    "    shutil.copy(os.path.join(report_dir, f'{report_id}.pdf'), os.path.join(test_report_dfs, f'{report_id}.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems found with the extraction\n",
    "\n",
    "Some of the reports cant be extracted and need to be repaired with their page names.\n",
    "\n",
    "I will move forward and potentially add the page numbers repairer later:\n",
    "\n",
    "| report | problem |\n",
    "| --- | --- |\n",
    "| ATSB_a_2001_348 | Not matching any of the roman numerals. Faulty first match of int. |\n",
    "| ATSB_a_2000_157 | No good regex matches of page numbers. Only matching roman numerals. |\n",
    "| ATSB_a_2002_328 | There is a random 65 that messes up the order when being filled in. By checkign the number of pages and the suggested amount it coul be fixed. |\n",
    "| ATSB_a_2008_052 | There is a missing roman numeral in the report and so it is causing the incorrect labelign of pages as the auto fill only works off anchors. | \n",
    "| ATSB_a_2023_012 | Not handling case of duplicate page number correctly. The solution should have the most amount of pages labelled correctly. |\n",
    "| ATSB_m_2004_203 | Early mistake in regex matching causes rest of page numbers to be off. Having some tier of a match and its quality could help fix this one by working from the back down. |\n",
    "| ATSB_m_2004_201 | Matching random non roman numerals. This could be fixed by counting the length of the pdf and fixing it. |\n",
    "| ATSB_r_2003_005 | Same as above. Could filter out page nubmers that imply a document that is too long. |\n",
    "| ATSB_r_2006_010 | Same as above |\n",
    "| ATSB_r_2021_002 | Not handling duplicate roman numerals correctly. | \n",
    "| ATSB_m_2022_001 | Not handling duplicate page numbers correctly. |\n",
    "| ATSB_r_2021_002 & ATSB_m_2021_001 & ATSB_a_2022_009 & ATSB_a_2022_007 & ATSB_a_2022_001 | Appendices are messing up the page numbers. |\n",
    "| ATSB_a_2022_068 | Need to match as many as possible page numbers correctly |\n",
    "| TAIC_a_2019_006 | The page one match is off as there is a mssing space. This could be fixed by looking at the later page matches. |\n",
    "| TAIC_m_2009_203 | The roman numerals has an error in the pdf. The raw matches are more acurate and it only fails when it does the syncing. Maybe cancel out the syncing if it is already valid? |\n",
    "| TAIC_r_2004_103 | This one has problems where there is internal error in the numbering. Therefore the syncing makes it less accurate then the simiple regex matching. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
