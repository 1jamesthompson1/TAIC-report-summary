{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "Now that #265 is complete I need to make sure that important text can be extracted from each of the reports.\n",
    "\n",
    "This will be done using the content section if present. The content section will be read and then the page numbers will be extracted. If the content section is not present or not useful then the entire rpeort up to 30_000 tokens will be used.\n",
    "\n",
    "Note that running this notebook has some costs due to the API calls for a LLM to read the content page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "import engine.extract.ReportExtracting as ReportExtracting\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "importlib.reload(ReportExtracting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content sections\n",
    "\n",
    "Currently the `reportExtractor.get_important_text()` will return the important text from the report given the content section, or pdf headers are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many reports have a content section\n",
    "\n",
    "parsed_reports = pd.read_pickle('../../output/parsed_reports.pkl')\n",
    "\n",
    "parsed_reports['agency'] = parsed_reports['report_id'].map(lambda x: x.split('_')[0])\n",
    "\n",
    "parsed_reports['content_section'] = parsed_reports.apply(lambda x: ReportExtracting.ReportExtractor(x['text'], x['report_id'], x['headers']).extract_contents_section(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reports that have a content section\")\n",
    "display(parsed_reports['content_section'].notna().value_counts())\n",
    "\n",
    "print(f\"What are the lengths of these content sections (both characters and tokens)\")\n",
    "display(parsed_reports['content_section'].dropna().map(len).describe())\n",
    "encoder = tiktoken.encoding_for_model('gpt-4o')\n",
    "encoded_content_sections = parsed_reports['content_section'].dropna().map(lambda x: len(encoder.encode(x))) \n",
    "display(encoded_content_sections.describe())\n",
    "print(f\"Total cost to read ({encoded_content_sections.sum()} tokens): USD ${encoded_content_sections.sum() * 0.15 / 1_000_000}\\n\")\n",
    "\n",
    "print(f\"Which content sections have come from PDF headers\")\n",
    "display(parsed_reports['content_section'].dropna().map(lambda x: True if re.search(r'^\\s+Title  Level', x) else False).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed_reports['content_section'].dropna().loc[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about half of the content sections coming from pdf headers and the other half coming from the text itself.\n",
    "\n",
    "Furthermore I expect that quite a few of the pdf headers are not actually useful (like above). It will be up to the LLM to decide if it is a relevant table of contents or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "parsed_reports['important_text'] = parsed_reports.progress_apply(lambda x: ReportExtracting.ReportExtractor(x['text'], x['report_id'], x['headers']).extract_important_text(), axis=1)\n",
    "parsed_reports.to_pickle('important_text.pkl')\n",
    "parsed_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reports['important_text'].map(lambda x: isinstance(x[0], str)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial results with just content section extraction was 1865 found and 1757 without an important text section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_to_extract_pages = parsed_reports[parsed_reports['important_text'].map(lambda x: not isinstance(x[0], str) and isinstance(x[1], list))]\n",
    "\n",
    "failed_completely = parsed_reports[~parsed_reports['important_text'].map(lambda x: isinstance(x[0], str) or isinstance(x[1], list))]\n",
    "\n",
    "failed_completely_with_content_section = parsed_reports[parsed_reports['content_section'].notna() & ~parsed_reports['important_text'].map(lambda x: isinstance(x[0], str) or isinstance(x[1], list))]\n",
    "\n",
    "print(f\"Failed to extract pages: {len(failed_to_extract_pages)}\\nFailed completely: {len(failed_completely)}\\nFailed completely with content section: {len(failed_completely_with_content_section)}\")\n",
    "\n",
    "print(\"How successful is the pdf headers at being a content section\")\n",
    "parsed_reports.dropna(subset=['content_section'])[parsed_reports['content_section'].dropna().map(lambda x: True if re.search(r'^\\s+Title  Level', x) else False)]['important_text'].map(lambda x: isinstance(x[0], str)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = parsed_reports['important_text'].map(lambda x: len(encoder.encode(x[0])) if isinstance(x[0], str) else 0)\n",
    "print(f\"Total cost to read ({encoded.sum()} tokens): USD ${encoded.sum() * 2.5 / 1_000_000:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Important text lengths followed by full text lengths\")\n",
    "\n",
    "display(encoded.describe())\n",
    "display(parsed_reports['text'].dropna().map(lambda x: len(encoder.encode(x))).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a look at the failed with content section they are either failing because the pdf headers are not good enough (i.e from a short report) or they are failing because there are a short investigation summary report.\n",
    "\n",
    "There are 940 TSB reports failing with 796 ATSB, only 11 are missing from TAIC. However this changes and we have 651 ATSB failing with only 311 TSB once we are only looking at the ones that have a content section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the generated important text is fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reports = pd.read_pickle(\"important_text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reports[['important_text', 'pages_read']] = parsed_reports['important_text'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reports['full_text'] = parsed_reports.progress_apply(lambda x: x['text'] == x['important_text'], axis=1)\n",
    "\n",
    "parsed_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = parsed_reports.query('full_text == False').query('agency != \"TAIC\"').sample(frac=0.1, random_state=45)\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "index = 7\n",
    "print(test_sample.iloc[index]['report_id'])\n",
    "# headers = test_sample.iloc[index]['headers']\n",
    "# print(\n",
    "# headers.assign(Page=headers['Page'].replace('', 0)).to_string(index=False)\n",
    "# )\n",
    "print(\n",
    "    test_sample.iloc[index]['content_section']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "test_sample[['important_text_new', 'pages_read_new']] = test_sample.progress_apply(lambda x: ReportExtracting.ReportExtractor(x['text'], x['report_id'], x['headers']).extract_important_text(), axis=1).apply(pd.Series)\n",
    "\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample[['important_text', 'important_text_new']].map(lambda x: len(x) if isinstance(x, str) else 0).describe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
