{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "After initial safety issue extraction was completed some time ago (https://github.com/1jamesthompson1/TAIC-report-summary/pull/176).\n",
    "\n",
    "I will expand this to include safety issue extraction for ATSB and TSB.\n",
    "\n",
    "This currently works by having a LLM read the important text. THen parsing the repsonse into a workable format. The important text has been added for ATSB and TSB #266.\n",
    "\n",
    "Note that ATSB actually has a safety issue dataset that goes back till 2010. So for all of their safety issues they have exact extract and dont need to have hte reports read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import engine.extract.ReportExtracting as ReportExtracting \n",
    "import engine.gather.WebsiteScraping as WebsiteScraping\n",
    "import pandas as pd\n",
    "\n",
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(WebsiteScraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATSB\n",
    "\n",
    "\n",
    "As mentioned above ATSB has a only dataset of the safety issues that they have identified.\n",
    "\n",
    "This means that I need to set up a scraper of this. I do also need to make sure that the pre 2010 reports also function alright.\n",
    "\n",
    "\n",
    "To keep with everything else I will add the safety issue dataset scrapping to the `WebScraping.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('../../output/safety_issues.pkl').iloc[0]['safety_issues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(WebsiteScraping)\n",
    "scraper = WebsiteScraping.ATSBSafetyIssueScraper('atsb_safety_issues.pkl', refresh=True)\n",
    "scraper.extract_safety_issues_from_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_pickle('atsb_safety_issues.pkl')\n",
    "new_df.drop_duplicates(subset='safety_issue_id', inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[~new_df['safety_issue_id'].isin(big_df['safety_issue_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scraping works. However it does not scrape the same amount each time. There are a varying amount +- 10 rows. This is quite weird."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
