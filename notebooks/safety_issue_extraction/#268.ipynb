{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "After initial safety issue extraction was completed some time ago (https://github.com/1jamesthompson1/TAIC-report-summary/pull/176).\n",
    "\n",
    "I will expand this to include safety issue extraction for ATSB and TSB.\n",
    "\n",
    "This currently works by having a LLM read the important text. THen parsing the repsonse into a workable format. The important text has been added for ATSB and TSB #266.\n",
    "\n",
    "Note that ATSB actually has a safety issue dataset that goes back till 2010. So for all of their safety issues they have exact extract and dont need to have hte reports read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import engine.extract.ReportExtracting as ReportExtracting \n",
    "import engine.gather.WebsiteScraping as WebsiteScraping\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(WebsiteScraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website scraping\n",
    "\n",
    "\n",
    "As mentioned above ATSB has a only dataset of the safety issues that they have identified.\n",
    "\n",
    "This means that I need to set up a scraper of this. I do also need to make sure that the pre 2010 reports also function alright.\n",
    "\n",
    "\n",
    "To keep with everything else I will add the safety issue dataset scrapping to the `WebScraping.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(WebsiteScraping)\n",
    "atsb_safety_issues_path = os.path.join(output_path, 'atsb_safety_issues.pkl')\n",
    "scraper = WebsiteScraping.ATSBSafetyIssueScraper(atsb_safety_issues_path, refresh=True)\n",
    "scraper.extract_safety_issues_from_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atsb_webscraped_safety_issues = pd.read_pickle(atsb_safety_issues_path)\n",
    "atsb_webscraped_safety_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(atsb_webscraped_safety_issues['safety_issues'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scraping works. However it does not scrape the same amount each time. There are a varying amount +- 10 rows. This is quite weird.\n",
    "\n",
    "I will move on from now and might come back to it at another point https://github.com/1jamesthompson1/TAIC-report-summary/issues/277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report extraction\n",
    "\n",
    "For the pre 2010 reports for ATSb and all fo the TSB reports I will need to extract the safety issues by reading the important text.\n",
    "\n",
    "Because it is going to be quite expensive I will take a sample\n",
    "\n",
    "Then I can start building some tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_text_df_path = os.path.join(output_path, 'important_text.pkl')\n",
    "important_text_df = pd.read_pickle(important_text_df_path)\n",
    "important_text_df['year'] = important_text_df['report_id'].map(lambda x: x.split(\"_\")[2])\n",
    "important_text_df['agency'] = important_text_df['report_id'].map(lambda x: x.split(\"_\")[0])\n",
    "important_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles_path = os.path.join(output_path, 'report_titles.pkl')\n",
    "report_titles = pd.read_pickle(report_titles_path)\n",
    "report_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reports_path = os.path.join(output_path, 'parsed_reports.pkl')\n",
    "parsed_reports = pd.read_pickle(parsed_reports_path)\n",
    "parsed_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(important_text_df, report_titles, how='outer', on='report_id')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df.dropna(subset=['important_text'])\n",
    "filtered_df = filtered_df[filtered_df['investigation_type'] != 'short']\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much will it cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.encoding_for_model('gpt-4o')\n",
    "def cost_to_read(df):\n",
    "    tokens = df['important_text'].map(lambda x: len(encoder.encode(x)))\n",
    "\n",
    "    return tokens.sum() * 2.5 / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_to_read(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will cost $54.10usd which is about 90 nzd. Thjerefore we will use a sample set that is small enough so that it costs a small amount to do a full extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_important_text = filtered_df.sample(frac=0.01, random_state=45, ignore_index=True)\n",
    "sample_df_path = 'sample_important_text.pkl'\n",
    "sample_important_text.to_pickle(sample_df_path)\n",
    "for _, id,sample_text in sample_important_text[['report_id', 'important_text']].itertuples():\n",
    "    shutil.copy(f'../../output/report_pdfs/{id}.pdf', f'sample/{id}.pdf')\n",
    "    with open(f'sample/{id}_important.txt', 'a') as f:\n",
    "        f.write(sample_text)\n",
    "sample_important_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_safety_from_df(df):\n",
    "    temp_df_path = 'temp_df.pkl'\n",
    "    df.to_pickle(temp_df_path)\n",
    "    parsed_reports[parsed_reports['report_id'].isin(df['report_id'])].to_pickle('sample_parsed_reports.pkl')\n",
    "    report_titles[report_titles['report_id'].isin(df['report_id'])].to_pickle('sample_report_titles.pkl')\n",
    "\n",
    "    importlib.reload(ReportExtracting)\n",
    "    processor = ReportExtracting.ReportExtractingProcessor('sample_parsed_reports.pkl', refresh=True)\n",
    "    \n",
    "    processor.extract_safety_issues_from_reports(temp_df_path, 'sample_report_titles.pkl', atsb_safety_issues_path, 'sample_safety_issues.pkl')\n",
    "    safety_issues_df = pd.read_pickle('sample_safety_issues.pkl')\n",
    "    safety_issues_df = safety_issues_df[~safety_issues_df['report_id'].isin(atsb_webscraped_safety_issues['report_id'])]\n",
    "    os.remove(temp_df_path)\n",
    "    return safety_issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_issues_df = extract_safety_from_df(sample_important_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATSB\n",
    "\n",
    "ATSB ha a concept of investigation level. That is that some investigation are full investigations and some are short investigations.\n",
    "\n",
    "I am not sure if these short invetigations actually have safety issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atsb_reports = merged_df.query('agency == \"ATSB\"')\n",
    "atsb_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atsb_reports['important_text_len'] = atsb_reports['important_text'].map(len)\n",
    "atsb_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atsb_reports['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSB\n",
    "\n",
    "As there are only 7 years (2000-2007) that are not already included in the safety issues dataset for ATSB it seems more important to do the safety issue extraction for TSB.\n",
    "\n",
    "TSB has the concept class of investigation. It goes from 6-1. With 1 being the most important.\n",
    "More information can be found lower down on this page: https://www.tsb.gc.ca/eng/lois-acts/evenements-occurrences.html\n",
    "\n",
    "Class 6 are for external investigations and class 1 are for thematic investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start scrapng the class occuracne from the webpages of the reports by adding the metadata requests to the TSB scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hrequests\n",
    "from bs4 import BeautifulSoup\n",
    "import importlib\n",
    "\n",
    "import engine.utils.Modes as Modes\n",
    "import engine.gather.WebsiteScraping as WebsiteScraping\n",
    "\n",
    "importlib.reload(WebsiteScraping)\n",
    "\n",
    "pdf_page = hrequests.get(\n",
    "   \"https://www.tsb.gc.ca/eng/rapports-reports/aviation/2012/a12w0004/a12w0004.html\" \n",
    ")\n",
    "soup = BeautifulSoup(pdf_page.text, 'html.parser')\n",
    "\n",
    "report_id = \"TSB_a_2012_w0004\"\n",
    "\n",
    "scraper = WebsiteScraping.TSBReportScraper(WebsiteScraping.ReportScraperSettings(\n",
    "    \"../../output/report_pdfs/\", \"../../output/report_titles.pkl\", \"{{rpeort_id}}.pdf\", 2010, 2020, 1000, Modes.all_modes, [], False\n",
    "))\n",
    "\n",
    "scraper.get_report_metadata(report_id, soup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_reports = merged_df.query('agency == \"TSB\"')\n",
    "tsb_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_reports['text_length'] = tsb_reports['important_text'].str.len()\n",
    "tsb_reports['investigation_type'] = tsb_reports.apply(\n",
    "    lambda row:\n",
    "        row['investigation_type'] if row['investigation_type'] != 'unknown' else 'full' if isinstance(row['pages_read'], list) else 'short' if row['text_length'] < 40_000 else 'full',\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing TSB sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_sample = tsb_reports.sample(frac=0.02, random_state=42)\n",
    "tsb_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_sample_safety_issues = extract_safety_from_df(tsb_sample)\n",
    "tsb_sample_safety_issues.to_pickle('tsb_sample_si.pkl')\n",
    "tsb_sample_safety_issues.set_index('report_id', inplace=True)\n",
    "tsb_sample_safety_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('tsb_sample')\n",
    "if not os.path.exists('tsb_sample'):\n",
    "    os.mkdir('tsb_sample')\n",
    "for _, id,sample_text, url in tsb_sample[['report_id', 'important_text', 'url']].itertuples():\n",
    "    with open(f'tsb_sample/{id}_important.txt', 'a') as f:\n",
    "        f.write(sample_text)\n",
    "    try:\n",
    "        shutil.copy(f'../../output/report_pdfs/{id}.pdf', f'tsb_sample/{id}.pdf')\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        with open(f'tsb_sample/{id}_si.txt', 'a') as f:\n",
    "            f.write(\"\\n\\n\".join(tsb_sample_safety_issues.loc[id]['safety_issues']['safety_issue']))\n",
    "    except KeyError:\n",
    "        print(f\"{id} has no safety issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_sample_safety_issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick adding in of investigation levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = pd.read_pickle('../../output/report_titles.pkl')\n",
    "reports['agency'] = reports['report_id'].map(lambda x: x.split('_')[0])\n",
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atsb = reports.query('agency == \"ATSB\"')\n",
    "atsb['investigation_type'] = atsb['misc'].map(lambda x: \"full\" if x['investigation_level'] in [\"Defined\", \"Systemic\"] else \"short\" if x['investigation_level'] == \"Short\" else \"unknown\")\n",
    "atsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taic = reports.query('agency == \"TAIC\"')\n",
    "taic.loc[:,'investigation_type'] = \"full\"\n",
    "taic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb = reports.query('agency == \"TSB\"')\n",
    "tsb['investigation_type'] = tsb['misc'].map(lambda x: \"unknown\" if x['investigation_class'] is None else \"full\" if int(x['investigation_class']) < 4 else \"short\")\n",
    "tsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_out = pd.concat([atsb, taic])[['report_id', 'title', 'event_type', 'investigation_type', 'misc']].reset_index(drop=True)\n",
    "reports_out['url'] = None\n",
    "reports_out.to_pickle('../../output/report_titles.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
