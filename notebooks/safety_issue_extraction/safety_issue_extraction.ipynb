{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting safety issues from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem introduction\n",
    "Safety issues are being extracted from the reports.\n",
    "\n",
    "This is linked with issue https://github.com/1jamesthompson1/TAIC-report-summary/issues/101\n",
    "\n",
    "There are two types of safety issues being extracted.\n",
    "Firstly are exact extraction. These are safety issues that are explicitly mentioned in the report. They will follow something like \"safety issue: ...\" or \"safety issue - ...\". Exact safety issue extraction is not too much of a worry and just needs to be tested and could be done so with unit tests.\n",
    "\n",
    "Secondly however are inferred safety issues. These safety issues are supposedly present but are not explicitly stated. This requires a little bit more validation then a simple unit test. Fortunately I can get a good test set.\n",
    "\n",
    "All the reports that have the safety issues explicitly mentioned will show me what the correct answer is. So all I need to do is remove the explicitly mention of the safety issue from the report then I can run it as an inferred and see if it comes up with the same safety issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution story\n",
    "\n",
    "I have peformed some testing on fine-tuning gpt 3.5 turbo.\n",
    "\n",
    "It was about 56% accurate on the training set (about 15 reports)\n",
    "However on a testing set it was only 17% accurate. This was becuase about 70% of the items had mismatched number of safety issues extracted.\n",
    "\n",
    "This means that I wanted to try with more data. There are about 70 reports that I have meaning I can split it between a training, validation and test set. This is what the code does below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required modules\n",
    "\n",
    "Some cells reload modules \n",
    "There are potentially new modules that are loaded in cells later on but I do like to have something at the top so here it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'engine.Extract_Analyze.ReportExtracting' from '/home/james/code/TAIC-report-summary/engine/Extract_Analyze/ReportExtracting.py'>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "import importlib\n",
    "from engine.Extract_Analyze import ReportExtracting\n",
    "import engine.OpenAICaller as OpenAICaller\n",
    "importlib.reload(ReportExtracting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets for working with\n",
    "\n",
    "To get the dataset you need I just pulled it from the viewer/output folder.\n",
    "\n",
    "This has all of the reports downloaded and extracted. From there everything else is done in the notebook.\n",
    "\n",
    "Here is a link to the exact copy of the output folder I used: https://github.com/1jamesthompson1/TAIC-report-summary/tree/48e3b09380364731bfb343f2105851cd3babf90a/viewer/output\n",
    "I have not included it in the repo to save space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  read_or_create_important_text_file(path, report_id, text):\n",
    "    important_text_path = os.path.join(path, report_id, f'{report_id}_important_text.txt')\n",
    "\n",
    "    if not os.path.isfile(important_text_path):\n",
    "        important_text, pages = ReportExtracting.ReportExtractor(text, report_id).extract_important_text()\n",
    "\n",
    "        important_text = \"\" if important_text == None else important_text\n",
    "\n",
    "        with open(important_text_path, 'w') as stream:\n",
    "            stream.write(important_text)\n",
    "\n",
    "        return important_text\n",
    "    \n",
    "    with open(important_text_path, 'r') as stream:\n",
    "        important_text = stream.read()\n",
    "        \n",
    "    important_text = None if important_text == \"\" else important_text\n",
    "\n",
    "    return important_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text_and_get_safety_issues(path, reports_ids):\n",
    "    \"\"\"\n",
    "    This will go through a folder and extract all of the safety issues that it can from the reports using regex. Then it will also remove those extracted safety issues from the report text and report a DataFrame that has the report_id, the safety issues extracted, and the cleaned report text.\n",
    "    \"\"\"\n",
    "    safety_issues_raw = []\n",
    "\n",
    "    for report in reports_ids:\n",
    "        with open(os.path.join(path, report, f'{report}.txt'), 'r') as stream:\n",
    "            text = stream.read()\n",
    "\n",
    "        # Check to see if the important text exists\n",
    "\n",
    "        important_text = read_or_create_important_text_file(path, report, text)\n",
    "\n",
    "        if important_text == None:\n",
    "            print(f'Could not extract important text from {report}')\n",
    "            continue\n",
    "        \n",
    "        safety_regex = lambda x: fr's ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s? {{0,3}}{x} {{0,3}}'\n",
    "        end_regex = r'([\\s\\S]+?)(?=(?:\\d+\\.(?:\\d+\\.)?(?:\\d+)?)|(?:^ [A-Z])|(?:s ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s?))'\n",
    "\n",
    "        uncompiled_regexes = [\"(\" + safety_regex(sep) + end_regex + \")\" for sep in [\"-\", \":\"]]\n",
    "\n",
    "        safety_issue_regexes = [re.compile(regex , re.MULTILINE | re.IGNORECASE) for regex in uncompiled_regexes]\n",
    "\n",
    "        safety_issues_for_report = []\n",
    "\n",
    "        cleaned_text = text\n",
    "\n",
    "        matches = [regex.findall(important_text) for regex in safety_issue_regexes]\n",
    "\n",
    "        # Choose one of the matches that has the most matches\n",
    "        matches = max(matches, key=lambda x: len(x))\n",
    "\n",
    "        for full_match, safety_issue_match in matches:\n",
    "            safety_issues_for_report.append(safety_issue_match)\n",
    "            cleaned_text = re.sub(\n",
    "                full_match, '',\n",
    "                cleaned_text,\n",
    "                flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "        safety_issues_raw.append({\n",
    "            \"file\": report,\n",
    "            \"safety_issues\": safety_issues_for_report,\n",
    "            'cleaned_report': cleaned_text})\n",
    "\n",
    "    safety_issues_df = pd.DataFrame(safety_issues_raw)\n",
    "\n",
    "    safety_issues_df['number_extracted'] = safety_issues_df['safety_issues'].apply(lambda x: len(x))\n",
    "\n",
    "    return safety_issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting important text... for 2019_106\n",
      "  I am going to be reading these pages: [6, 12]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 12 >>\n",
      "Getting important text... for 2013_107\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2011_003\n",
      "  I am going to be reading these pages: [14, 31]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 31 >>\n",
      "Getting important text... for 2012_105\n",
      "  I am going to be reading these pages: [11, 16]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2019_202\n",
      "  I am going to be reading these pages: [8, 13]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 13 >>\n",
      "Getting important text... for 2010_010\n",
      "  I am going to be reading these pages: [23, 31]\n",
      "  searching with << Page 23 >>[\\s\\S]*<< Page 31 >>\n",
      "Getting important text... for 2013_011\n",
      "  I am going to be reading these pages: [8, 10]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 10 >>\n",
      "Getting important text... for 2015_103\n",
      "  I am going to be reading these pages: [7, 12]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 12 >>\n",
      "Getting important text... for 2019_201\n",
      "  I am going to be reading these pages: [18, 23]\n",
      "  searching with << Page 18 >>[\\s\\S]*<< Page 23 >>\n",
      "Getting important text... for 2013_006\n",
      "  I am going to be reading these pages: [12, 19]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2010_009\n",
      "  I am going to be reading these pages: [25, 37]\n",
      "  searching with << Page 25 >>[\\s\\S]*<< Page 37 >>\n",
      "Getting important text... for 2019_204\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2010_007\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 36]\n",
      "  searching with << Page 25 >>[\\s\\S]*<< Page 36 >>\n",
      "Getting important text... for 2015_202\n",
      "  I am going to be reading these pages: [9, 17]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2010_101\n",
      "  I am going to be reading these pages: [17, 29]\n",
      "  searching with << Page 17 >>[\\s\\S]*<< Page 29 >>\n",
      "Getting important text... for 2016_206\n",
      "  I am going to be reading these pages: [14, 23]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 23 >>\n",
      "Getting important text... for 2010_202\n",
      "  I am going to be reading these pages: [11, 17]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2011_001\n",
      "  I am going to be reading these pages: [14, 18]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 18 >>\n",
      "Getting important text... for 2012_001\n",
      "  I am going to be reading these pages: [29, 42]\n",
      "  searching with << Page 29 >>[\\s\\S]*<< Page 42 >>\n",
      "Getting important text... for 2016_006\n",
      "  I am going to be reading these pages: [13, 19]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2019_203\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2011_103\n",
      "  I am going to be reading these pages: [12, 21]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2019_001\n",
      "  I am going to be reading these pages: [15, 23]\n",
      "  searching with << Page 15 >>[\\s\\S]*<< Page 23 >>\n",
      "Getting important text... for 2011_203\n",
      "  I am going to be reading these pages: [11, 18]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 18 >>\n",
      "Getting important text... for 2015_003\n",
      "  I am going to be reading these pages: [13, 23]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 23 >>\n",
      "Getting important text... for 2017_201\n",
      "  I am going to be reading these pages: [12, 24]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 24 >>\n",
      "Getting important text... for 2013_002\n",
      "  I am going to be reading these pages: [8, 15]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2010_011\n",
      "  I am going to be reading these pages: [32, 33]\n",
      "  searching with << Page 32 >>[\\s\\S]*<< Page 33 >>\n",
      "Getting important text... for 2017_102\n",
      "  I am going to be reading these pages: [4, 5]\n",
      "  searching with << Page 4 >>[\\s\\S]*<< Page 5 >>\n",
      "Getting important text... for 2010_206\n",
      "  I am going to be reading these pages: [9, 11]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 11 >>\n",
      "Getting important text... for 2011_004\n",
      "  I am going to be reading these pages: [17, 23]\n",
      "  searching with << Page 17 >>[\\s\\S]*<< Page 23 >>\n",
      "Getting important text... for 2010_207\n",
      "Error cant find the end of the contents section\n",
      "  Could not find contents section in 2010_207\n",
      "Getting important text... for 2012_202\n",
      "  I am going to be reading these pages: [9, 10]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 10 >>\n",
      "Getting important text... for 2013_202\n",
      "  I am going to be reading these pages: [8, 14]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 14 >>\n",
      "Getting important text... for 2011_202\n",
      "  I am going to be reading these pages: [8, 14]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 14 >>\n",
      "Getting important text... for 2019_105\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 17 >>\n",
      "  Could not find text between pages 10 and 17\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2010_006\n",
      "  I am going to be reading these pages: [9, 11]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 11 >>\n",
      "Getting important text... for 2020_103\n",
      "  I am going to be reading these pages: [5, 11]\n",
      "  searching with << Page 5 >>[\\s\\S]*<< Page 11 >>\n",
      "Getting important text... for 2012_102\n",
      "  I am going to be reading these pages: [6, 14]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 14 >>\n",
      "Getting important text... for 2012_104\n",
      "  I am going to be reading these pages: [11, 20]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 20 >>\n",
      "Getting important text... for 2011_106\n",
      "  I am going to be reading these pages: [7, 15]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2016_203\n",
      "  I am going to be reading these pages: [9, 12]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 12 >>\n",
      "Getting important text... for 2012_101\n",
      "  I am going to be reading these pages: [8, 12]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 12 >>\n",
      "Getting important text... for 2016_004\n",
      "  I am going to be reading these pages: [12, 15]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2019_107\n",
      "  I am going to be reading these pages: [7, 9]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 9 >>\n",
      "Getting important text... for 2015_201\n",
      "  I am going to be reading these pages: [14, 20]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 20 >>\n",
      "Getting important text... for 2016_205\n",
      "  I am going to be reading these pages: [11, 15]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2010_003\n",
      "  I am going to be reading these pages: [10, 12]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 12 >>\n",
      "Getting important text... for 2010_205\n",
      "Error cant find the end of the contents section\n",
      "  Could not find contents section in 2010_205\n",
      "Getting important text... for 2019_102\n",
      "  I am going to be reading these pages: [7, 13]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 13 >>\n",
      "Getting important text... for 2013_005\n",
      "  I am going to be reading these pages: [13, 18]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 18 >>\n",
      "Getting important text... for 2018_005\n",
      "  I am going to be reading these pages: [18, 30]\n",
      "  searching with << Page 18 >>[\\s\\S]*<< Page 30 >>\n",
      "Getting important text... for 2019_108\n",
      "  I am going to be reading these pages: [6, 10]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 10 >>\n",
      "Getting important text... for 2013_009\n",
      "  I am going to be reading these pages: [20, 29]\n",
      "  searching with << Page 20 >>[\\s\\S]*<< Page 29 >>\n",
      "  Could not find text between pages 20 and 29\n",
      "  searching with << Page 20 >>[\\s\\S]*<< Page 30 >>\n",
      "Getting important text... for 2018_102\n",
      "  I am going to be reading these pages: [6, 13]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 13 >>\n",
      "Getting important text... for 2019_104\n",
      "  I am going to be reading these pages: [9, 13]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 13 >>\n",
      "Getting important text... for 2017_205\n",
      "  I am going to be reading these pages: [14, 19]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2016_102\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2020_002\n",
      "  I am going to be reading these pages: [13, 21]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2014_002\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2014_006\n",
      "  I am going to be reading these pages: [15, 33]\n",
      "  searching with << Page 15 >>[\\s\\S]*<< Page 33 >>\n",
      "Getting important text... for 2014_005\n",
      "  I am going to be reading these pages: [19, 36]\n",
      "  searching with << Page 19 >>[\\s\\S]*<< Page 36 >>\n",
      "Getting important text... for 2014_101\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2015_102\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2015_009\n",
      "  I am going to be reading these pages: [12, 24]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 24 >>\n",
      "Getting important text... for 2015_005\n",
      "  I am going to be reading these pages: [8, 20]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 20 >>\n",
      "Getting important text... for 2013_101\n",
      "  I am going to be reading these pages: [10, 16]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2017_103\n",
      "  I am going to be reading these pages: [7, 8]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 8 >>\n",
      "Getting important text... for 2013_203\n",
      "  I am going to be reading these pages: [8, 15, 19, 21, 25, 30, 39, 40, 43, 44]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 44 >>\n",
      "Getting important text... for 2017_002\n",
      "  I am going to be reading these pages: [10, 19]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2012_103\n",
      "  I am going to be reading these pages: [8, 12]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 12 >>\n",
      "  Could not find text between pages 8 and 12\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 12 >>\n",
      "Getting important text... for 2020_003\n",
      "  I am going to be reading these pages: [15, 26]\n",
      "  searching with << Page 15 >>[\\s\\S]*<< Page 26 >>\n",
      "Getting important text... for 2014_105\n",
      "  I am going to be reading these pages: [10, 16]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2018_203\n",
      "  I am going to be reading these pages: [12, 25]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 25 >>\n",
      "Getting important text... for 2014_202\n",
      "  I am going to be reading these pages: [4, 5]\n",
      "  searching with << Page 4 >>[\\s\\S]*<< Page 5 >>\n",
      "Getting important text... for 2010_004\n",
      "  I am going to be reading these pages: [8, 10]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 10 >>\n",
      "Getting important text... for 2013_010\n",
      "  I am going to be reading these pages: [13, 23]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 23 >>\n",
      "Getting important text... for 2012_201\n",
      "  I am going to be reading these pages: [18, 38]\n",
      "  searching with << Page 18 >>[\\s\\S]*<< Page 38 >>\n",
      "Getting important text... for 2017_105\n",
      "  I am going to be reading these pages: [11, 18]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 18 >>\n",
      "Getting important text... for 2010_201\n",
      "  I am going to be reading these pages: [14, 19]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2018_206\n",
      "  I am going to be reading these pages: [14, 25]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 25 >>\n",
      "Getting important text... for 2016_002\n",
      "Error cant find the end of the contents section\n",
      "  Could not find contents section in 2016_002\n",
      "Getting important text... for 2011_105\n",
      "  I am going to be reading these pages: [8, 13]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 13 >>\n",
      "Getting important text... for 2011_002\n",
      "  I am going to be reading these pages: [20, 21, 23, 24, 25]\n",
      "  searching with << Page 20 >>[\\s\\S]*<< Page 25 >>\n",
      "Getting important text... for 2011_101\n",
      "  I am going to be reading these pages: [14, 15, 17, 18, 19]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2020_201\n",
      "  I am going to be reading these pages: [11, 18]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 18 >>\n",
      "Getting important text... for 2015_002\n",
      "  I am going to be reading these pages: [20, 21, 22, 23, 23, 23, 24, 25, 26, 27, 28]\n",
      "  searching with << Page 20 >>[\\s\\S]*<< Page 28 >>\n",
      "Getting important text... for 2014_102\n",
      "  I am going to be reading these pages: [6, 11]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 11 >>\n",
      "Getting important text... for 2014_103\n",
      "  I am going to be reading these pages: [13, 20]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 20 >>\n",
      "Getting important text... for 2010_204\n",
      "  I am going to be reading these pages: [16, 24]\n",
      "  searching with << Page 16 >>[\\s\\S]*<< Page 24 >>\n",
      "Getting important text... for 2014_004\n",
      "  I am going to be reading these pages: [13, 22]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 22 >>\n",
      "Getting important text... for 2018_204\n",
      "  I am going to be reading these pages: [9, 11]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 11 >>\n",
      "Getting important text... for 2020_001\n",
      "  I am going to be reading these pages: [10, 14]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 14 >>\n",
      "Getting important text... for 2014_203\n",
      "  I am going to be reading these pages: [12, 18]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 18 >>\n",
      "Getting important text... for 2019_005\n",
      "  I am going to be reading these pages: [25, 46]\n",
      "  searching with << Page 25 >>[\\s\\S]*<< Page 46 >>\n",
      "Getting important text... for 2013_105\n",
      "  I am going to be reading these pages: [8, 10]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 10 >>\n",
      "Getting important text... for 2016_101\n",
      "  I am going to be reading these pages: [9, 17]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2019_007\n",
      "  I am going to be reading these pages: [5, 9]\n",
      "  searching with << Page 5 >>[\\s\\S]*<< Page 9 >>\n",
      "Getting important text... for 2010_203\n",
      "  I am going to be reading these pages: [13, 16]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2016_008\n",
      "  I am going to be reading these pages: [12, 19]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2011_104\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2011_005\n",
      "  I am going to be reading these pages: [11, 13]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 13 >>\n",
      "Getting important text... for 2019_101\n",
      "  I am going to be reading these pages: [7, 10]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 10 >>\n",
      "Getting important text... for 2011_007\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2017_202\n",
      "  I am going to be reading these pages: [12, 21]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2013_007\n",
      "  I am going to be reading these pages: [7, 9]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 9 >>\n",
      "Getting important text... for 2019_103\n",
      "  I am going to be reading these pages: [12, 21]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2015_101\n",
      "  I am going to be reading these pages: [4, 5]\n",
      "  searching with << Page 4 >>[\\s\\S]*<< Page 5 >>\n",
      "Getting important text... for 2010_001\n",
      "  I am going to be reading these pages: [8, 10]\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 10 >>\n",
      "Getting important text... for 2013_103\n",
      "  I am going to be reading these pages: [13, 20]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 20 >>\n",
      "Getting important text... for 2013_008\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2018_101\n",
      "  I am going to be reading these pages: [6, 15]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2018_205\n",
      "  I am going to be reading these pages: [14, 21]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2017_106\n",
      "  I am going to be reading these pages: [7, 11]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 11 >>\n",
      "Getting important text... for 2012_002\n",
      "  I am going to be reading these pages: [13, 19]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2017_204\n",
      "  I am going to be reading these pages: [16, 26]\n",
      "  searching with << Page 16 >>[\\s\\S]*<< Page 26 >>\n",
      "Getting important text... for 2011_102\n",
      "  I am going to be reading these pages: [11, 25]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 25 >>\n",
      "Getting important text... for 2019_003\n",
      "  I am going to be reading these pages: [17, 28]\n",
      "  searching with << Page 17 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 17 and 28\n",
      "  searching with << Page 16 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 16 and 28\n",
      "  searching with << Page 15 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 15 and 28\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 14 and 28\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 13 and 28\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 12 and 28\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 11 and 28\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 10 and 28\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 9 and 28\n",
      "  searching with << Page 8 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 8 and 28\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 7 and 28\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 6 and 28\n",
      "  searching with << Page 5 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 5 and 28\n",
      "  searching with << Page 4 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 4 and 28\n",
      "  searching with << Page 3 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 3 and 28\n",
      "  searching with << Page 2 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 2 and 28\n",
      "  searching with << Page 1 >>[\\s\\S]*<< Page 28 >>\n",
      "  Could not find text between pages 1 and 28\n",
      "     giving up search for text between pages\n",
      "Getting important text... for 2012_203\n",
      "  I am going to be reading these pages: [7, 15]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2013_003\n",
      "  I am going to be reading these pages: [14, 26]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 26 >>\n",
      "Getting important text... for 2015_203\n",
      "  I am going to be reading these pages: [10, 27]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 27 >>\n",
      "Getting important text... for 2010_102\n",
      "  I am going to be reading these pages: [14, 21]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2013_104\n",
      "  I am going to be reading these pages: [11, 13]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 13 >>\n",
      "Getting important text... for 2010_005\n",
      "  I am going to be reading these pages: [15, 22]\n",
      "  searching with << Page 15 >>[\\s\\S]*<< Page 22 >>\n",
      "Getting important text... for 2016_201\n",
      "  I am going to be reading these pages: [9, 17]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2015_007\n",
      "  I am going to be reading these pages: [25, 39]\n",
      "  searching with << Page 25 >>[\\s\\S]*<< Page 39 >>\n",
      "Getting important text... for 2019_002\n",
      "  I am going to be reading these pages: [15, 25]\n",
      "  searching with << Page 15 >>[\\s\\S]*<< Page 25 >>\n",
      "Getting important text... for 2016_204\n",
      "  I am going to be reading these pages: [11, 19]\n",
      "  searching with << Page 11 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2018_001\n",
      "  I am going to be reading these pages: [23, 39]\n",
      "  searching with << Page 23 >>[\\s\\S]*<< Page 39 >>\n",
      "Getting important text... for 2018_202\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "  searching with << Page 9 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2013_102\n",
      "  I am going to be reading these pages: [6, 9]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 9 >>\n",
      "Getting important text... for 2019_006\n",
      "  I am going to be reading these pages: [20, 35]\n",
      "  searching with << Page 20 >>[\\s\\S]*<< Page 35 >>\n",
      "Getting important text... for 2011_204\n",
      "  I am going to be reading these pages: [17, 51]\n",
      "  searching with << Page 17 >>[\\s\\S]*<< Page 51 >>\n",
      "Getting important text... for 2014_104\n",
      "  I am going to be reading these pages: [10, 15]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 15 >>\n",
      "  Could not find text between pages 10 and 15\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 16 >>\n",
      "  Could not find text between pages 10 and 16\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 17 >>\n",
      "Getting important text... for 2013_108\n",
      "  I am going to be reading these pages: [3, 8]\n",
      "  searching with << Page 3 >>[\\s\\S]*<< Page 8 >>\n",
      "Getting important text... for 2017_203\n",
      "  I am going to be reading these pages: [17, 22]\n",
      "  searching with << Page 17 >>[\\s\\S]*<< Page 22 >>\n",
      "Getting important text... for 2020_101\n",
      "  I am going to be reading these pages: [12, 22]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 22 >>\n",
      "Getting important text... for 2013_201\n",
      "  I am going to be reading these pages: [7, 16]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 16 >>\n",
      "Getting important text... for 2017_007\n",
      "  I am going to be reading these pages: [10, 15]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2014_201\n",
      "  I am going to be reading these pages: [13, 19]\n",
      "  searching with << Page 13 >>[\\s\\S]*<< Page 19 >>\n",
      "Getting important text... for 2015_001\n",
      "  I am going to be reading these pages: [14, 21]\n",
      "  searching with << Page 14 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2013_106\n",
      "  I am going to be reading these pages: [7, 15]\n",
      "  searching with << Page 7 >>[\\s\\S]*<< Page 15 >>\n",
      "Getting important text... for 2017_101\n",
      "  I am going to be reading these pages: [4, 20]\n",
      "  searching with << Page 4 >>[\\s\\S]*<< Page 20 >>\n",
      "Getting important text... for 2011_006\n",
      "  I am going to be reading these pages: [10, 22]\n",
      "  searching with << Page 10 >>[\\s\\S]*<< Page 22 >>\n",
      "Getting important text... for 2017_003\n",
      "  I am going to be reading these pages: [12, 21]\n",
      "  searching with << Page 12 >>[\\s\\S]*<< Page 21 >>\n",
      "Getting important text... for 2017_104\n",
      "  I am going to be reading these pages: [6, 12]\n",
      "  searching with << Page 6 >>[\\s\\S]*<< Page 12 >>\n",
      "Getting important text... for 2020_104\n",
      "  I am going to be reading these pages: [3, 4]\n",
      "  searching with << Page 3 >>[\\s\\S]*<< Page 4 >>\n",
      "  Could not find text between pages 3 and 4\n",
      "  searching with << Page 3 >>[\\s\\S]*<< Page 5 >>\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "reports_path = \"output\"\n",
    "\n",
    "reports = os.listdir(reports_path)\n",
    "\n",
    "reports_to_ignore = [\n",
    "    \"2020_102\", # This report has a funny safety issue that is not formatted correctly so cant be picked up by the regex\n",
    "]\n",
    "reports = [report for report in reports if report not in reports_to_ignore]\n",
    "\n",
    "all_reports_df = clean_text_and_get_safety_issues(reports_path, reports)\n",
    "\n",
    "# Remove all reports that have no found safety issues\n",
    "all_reports_df = all_reports_df[all_reports_df['number_extracted'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get split of 60% train, 20% validation, 20% test\n",
    "seed = 42\n",
    "\n",
    "train_df = all_reports_df.sample(frac=0.6, random_state=seed)\n",
    "\n",
    "validation_df = all_reports_df.drop(train_df.index).sample(frac=0.5, random_state=seed)\n",
    "\n",
    "test_df = all_reports_df.drop(train_df.index).drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was discovered in the testing of the model there is a problem where responses from the fine-tuned model will only have \"inferred\" quality even if they are in fact \"exact\" safety issues.\n",
    "\n",
    "To fix this I will take a small amount of the rows from the train_df and change it so that it is just getting the exact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_exact_examples(df, seed):\n",
    "    df_exact = df.sample(frac=0.2, random_state=seed)\n",
    "\n",
    "    df_exact['uncleaned_report'] = df_exact['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n",
    "    df_exact['expected_response_quality'] = 'exact'\n",
    "\n",
    "    df['expected_response_quality'] = 'inferred'\n",
    "\n",
    "    df_final = pd.concat([df, df_exact]).sort_index()\n",
    "\n",
    "    return df_final\n",
    "\n",
    "train_df = add_random_exact_examples(train_df, seed)\n",
    "\n",
    "validation_df = add_random_exact_examples(validation_df, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data is in three distinct categories it would be prudent to make sure that the resultant data has similar distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>mode</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>validation</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>validation</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>validation</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        split mode  count\n",
       "0        test    0      4\n",
       "1        test    1      5\n",
       "2        test    2      6\n",
       "3       train    0     16\n",
       "4       train    1     20\n",
       "5       train    2     19\n",
       "6  validation    0      7\n",
       "7  validation    1      8\n",
       "8  validation    2      3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the three dataframes into one and add a column to indicate the split\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "validation_df['split'] = 'validation'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "all_datasets_df = pd.concat([train_df, validation_df, test_df])\n",
    "\n",
    "all_datasets_df['mode'] = all_datasets_df['file'].apply(lambda x: x.split('_')[1][0])\n",
    "\n",
    "# Group by split and get distribution of mode in each split\n",
    "all_datasets_df.groupby(['split', 'mode']).count().reset_index()[['split', 'mode', 'file']].rename(columns={'file': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not perfect but we can see that it is pretty darn close. The validation set is low on reports. However this is not something that can be completely fixed and aught not to affect the final outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fine tuned model\n",
    "\n",
    "Because of the dismal success of just asking it I am going to give it a couple of examples and then see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions for formatting the data\n",
    "def format_safety_issue_to_yaml(safety_issue, quality):\n",
    "    safety_issues_dict = []\n",
    "    for issue in safety_issue:\n",
    "        safety_issues_dict.append({\"safety_issue\": issue, \"quality\": quality})\n",
    "\n",
    "    return yaml.dump(safety_issues_dict, sort_keys=False)\n",
    "\n",
    "def format_data(data):\n",
    "  formatted_data = []\n",
    "  for index, row in data.iterrows():\n",
    "    formatted_data.append({\"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "You are going help me read a transport accident investigation report.\n",
    "\n",
    "I want you to please read the report and respond with the safety issues identified in the report.\n",
    "\n",
    "Please only respond with safety issues that are quite clearly stated (\"inferred\" safety issues) or implied (\"inferred\" safety issues) in the report. Each report will only contain one type of safety issue.\n",
    "\n",
    "Remember the definitions give\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "• can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "• is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "{ReportExtracting.ReportExtractor(row['cleaned_report'] if row['expected_response_quality'] == \"inferred\" else row['uncleaned_report'], row['file']).extract_important_text()[0]}\n",
    "        \n",
    "        \n",
    "=Instructions=\n",
    "\n",
    "I want to know the safety issues which this investigation has found.\n",
    "\n",
    "For each safety issue you find I need to know what is the quality of this safety issue.\n",
    "Some reports will have safety issues explicitly stated with something like \"safety issue - ...\" or \"safety issue: ...\", these are \"exact\" safety issues. Now that the text may have extra spaces or characters in it.\n",
    "\n",
    "However if no safety issues are stated explicitly, then you need to inferred them. These inferred safety issues are \"inferred\" safety issues.\n",
    "\n",
    "\n",
    "Can your response please be in yaml format as shown below.\n",
    "\n",
    "- safety_issue: |\n",
    "    bla bla talking about this and that bla bla bla\n",
    "  quality: exact\n",
    "- safety_issue: |\n",
    "    bla bla talking about this and that bla bla bla\n",
    "  quality: exact\n",
    "\n",
    "\n",
    "There is no need to enclose the yaml in any tags.\n",
    "\n",
    "=Here are some definitions=\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "• can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "• is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"{format_safety_issue_to_yaml(row['safety_issues'], row['expected_response_quality'])}\"\"\"\n",
    "        }\n",
    "    ]})\n",
    "\n",
    "  return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  Could not find text between pages 13 and 14\n",
      "  Could not extract text from page 13\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  Could not find text between pages 13 and 14\n",
      "  Could not extract text from page 13\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  Could not find text between pages 10 and 11\n",
      "  Could not extract text from page 10\n",
      "  Could not find text between pages 11 and 12\n",
      "  Could not extract text from page 11\n",
      "  Could not find text between pages 13 and 14\n",
      "  Could not extract text from page 13\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "  Could not find text between pages 23 and 24\n",
      "  Could not extract text from page 23\n",
      "  I am going to be reading these pages: [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "  Could not find text between pages 23 and 24\n",
      "  Could not extract text from page 23\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "  I am going to be reading these pages: [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "  I am going to be reading these pages: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "  I am going to be reading these pages: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "  Could not find text between pages 25 and 26\n",
      "  Could not extract text from page 25\n",
      "  Could not find text between pages 29 and 30\n",
      "  Could not extract text from page 29\n",
      "  Could not find text between pages 31 and 32\n",
      "  Could not extract text from page 31\n",
      "  Could not find text between pages 35 and 36\n",
      "  Could not extract text from page 35\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15]\n",
      "  Could not find text between pages 15 and 16\n",
      "  Could not extract text from page 15\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    }
   ],
   "source": [
    "# Format and save dataset\n",
    "\n",
    "import json\n",
    "\n",
    "training_data = format_data(train_df)\n",
    "\n",
    "validation_data = format_data(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "with open('training_data.jsonl', 'w') as outfile:\n",
    "    for entry in training_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "with open('validation_data.jsonl', 'w') as outfile:\n",
    "    for entry in validation_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to OpenAI\n",
    "\n",
    "\n",
    "openai_file = client.files.create(\n",
    "  file=open(\"training_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "openai_file_val = client.files.create(\n",
    "  file=open(\"validation_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-VprSIyIeSn49zZRcrUdSeoDs', created_at=1712285197, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-imErdBW1EpSlnDOl8RluwTmE', result_files=[], status='validating_files', trained_tokens=None, training_file='file-BNQFBZSVYecd0f5qFf9qsvkE', validation_file='file-I1iFeGRfytqYUBECvkEnd70M', user_provided_suffix=None, seed=1869439520, integrations=[])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the fine-tune job\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=openai_file.id, \n",
    "  validation_file=openai_file_val.id,\n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I am creating multiple fine-tuned models I should have some reference text to explain what is going on.\n",
    "\n",
    "| name | description |\n",
    "| ---- | ---- |\n",
    "| ft:gpt-3.5-turbo-0125:personal::99hWfq4f | second model fined tuned. This is the first model that uses the full 70 or so explicit reports with redacted safety issues. After fine tuning I tweaked the prompts and got it to 73% matched exact vs inferred. With just over half of the validation set having all of its respective safety issues matched by the inferred. Exact vs exact was at 100% |\n",
    "| gpt-4-turbo (untrained) | Just to store the metrics somewhere I will test it on gpt 4 turbo. I got about 83% on average match with over 77% perfect match.|\n",
    "| ft:gpt-3.5-turbo-0125:personal::9AU7vXs3 | This is my third ft model. It is trained on the same data base as the previous one. However it is differnet in two ways. The prompts in the training data are differnet and there are some exmaples included of what it is like to have exact safety issues.  Out of the box I have it at 61% match percent and about just over half are fully matched. However I am starting to doubt that the comparer is being reaosnable. I.e I think it might be being too hard.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this I am working on fine tuning a model so that I can accurately extract the safety issues from the report.\n",
    "\n",
    "I am getting to the point of thinking that it might not be possible to extract them 100% accurately.\n",
    "\n",
    "Instead maybe I should just focus on making a good distinction between exact and inferred.\n",
    "\n",
    "False news!\n",
    "\n",
    "False news!\n",
    "\n",
    "I had made a mistake and moved over the wrong query request to the new ft model.\n",
    "\n",
    "After checking it on the training data it is about 50% accurate which is pretty good.\n",
    "I will now check it on some new data that the training hasn't seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing fine-tuned model\n",
    "\n",
    "I will now test the fine-tuned model and see how it fares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and comparing datasets inferred and exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine.OpenAICaller as OpenAICaller\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "def compare_safety_issues(safety_issues, inferred_safety_issues):\n",
    "    \"\"\"\n",
    "    This will receive two lists of safety issues and then ask the LLM to compare each safety issue and see if they are the same or different.\n",
    "    \"\"\"\n",
    "\n",
    "    pairs = [(i, j) for i, _ in enumerate(safety_issues) for j, _ in enumerate(inferred_safety_issues)]\n",
    "\n",
    "    pairs_comparison_results = []\n",
    "\n",
    "    for extracted, inferred in pairs:\n",
    "        response = openAICaller.query(\n",
    "            \"\"\"\n",
    "You are going to help me compare if safety issues are the same.\n",
    "\n",
    "You will be given two safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that they are the same safety issue and maybe just worded differently. Whereas different will mean that they are fundamentally different safety issues.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "    Here is the first safety issues:\n",
    "    {safety_issues[extracted]}\n",
    "\n",
    "    Here is the second safety issues:\n",
    "    {inferred_safety_issues[inferred]}\n",
    "    \"\"\",\n",
    "            temp =  0,\n",
    "            model = \"gpt-4\"\n",
    "        ).lower()\n",
    "\n",
    "        if response != 'yes' and response != 'no':\n",
    "            print(f\"Error response in the wrong format {response}\")\n",
    "            response = 'undetermined'\n",
    "    \n",
    "        pairs_comparison_results.append({\n",
    "            'pair': (extracted, inferred),\n",
    "            'result': response\n",
    "        })\n",
    "    return pairs_comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt used to compare two lists of safety themes.\n",
    "```\n",
    "You are going to help me compare if these listed safety issues are the same.\n",
    "\n",
    "You will be given two lists of safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that for each safety issue in the set it will match a safety issue found in the other set albeit if worded slightly differently.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "Here is the first set of safety issues:\n",
    "{safety_issues}\n",
    "\n",
    "Here is the second set of safety issues:\n",
    "{inferred_safety_issues}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_safety_issues_df(df):\n",
    "    # Compare safety issues with the LLM\n",
    "    df['same_comparison_results'] = df.apply(\n",
    "        lambda x: compare_safety_issues(x['safety_issues'],\n",
    "                                        x['inferred_safety_issues']),\n",
    "        axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_safety_issues_from_cleaned_reports(df):\n",
    "    \"\"\"\n",
    "    This will call the inference function and extract the safety issues from the cleaned reports.\n",
    "    \"\"\"\n",
    "    df['inferred_safety_issues_struct'] = df.apply(lambda x: ReportExtracting.SafetyIssueExtractor(x['cleaned_report'], x['file'])._extract_safety_issues_with_inference(), axis=1)\n",
    "\n",
    "    # Extract the safety issues from the data structured returned from the inferences extraction function.\n",
    "    df['inferred_safety_issues'] = df['inferred_safety_issues_struct'].apply(lambda x: [issue['safety_issue'] for issue in x] if (x is not None and isinstance(x[0], dict)) else None if x is None else x)\n",
    "\n",
    "    df['number_inferred'] = df['inferred_safety_issues'].apply(lambda x: len(x) if x is not None else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import textwrap\n",
    "\n",
    "def make_match_visualization(df, safety_issue_text_dict, report_id):\n",
    "    '''\n",
    "    Create a picture that shows both the extracted and inferred safety issues and the matches between them as lines/arrows.\n",
    "    '''\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes for the 'extracted' and 'inferred' indices\n",
    "    extracted_indices = df['extracted'].unique()\n",
    "    inferred_indices = df['inferred'].unique()\n",
    "\n",
    "    NODE_WIDTH = 50\n",
    "\n",
    "    for i, index in enumerate(extracted_indices):\n",
    "        node_label = '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"extracted\"][index], width=NODE_WIDTH))\n",
    "        G.add_node(node_label, pos=(0, i))\n",
    "\n",
    "    for i, index in enumerate(inferred_indices):\n",
    "        node_label = '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"inferred\"][index], width=NODE_WIDTH))\n",
    "        G.add_node(node_label, pos=(3, i))\n",
    "\n",
    "    # Add edges between the matched indices\n",
    "    for _, row in df.iterrows():\n",
    "        if row['result'] == 'yes':\n",
    "            G.add_edge('\\n'.join(textwrap.wrap(safety_issue_text_dict[\"extracted\"][row[\"extracted\"]], width=NODE_WIDTH)), \n",
    "                        '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"inferred\"][row[\"inferred\"]], width=NODE_WIDTH)))\n",
    "\n",
    "    max_num_of_nodes_column = max(\n",
    "        len(safety_issue_text_dict['extracted']),\n",
    "        len(safety_issue_text_dict['inferred'])\n",
    "        )\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    plt.figure(figsize=(8, max_num_of_nodes_column * 2.5))  \n",
    "    nx.draw(G, pos, with_labels=False, node_color='lightblue', node_size=[len(node) * NODE_WIDTH for node in G.nodes()])\n",
    "    nx.draw_networkx_labels(G, pos, font_size=7)\n",
    "    \n",
    "    plt.xlim(-1, 4.5)  # Add buffer to the outside side edges\n",
    "    plt.ylim(-1, max_num_of_nodes_column*1)  # Add buffer to the outside top and bottom edges\n",
    "\n",
    "    # Add report ID and headers\n",
    "    plt.title(f'Report ID: {report_id}')\n",
    "    plt.text(0, max_num_of_nodes_column-0.2, 'Real', fontsize=12, ha='center')\n",
    "    plt.text(3, max_num_of_nodes_column-0.2, 'Inferred', fontsize=12, ha='center')\n",
    "    plt.text(1.5, max_num_of_nodes_column-0.5, 'Arrow indicates that real has been \"matched\" by LLM with inferred', fontsize=6, ha='center')\n",
    "\n",
    "    plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_comparison_of_reports_SI(row, with_visual_generation = True):\n",
    "    '''\n",
    "    Take a row of the DataFrame with the comparison results and interpret them by giving you a match percentage. This percentage is how 'similar' the extracted safety issues are to the inferred safety issues. However similar is hard to define so currently it is simply how many of the extracted safety issues have a match.\n",
    "    '''\n",
    "    result = row['same_comparison_results']\n",
    "\n",
    "    # Make it a DataFrame\n",
    "    result_df = pd.DataFrame(result)\n",
    "    # Split tuple into two columns\n",
    "    result_df['extracted'] = result_df['pair'].apply(lambda x: x[0])\n",
    "    result_df['inferred'] = result_df['pair'].apply(lambda x: x[1])\n",
    "\n",
    "    num_extracted_safety_issues = result_df['extracted'].unique().shape[0]\n",
    "    num_inferred_safety_issues = result_df['inferred'].unique().shape[0]\n",
    "\n",
    "    # Interpret the results\n",
    "    # Make sure that each exact safety issues is matched to only one inferred safety issue\n",
    "\n",
    "    inferred_safety_issues_matched = [False] * num_inferred_safety_issues\n",
    "    safety_issues_matched = [False] * num_extracted_safety_issues\n",
    "\n",
    "    for pair_comparison in result:\n",
    "        if pair_comparison['result'] == 'yes':\n",
    "            inferred_safety_issues_matched[pair_comparison['pair'][1]] = True\n",
    "            safety_issues_matched[pair_comparison['pair'][0]] = True\n",
    "\n",
    "    match_percent = sum(safety_issues_matched) / num_extracted_safety_issues\n",
    "\n",
    "    if with_visual_generation:\n",
    "        safety_issue_text ={\n",
    "            \"extracted\": [row['safety_issues'][index] for index in result_df['extracted'].unique()],\n",
    "            \"inferred\": [row['inferred_safety_issues'][index] for index in result_df['inferred'].unique()]\n",
    "        }\n",
    "\n",
    "        make_match_visualization(result_df, safety_issue_text, row['file'])\n",
    "        \n",
    "\n",
    "    return match_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_dataset_SI_comparison(dataset):\n",
    "    \"\"\"\n",
    "    The dataset is going to have a data on the safety issues extracted and inferred as well as the comparison results of these safety issues. This function will look at the while results and give you a summary of what is happening.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['SI_match_percent'] = dataset.apply(lambda row: interpret_comparison_of_reports_SI(row, True), axis = 1)\n",
    "    dataset['SI_match'] = dataset['SI_match_percent'] == 1\n",
    "\n",
    "    dataset['SI_count_match'] = dataset['number_extracted'] == dataset['number_inferred']\n",
    "    dataset['SI_count_difference'] = dataset['number_extracted'] - dataset['number_inferred']\n",
    "\n",
    "    print(f\"\"\"\n",
    "==============                                   ==============\n",
    "                Comparing Safety Issues results\n",
    "==============                                   ==============\n",
    "          \n",
    "There are {dataset.shape[0]} reports in the dataset.\n",
    "    \n",
    "Average match percentage: {round(dataset['SI_match_percent'].mean(), 2)}\n",
    "Amount of all matched safety issues: {dataset['SI_match'].sum()/dataset.shape[0]}\n",
    "    This is the percentage of extracted safety issues that have been matched to inferred safety issues.\n",
    "Information on the number of safety issues extracted and inferred:\n",
    "          \"\"\")\n",
    "    \n",
    "    display(\n",
    "        pd.DataFrame([\n",
    "            ['same number of extracted and inferred safety issues', dataset['SI_count_match'].mean()],\n",
    "            ['Average match of same count of SI', dataset[dataset['SI_count_match']]['SI_match_percent'].mean()],\n",
    "            ['Average difference in count of SI', dataset['SI_count_difference'].mean()],\n",
    "            ['Average number of extracted SI', dataset['number_extracted'].mean()],\n",
    "            ['Average number of inferred SI', dataset['number_inferred'].mean()]\n",
    "        ], columns=[\"description\", \"statistic\"]).style \\\n",
    "            .format(precision=2) \\\n",
    "            .hide(axis=\"index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a dataset\n",
    "I will compare the models responses with the exact extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting important text... for 2013_106\n",
      "  I am going to be reading these pages: [7, 15]\n",
      "Getting important text... for 2019_005\n",
      "  I am going to be reading these pages: [25, 46]\n",
      "Getting important text... for 2018_001\n",
      "  I am going to be reading these pages: [23, 39]\n",
      "Getting important text... for 2017_202\n",
      "  I am going to be reading these pages: [12, 21]\n",
      "Getting important text... for 2018_005\n",
      "  I am going to be reading these pages: [18, 30]\n",
      "Getting important text... for 2019_104\n",
      "  I am going to be reading these pages: [9, 13]\n",
      "Getting important text... for 2017_101\n",
      "  I am going to be reading these pages: [4, 20]\n",
      "Getting important text... for 2017_203\n",
      "  I am going to be reading these pages: [17, 22]\n",
      "Getting important text... for 2015_102\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "Getting important text... for 2011_003\n",
      "  I am going to be reading these pages: [14, 31]\n",
      "Getting important text... for 2019_105\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "  Could not find text between pages 10 and 17\n",
      "Getting important text... for 2016_008\n",
      "  I am going to be reading these pages: [12, 19]\n",
      "Getting important text... for 2012_101\n",
      "  I am going to be reading these pages: [8, 12]\n",
      "Getting important text... for 2018_203\n",
      "  I am going to be reading these pages: [12, 25]\n",
      "Getting important text... for 2020_002\n",
      "  I am going to be reading these pages: [13, 21]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(OpenAICaller)\n",
    "\n",
    "# Infer safety issues\n",
    "validation_df_with_SI_Inference = extract_safety_issues_from_cleaned_reports(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare safety issues\n",
    "validation_df_with_SI_Inference = compare_safety_issues_df(validation_df_with_SI_Inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148603/2219356858.py:53: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n",
      "/tmp/ipykernel_148603/2219356858.py:53: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============                                   ==============\n",
      "                Comparing Safety Issues results\n",
      "==============                                   ==============\n",
      "          \n",
      "There are 15 reports in the dataset.\n",
      "    \n",
      "Average match percentage: 0.61\n",
      "Amount of all matched safety issues: 0.5333333333333333\n",
      "    This is the percentage of extracted safety issues that have been matched to inferred safety issues.\n",
      "Information on the number of safety issues extracted and inferred:\n",
      "          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a46b6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_a46b6_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n",
       "      <th id=\"T_a46b6_level0_col1\" class=\"col_heading level0 col1\" >statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_a46b6_row0_col0\" class=\"data row0 col0\" >same number of extracted and inferred safety issues</td>\n",
       "      <td id=\"T_a46b6_row0_col1\" class=\"data row0 col1\" >0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a46b6_row1_col0\" class=\"data row1 col0\" >Average match of same count of SI</td>\n",
       "      <td id=\"T_a46b6_row1_col1\" class=\"data row1 col1\" >0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a46b6_row2_col0\" class=\"data row2 col0\" >Average difference in count of SI</td>\n",
       "      <td id=\"T_a46b6_row2_col1\" class=\"data row2 col1\" >0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a46b6_row3_col0\" class=\"data row3 col0\" >Average number of extracted SI</td>\n",
       "      <td id=\"T_a46b6_row3_col1\" class=\"data row3 col1\" >2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a46b6_row4_col0\" class=\"data row4 col0\" >Average number of inferred SI</td>\n",
       "      <td id=\"T_a46b6_row4_col1\" class=\"data row4 col1\" >1.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7cb253374dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Understand how well they compared\n",
    "interpret_dataset_SI_comparison(validation_df_with_SI_Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there is some interesting performance going on.\n",
    "\n",
    "We have about 0.54 percent of the exact ones have a match. However there are problems where there is about half an extra safety issue inferred for every safety issues extracted. That is only about a 1/3 of the time does it generate the same amount of safety issues. However when it does there is a 60% chance they will all match up.\n",
    "\n",
    "I need to still do 2 things before the completion of the safety issue extraction upgrade:\n",
    "\n",
    "- [x] Make sure that the comparison of the extracted and inferred safety issues are reasonable.\n",
    "- [x] Check whether it does a good job getting the difference between inferred and exact safety issues\n",
    "To be completed with a re fine-tune of the model with giving it some new examples  that include ones where the safety issues are actaully exact because I havent redacted them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of safety issues reasonable\n",
    "\n",
    "I will start by just looking at the validation_df and the 15 examples there and see if I agree with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly will go through and make manual comparisons of the extracted and inferred safety issues\n",
    "\n",
    "with open('manual_comparison.txt', 'w') as file:\n",
    "    for index, row in validation_df_with_SI_Inference.iterrows():\n",
    "        file.write(f\"\"\"\n",
    "=============================================================================\n",
    "                   Report: {row['file']}\n",
    "=============================================================================\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Extracted Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Inferred Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['inferred_safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_comparison = [\n",
    "    {\n",
    "        \"file\": \"2013_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count extra\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_204\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Here the extracted safety issues have also captured a summary of hte safety issues at the top of the analysis. This means that there are 5 where there should only be 4 and the first one is currently a list of all of them.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_005\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The wording is different but it is similar enough. However the inferred one does not 'aviation industry practice'\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_108\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The first ones match however the inferred one is much longer and even gives a reference to a document.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_006\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        'notes': \"Completely different safety issues\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2017_203\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"The one inferred one is an exact match and I wonder if it was not removed prior to the inference\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_101\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"They are similar but not the same\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_003\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred safety issues are actually correct as there is a summary which it has read from.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_202\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"The inferred is more specific about what equipment but is more vague when it comes to defining what the problem to be mitigated was.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred are actually exact ones lifted from the report\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2012_103\",\n",
    "        \"same\": \"yes\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Two of the safety issues are exact however it should of extracted 4 exact issues.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made me noted that actaully I should have a look at which ones are be extracted exactly or inferred. But first I need to compare this with what my comparision function says"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>safety_issues</th>\n",
       "      <th>cleaned_report</th>\n",
       "      <th>number_extracted</th>\n",
       "      <th>split</th>\n",
       "      <th>inferred_safety_issues</th>\n",
       "      <th>number_inferred</th>\n",
       "      <th>same</th>\n",
       "      <th>manual_same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2014_101</td>\n",
       "      <td>[The sighting distance available to drivers of...</td>\n",
       "      <td>﻿ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>validation</td>\n",
       "      <td>[There was  an insufficient sighting distance ...</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                      safety_issues  \\\n",
       "60  2014_101  [The sighting distance available to drivers of...   \n",
       "\n",
       "                                       cleaned_report  number_extracted  \\\n",
       "60  ﻿ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...                 2   \n",
       "\n",
       "         split                             inferred_safety_issues  \\\n",
       "60  validation  [There was  an insufficient sighting distance ...   \n",
       "\n",
       "    number_inferred same manual_same  \n",
       "60                2  yes          no  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df_with_SI_Inference['manual_same'] = validation_df_with_SI_Inference['file'].apply(lambda x: next((item['same'] for item in manual_comparison if item[\"file\"] == x), None))\n",
    "\n",
    "# Show cases where the manual comparison is different to the LLM comparison\n",
    "validation_df_with_SI_Inference[validation_df_with_SI_Inference['same'] != validation_df_with_SI_Inference['manual_same']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a look at this I have changed the comparision to be per safety issue.\n",
    "\n",
    "Looking at the charts that are produced it seems to do a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between exact and inferred\n",
    "\n",
    "The Model is going to be asked to read the important part of a report and extract the safety issues there will be two sorts of safety issues extracted.\n",
    "\n",
    "Firstly will be the exact safety issues. These are where the report explicitly states the safety issues. Then there will be inferred which is where no exact safety issues can be found. Then the model will \"invent\" some.\n",
    "\n",
    "This needs to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any reports that have exact safety issues that are inferred\n",
    "\n",
    "for _, row in validation_df_with_SI_Inference.iterrows():\n",
    "    \n",
    "    for issue in row['inferred_safety_issues_struct']:\n",
    "        if issue['quality'] == 'exact':\n",
    "            print(f\"Report {row.file} has an exact safety issue that is inferred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no exact safety issues from the validation which makes sense as the all the of the validation report have ha the safety issues removed.\n",
    "\n",
    "I could test this by seeing what happenings if I get a report that should have a exact safety issues in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148603/2219356858.py:53: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n",
      "/tmp/ipykernel_148603/2219356858.py:53: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============                                   ==============\n",
      "                Comparing Safety Issues results\n",
      "==============                                   ==============\n",
      "          \n",
      "There are 18 reports in the dataset.\n",
      "    \n",
      "Average match percentage: 1.0\n",
      "Amount of all matched safety issues: 1.0\n",
      "    This is the percentage of extracted safety issues that have been matched to inferred safety issues.\n",
      "Information on the number of safety issues extracted and inferred:\n",
      "          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9d856\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_9d856_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n",
       "      <th id=\"T_9d856_level0_col1\" class=\"col_heading level0 col1\" >statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_9d856_row0_col0\" class=\"data row0 col0\" >same number of extracted and inferred safety issues</td>\n",
       "      <td id=\"T_9d856_row0_col1\" class=\"data row0 col1\" >0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9d856_row1_col0\" class=\"data row1 col0\" >Average match of same count of SI</td>\n",
       "      <td id=\"T_9d856_row1_col1\" class=\"data row1 col1\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9d856_row2_col0\" class=\"data row2 col0\" >Average difference in count of SI</td>\n",
       "      <td id=\"T_9d856_row2_col1\" class=\"data row2 col1\" >-1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9d856_row3_col0\" class=\"data row3 col0\" >Average number of extracted SI</td>\n",
       "      <td id=\"T_9d856_row3_col1\" class=\"data row3 col1\" >2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9d856_row4_col0\" class=\"data row4 col0\" >Average number of inferred SI</td>\n",
       "      <td id=\"T_9d856_row4_col1\" class=\"data row4 col1\" >3.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7cb25509ded0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "\n",
    "# Getting some examples of exact safety issues that are found using the model\n",
    "reports_path = \"output\"\n",
    "\n",
    "comparing_with_exact = validation_df.copy()\n",
    "\n",
    "comparing_with_exact['cleaned_report'] = comparing_with_exact['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n",
    "comparing_with_exact = extract_safety_issues_from_cleaned_reports(comparing_with_exact)\n",
    "\n",
    "comparing_with_exact = compare_safety_issues_df(comparing_with_exact)\n",
    "\n",
    "interpret_dataset_SI_comparison(comparing_with_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that it does a good job of extracted safety issues even if they are mentioned exactly. However two things. Some explicit safety issues are missing. Secondly the safety quality is not being correctly identified as exact.\n",
    "\n",
    "The problem of explicit safety issues missed are solved by: 735d98b1e122b66c229b93021c266424af5cd9d4\n",
    "\n",
    "Where as the incorrect labelling will be fixed by a fine tuning of the model with new data that has some \"exact\" safety issue examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to plain gpt-4-turbo\n",
    "\n",
    "As the fine tuning model is expensive to train I was interested in trying gpt 4 now with all the changes in the prompt.\n",
    "\n",
    "It is doing quite a good job and so I might try use it.\n",
    "\n",
    "I am getting about 77% completely matched reports (that is that all the safety issues in a report is matched with at least one of the inferred safety issues.)\n",
    "\n",
    "This was quite a surprise however now it is listing all of the safety issue qualities as exact. Therefore I will update the prompt and see what happens. After trying again and again with the prompts I have realized that i just cant ge it to do want and instead it is making it worse.\n",
    "\n",
    "I will fine-tune the data with the updated prompts and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(OpenAICaller)\n",
    "\n",
    "# For this unique test I will change the df so that it has the uncleaned reports in where the cleaned reports should be. THis is just to test that it can do both.\n",
    "\n",
    "validation_df_gpt4 = validation_df.copy()\n",
    "\n",
    "# Only replace if uncleaned_report exists\n",
    "validation_df_gpt4['cleaned_report'] = validation_df_gpt4.apply(lambda row: row['cleaned_report'] if row['uncleaned_report'] else row['uncleaned_report'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148603/2219356858.py:53: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n",
      "/tmp/ipykernel_148603/2219356858.py:53: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============                                   ==============\n",
      "                Comparing Safety Issues results\n",
      "==============                                   ==============\n",
      "          \n",
      "There are 18 reports in the dataset.\n",
      "    \n",
      "Average match percentage: 0.65\n",
      "Amount of all matched safety issues: 0.4444444444444444\n",
      "    This is the percentage of extracted safety issues that have been matched to inferred safety issues.\n",
      "Information on the number of safety issues extracted and inferred:\n",
      "          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a07f3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_a07f3_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n",
       "      <th id=\"T_a07f3_level0_col1\" class=\"col_heading level0 col1\" >statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_a07f3_row0_col0\" class=\"data row0 col0\" >same number of extracted and inferred safety issues</td>\n",
       "      <td id=\"T_a07f3_row0_col1\" class=\"data row0 col1\" >0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a07f3_row1_col0\" class=\"data row1 col0\" >Average match of same count of SI</td>\n",
       "      <td id=\"T_a07f3_row1_col1\" class=\"data row1 col1\" >0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a07f3_row2_col0\" class=\"data row2 col0\" >Average difference in count of SI</td>\n",
       "      <td id=\"T_a07f3_row2_col1\" class=\"data row2 col1\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a07f3_row3_col0\" class=\"data row3 col0\" >Average number of extracted SI</td>\n",
       "      <td id=\"T_a07f3_row3_col1\" class=\"data row3 col1\" >2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a07f3_row4_col0\" class=\"data row4 col0\" >Average number of inferred SI</td>\n",
       "      <td id=\"T_a07f3_row4_col1\" class=\"data row4 col1\" >2.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7cb2548be290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Infer safety issues\n",
    "validation_df_gpt4_with_inference = extract_safety_issues_from_cleaned_reports(validation_df_gpt4)\n",
    "\n",
    "# Compare safety issues\n",
    "validation_df_gpt4_with_inference = compare_safety_issues_df(validation_df_gpt4_with_inference)\n",
    "\n",
    "# Understand how well they compared\n",
    "interpret_dataset_SI_comparison(validation_df_gpt4_with_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending off dataset to chris and ingrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract important text from 2010_207\n",
      "Could not extract important text from 2010_205\n",
      "Could not extract important text from 2016_002\n",
      "Could not extract important text from 2019_003\n",
      "Getting important text... for 2010_102\n",
      "  I am going to be reading these pages: [14, 21]\n",
      "Getting important text... for 2016_006\n",
      "  I am going to be reading these pages: [13, 19]\n",
      "Getting important text... for 2020_201\n",
      "  I am going to be reading these pages: [11, 18]\n",
      "Getting important text... for 2011_104\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Getting important text... for 2020_002\n",
      "  I am going to be reading these pages: [13, 21]\n",
      "Getting important text... for 2010_007\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 36]\n",
      "Getting important text... for 2017_203\n",
      "  I am going to be reading these pages: [17, 22]\n",
      "Getting important text... for 2017_103\n",
      "  I am going to be reading these pages: [7, 8]\n",
      "Getting important text... for 2013_203\n",
      "  I am going to be reading these pages: [8, 15, 19, 21, 25, 30, 39, 40, 43, 44]\n",
      "Too many tokens 22032, not sending to OpenAI\n",
      "  Could not get safety issues from the report.\n",
      "Getting important text... for 2012_001\n",
      "  I am going to be reading these pages: [29, 42]\n",
      "Getting important text... for 2013_009\n",
      "  I am going to be reading these pages: [20, 29]\n",
      "  Could not find text between pages 20 and 29\n",
      "Getting important text... for 2018_206\n",
      "  I am going to be reading these pages: [14, 25]\n",
      "Getting important text... for 2019_007\n",
      "  I am going to be reading these pages: [5, 9]\n",
      "Getting important text... for 2020_101\n",
      "  I am going to be reading these pages: [12, 22]\n",
      "Getting important text... for 2011_007\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "Getting important text... for 2013_101\n",
      "  I am going to be reading these pages: [10, 16]\n",
      "Getting important text... for 2010_011\n",
      "  I am going to be reading these pages: [32, 33]\n",
      "Getting important text... for 2020_003\n",
      "  I am going to be reading these pages: [15, 26]\n",
      "Getting important text... for 2018_202\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "Getting important text... for 2010_004\n",
      "  I am going to be reading these pages: [8, 10]\n",
      "Getting important text... for 2019_204\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "Getting important text... for 2010_005\n",
      "  I am going to be reading these pages: [15, 22]\n",
      "Getting important text... for 2017_106\n",
      "  I am going to be reading these pages: [7, 11]\n",
      "Getting important text... for 2011_102\n",
      "  I am going to be reading these pages: [11, 25]\n",
      "Getting important text... for 2016_102\n",
      "  I am going to be reading these pages: [9, 16]\n",
      "Getting important text... for 2016_205\n",
      "  I am going to be reading these pages: [11, 15]\n",
      "Getting important text... for 2013_006\n",
      "  I am going to be reading these pages: [12, 19]\n",
      "Getting important text... for 2019_202\n",
      "  I am going to be reading these pages: [8, 13]\n",
      "Getting important text... for 2013_002\n",
      "  I am going to be reading these pages: [8, 15]\n",
      "Getting important text... for 2018_001\n",
      "  I am going to be reading these pages: [23, 39]\n",
      "Getting important text... for 2012_202\n",
      "  I am going to be reading these pages: [9, 10]\n",
      "Getting important text... for 2011_005\n",
      "  I am going to be reading these pages: [11, 13]\n",
      "Getting important text... for 2020_103\n",
      "  I am going to be reading these pages: [5, 11]\n",
      "Getting important text... for 2010_202\n",
      "  I am going to be reading these pages: [11, 17]\n",
      "Getting important text... for 2010_009\n",
      "  I am going to be reading these pages: [25, 37]\n",
      "Getting important text... for 2016_101\n",
      "  I am going to be reading these pages: [9, 17]\n",
      "Getting important text... for 2012_002\n",
      "  I am going to be reading these pages: [13, 19]\n",
      "Getting important text... for 2016_004\n",
      "  I am going to be reading these pages: [12, 15]\n",
      "Getting important text... for 2013_104\n",
      "  I am going to be reading these pages: [11, 13]\n",
      "Getting important text... for 2019_006\n",
      "  I am going to be reading these pages: [20, 35]\n",
      "Getting important text... for 2016_008\n",
      "  I am going to be reading these pages: [12, 19]\n",
      "Getting important text... for 2014_101\n",
      "  I am going to be reading these pages: [10, 17]\n",
      "Getting important text... for 2019_106\n",
      "  I am going to be reading these pages: [6, 12]\n",
      "Getting important text... for 2014_102\n",
      "  I am going to be reading these pages: [6, 11]\n",
      "Getting important text... for 2017_102\n",
      "  I am going to be reading these pages: [4, 5]\n",
      "Getting important text... for 2016_203\n",
      "  I am going to be reading these pages: [9, 12]\n",
      "Getting important text... for 2010_204\n",
      "  I am going to be reading these pages: [16, 24]\n",
      "Getting important text... for 2015_003\n",
      "  I am going to be reading these pages: [13, 23]\n",
      "Getting important text... for 2017_201\n",
      "  I am going to be reading these pages: [12, 24]\n",
      "Getting important text... for 2011_203\n",
      "  I am going to be reading these pages: [11, 18]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "reports_path = \"output\"\n",
    "\n",
    "reports = os.listdir(reports_path)\n",
    "\n",
    "reports_to_ignore = [\n",
    "    \"2020_102\", # This report has a funny safety issue that is not formatted correctly so cant be picked up by the regex\n",
    "]\n",
    "reports = [report for report in reports if report not in reports_to_ignore]\n",
    "\n",
    "all_reports_df = clean_text_and_get_safety_issues(reports_path, reports)\n",
    "\n",
    "reports_to_send = all_reports_df.sample(50, random_state=42)\n",
    "\n",
    "reports_to_send['cleaned_report'] = reports_to_send['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reports_to_send_with_safety_issues = extract_safety_issues_from_cleaned_reports(reports_to_send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_df = reports_to_send_with_safety_issues[['file', 'inferred_safety_issues_struct']]\n",
    "\n",
    "# Make minimum_df longer by only having one safety issues per row. As currently safety issues are stored in a list\n",
    "\n",
    "minimum_df = minimum_df.explode('inferred_safety_issues_struct')\n",
    "\n",
    "#Remove the rows that have no safety issues\n",
    "minimum_df = minimum_df[minimum_df['inferred_safety_issues_struct'].notna()]\n",
    "\n",
    "minimum_df['quality'] = minimum_df['inferred_safety_issues_struct'].apply(lambda x: x['quality'])\n",
    "minimum_df['safety_issue'] = minimum_df['inferred_safety_issues_struct'].apply(lambda x: x['safety_issue'])\n",
    "\n",
    "minimum_df[['file', 'safety_issue', 'quality']].to_excel('inferred_safety_issues.xlsx')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
