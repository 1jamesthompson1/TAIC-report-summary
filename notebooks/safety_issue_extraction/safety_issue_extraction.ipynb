{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting safety issues from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem introduction\n",
    "Safety issues are being extracted from the reports.\n",
    "\n",
    "This is linked with issue https://github.com/1jamesthompson1/TAIC-report-summary/issues/101\n",
    "\n",
    "There are two types of safety issues being extracted.\n",
    "Firstly are exact extraction. These are safety issues that are explicitly mentioned in the report. They will follow something like \"safety issue: ...\" or \"safety issue - ...\". Exact safety issue extraction is not too much of a worry and just needs to be tested and could be done so with unit tests.\n",
    "\n",
    "Secondly however are inferred safety issues. These safety issues are supposedly present but are not explicitly stated. This requires a little bit more validation then a simple unit test. Fortunately I can get a good test set.\n",
    "\n",
    "All the reports that have the safety issues explicitly mentioned will show me what the correct answer is. So all I need to do is remove the explicitly mention of the safety issue from the report then I can run it as an inferred and see if it comes up with the same safety issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution story\n",
    "\n",
    "I have peformed some testing on fine-tuning gpt 3.5 turbo.\n",
    "\n",
    "It was about 56% accurate on the training set (about 15 reports)\n",
    "However on a testing set it was only 17% accurate. This was becuase about 70% of the items had mismatched number of safety issues extracted.\n",
    "\n",
    "This means that I wanted to try with more data. There are about 70 reports that I have meaning I can split it between a training, validation and test set. This is what the code does below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required modules\n",
    "\n",
    "Some cells reload modules \n",
    "There are potentially new modules that are loaded in cells later on but I do like to have something at the top so here it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "import importlib\n",
    "from engine.Extract_Analyze import ReportExtracting\n",
    "import engine.OpenAICaller as OpenAICaller\n",
    "importlib.reload(ReportExtracting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets for working with\n",
    "\n",
    "To get the dataset you need I just pulled it from the viewer/output folder.\n",
    "\n",
    "This has all of the reports downloaded and extracted. From there everything else is done in the notebook.\n",
    "\n",
    "Here is a link to the exact copy of the output folder I used: https://github.com/1jamesthompson1/TAIC-report-summary/tree/48e3b09380364731bfb343f2105851cd3babf90a/viewer/output\n",
    "I have not included it in the repo to save space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  read_or_create_important_text_file(path, report_id, text):\n",
    "    important_text_path = os.path.join(path, report_id, f'{report_id}_important_text.txt')\n",
    "\n",
    "    if not os.path.isfile(important_text_path):\n",
    "        important_text, pages = ReportExtracting.ReportExtractor(text, report_id).extract_important_text()\n",
    "\n",
    "        important_text = \"\" if important_text == None else important_text\n",
    "\n",
    "        with open(important_text_path, 'w') as stream:\n",
    "            stream.write(important_text)\n",
    "\n",
    "        return important_text\n",
    "    \n",
    "    with open(important_text_path, 'r') as stream:\n",
    "        important_text = stream.read()\n",
    "        \n",
    "    important_text = None if important_text == \"\" else important_text\n",
    "\n",
    "    return important_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text_and_get_safety_issues(path, reports_ids):\n",
    "    \"\"\"\n",
    "    This will go through a folder and extract all of the safety issues that it can from the reports using regex. Then it will also remove those extracted safety issues from the report text and report a DataFrame that has the report_id, the safety issues extracted, and the cleaned report text.\n",
    "    \"\"\"\n",
    "    safety_issues_raw = []\n",
    "\n",
    "    for report in reports_ids:\n",
    "        with open(os.path.join(path, report, f'{report}.txt'), 'r') as stream:\n",
    "            text = stream.read()\n",
    "\n",
    "        # Check to see if the important text exists\n",
    "\n",
    "        important_text = read_or_create_important_text_file(path, report, text)\n",
    "\n",
    "        if important_text == None:\n",
    "            print(f'Could not extract important text from {report}')\n",
    "            continue\n",
    "        \n",
    "        safety_regex = lambda x: fr's ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s? {{0,3}}{x} {{0,3}}'\n",
    "        end_regex = r'([\\s\\S]+?)(?=(?:\\d+\\.(?:\\d+\\.)?(?:\\d+)?)|(?:^ [A-Z])|(?:s ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s?))'\n",
    "\n",
    "        uncompiled_regexes = [\"(\" + safety_regex(sep) + end_regex + \")\" for sep in [\"-\", \":\"]]\n",
    "\n",
    "        safety_issue_regexes = [re.compile(regex , re.MULTILINE | re.IGNORECASE) for regex in uncompiled_regexes]\n",
    "\n",
    "        safety_issues_for_report = []\n",
    "\n",
    "        cleaned_text = text\n",
    "\n",
    "        matches = [regex.findall(important_text) for regex in safety_issue_regexes]\n",
    "\n",
    "        # Choose one of the matches that has the most matches\n",
    "        matches = max(matches, key=lambda x: len(x))\n",
    "\n",
    "        for full_match, safety_issue_match in matches:\n",
    "            safety_issues_for_report.append(safety_issue_match)\n",
    "            cleaned_text = re.sub(\n",
    "                re.escape(full_match), '',\n",
    "                cleaned_text,\n",
    "                flags=re.IGNORECASE | re.MULTILINE)\n",
    "            \n",
    "        # Remove extracted safety issues that are just summaries in the analysis section introduction\n",
    "        # This prevents the double matching of the safety issues.\n",
    "        for safety_issue in safety_issues_for_report:\n",
    "            if re.search(r'[ï‚·]|(\\uf0b7)', safety_issue, re.MULTILINE | re.IGNORECASE):\n",
    "                print(f\"Remove safety issue summary from {report}\")\n",
    "                safety_issues_for_report.remove(safety_issue)\n",
    "    \n",
    "        safety_issues_raw.append({\n",
    "            \"file\": report,\n",
    "            \"safety_issues\": safety_issues_for_report,\n",
    "            'cleaned_report': cleaned_text})\n",
    "\n",
    "    safety_issues_df = pd.DataFrame(safety_issues_raw)\n",
    "\n",
    "    safety_issues_df['number_extracted'] = safety_issues_df['safety_issues'].apply(lambda x: len(x))\n",
    "\n",
    "    return safety_issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "reports_path = \"output\"\n",
    "\n",
    "reports = os.listdir(reports_path)\n",
    "\n",
    "reports_to_ignore = [\n",
    "    \"2020_102\", # This report has a funny safety issue that is not formatted correctly so cant be picked up by the regex\n",
    "]\n",
    "reports = [report for report in reports if report not in reports_to_ignore]\n",
    "\n",
    "all_reports_df = clean_text_and_get_safety_issues(reports_path, reports)\n",
    "\n",
    "# Remove all reports that have no found safety issues\n",
    "removed_reports = all_reports_df[all_reports_df['number_extracted'] == 0]\n",
    "all_reports_df = all_reports_df[all_reports_df['number_extracted'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get split of 60% train, 20% validation, 20% test\n",
    "seed = 42\n",
    "\n",
    "train_df = all_reports_df.sample(frac=0.6, random_state=seed)\n",
    "\n",
    "validation_df = all_reports_df.drop(train_df.index).sample(frac=0.5, random_state=seed)\n",
    "\n",
    "test_df = all_reports_df.drop(train_df.index).drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was discovered in the testing of the model there is a problem where responses from the fine-tuned model will only have \"inferred\" quality even if they are in fact \"exact\" safety issues.\n",
    "\n",
    "To fix this I will take a small amount of the rows from the train_df and change it so that it is just getting the exact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_exact_examples(df, seed):\n",
    "    df_exact = df.sample(frac=0.2, random_state=seed)\n",
    "\n",
    "    df_exact['uncleaned_report'] = df_exact['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n",
    "    df_exact['expected_response_quality'] = 'exact'\n",
    "\n",
    "    df['expected_response_quality'] = 'inferred'\n",
    "\n",
    "    df_final = pd.concat([df, df_exact]).sort_index()\n",
    "\n",
    "    return df_final\n",
    "\n",
    "train_df = add_random_exact_examples(train_df, seed)\n",
    "\n",
    "validation_df = add_random_exact_examples(validation_df, seed)\n",
    "\n",
    "test_df = add_random_exact_examples(test_df, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data is in three distinct categories it would be prudent to make sure that the resultant data has similar distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three dataframes into one and add a column to indicate the split\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "\n",
    "validation_df['split'] = 'validation'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "all_datasets_df = pd.concat([train_df, validation_df, test_df])\n",
    "\n",
    "all_datasets_df['mode'] = all_datasets_df['file'].apply(lambda x: x.split('_')[1][0])\n",
    "\n",
    "# Group by split and get distribution of mode in each split\n",
    "all_datasets_df.groupby(['split', 'mode']).count().reset_index()[['split', 'mode', 'file']].rename(columns={'file': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not perfect but we can see that it is pretty darn close. The validation set is low on reports. However this is not something that can be completely fixed and aught not to affect the final outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fine tuned model\n",
    "\n",
    "Because of the dismal success of just asking it I am going to give it a couple of examples and then see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions for formatting the data\n",
    "def format_safety_issue_to_yaml(safety_issue, quality):\n",
    "    safety_issues_dict = []\n",
    "    for issue in safety_issue:\n",
    "        safety_issues_dict.append({\"safety_issue\": issue, \"quality\": quality})\n",
    "\n",
    "    return yaml.dump(safety_issues_dict, sort_keys=False)\n",
    "\n",
    "def format_data(data):\n",
    "  formatted_data = []\n",
    "  for index, row in data.iterrows():\n",
    "    formatted_data.append({\"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "You are going help me read a transport accident investigation report.\n",
    "\n",
    "I want you to please read the report and respond with the safety issues identified in the report.\n",
    "\n",
    "Please only respond with safety issues that are quite clearly stated (\"inferred\" safety issues) or implied (\"inferred\" safety issues) in the report. Each report will only contain one type of safety issue.\n",
    "\n",
    "Remember the definitions give\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "â€¢ can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "â€¢ is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "{ReportExtracting.ReportExtractor(row['cleaned_report'] if row['expected_response_quality'] == \"inferred\" else row['uncleaned_report'], row['file']).extract_important_text()[0]}\n",
    "        \n",
    "        \n",
    "=Instructions=\n",
    "\n",
    "I want to know the safety issues which this investigation has found.\n",
    "\n",
    "For each safety issue you find I need to know what is the quality of this safety issue.\n",
    "Some reports will have safety issues explicitly stated with something like \"safety issue - ...\" or \"safety issue: ...\", these are \"exact\" safety issues. Now that the text may have extra spaces or characters in it.\n",
    "\n",
    "However if no safety issues are stated explicitly, then you need to inferred them. These inferred safety issues are \"inferred\" safety issues.\n",
    "\n",
    "\n",
    "Can your response please be in yaml format as shown below.\n",
    "\n",
    "- safety_issue: |\n",
    "    bla bla talking about this and that bla bla bla\n",
    "  quality: exact\n",
    "- safety_issue: |\n",
    "    bla bla talking about this and that bla bla bla\n",
    "  quality: exact\n",
    "\n",
    "\n",
    "There is no need to enclose the yaml in any tags.\n",
    "\n",
    "=Here are some definitions=\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "â€¢ can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "â€¢ is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"{format_safety_issue_to_yaml(row['safety_issues'], row['expected_response_quality'])}\"\"\"\n",
    "        }\n",
    "    ]})\n",
    "\n",
    "  return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and save dataset\n",
    "\n",
    "import json\n",
    "\n",
    "training_data = format_data(train_df)\n",
    "\n",
    "validation_data = format_data(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "with open('training_data.jsonl', 'w') as outfile:\n",
    "    for entry in training_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "with open('validation_data.jsonl', 'w') as outfile:\n",
    "    for entry in validation_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to OpenAI\n",
    "\n",
    "\n",
    "openai_file = client.files.create(\n",
    "  file=open(\"training_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "openai_file_val = client.files.create(\n",
    "  file=open(\"validation_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fine-tune job\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=openai_file.id, \n",
    "  validation_file=openai_file_val.id,\n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I am creating multiple fine-tuned models I should have some reference text to explain what is going on.\n",
    "\n",
    "| name | description |\n",
    "| ---- | ---- |\n",
    "| ft:gpt-3.5-turbo-0125:personal::99hWfq4f | second model fined tuned. This is the first model that uses the full 70 or so explicit reports with redacted safety issues. After fine tuning I tweaked the prompts and got it to 73% matched exact vs inferred. With just over half of the validation set having all of its respective safety issues matched by the inferred. Exact vs exact was at 100% |\n",
    "| gpt-4-turbo (untrained) | Just to store the metrics somewhere I will test it on gpt 4 turbo. I got about 83% on average match with over 77% perfect match.|\n",
    "| ft:gpt-3.5-turbo-0125:personal::9AU7vXs3 | This is my third ft model. It is trained on the same data base as the previous one. However it is differnet in two ways. The prompts in the training data are differnet and there are some exmaples included of what it is like to have exact safety issues.  Out of the box I have it at 61% match percent and about just over half are fully matched. However I am starting to doubt that the comparer is being reaosnable. I.e I think it might be being too hard.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this I am working on fine tuning a model so that I can accurately extract the safety issues from the report.\n",
    "\n",
    "I am getting to the point of thinking that it might not be possible to extract them 100% accurately.\n",
    "\n",
    "Instead maybe I should just focus on making a good distinction between exact and inferred.\n",
    "\n",
    "False news!\n",
    "\n",
    "False news!\n",
    "\n",
    "I had made a mistake and moved over the wrong query request to the new ft model.\n",
    "\n",
    "After checking it on the training data it is about 50% accurate which is pretty good.\n",
    "I will now check it on some new data that the training hasn't seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing fine-tuned model\n",
    "\n",
    "I will now test the fine-tuned model and see how it fares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and comparing datasets inferred and exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine.OpenAICaller as OpenAICaller\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "def compare_safety_issues(safety_issues, inferred_safety_issues):\n",
    "    \"\"\"\n",
    "    This will receive two lists of safety issues and then ask the LLM to compare each safety issue and see if they are the same or different.\n",
    "    \"\"\"\n",
    "\n",
    "    pairs = [(i, j) for i, _ in enumerate(safety_issues) for j, _ in enumerate(inferred_safety_issues)]\n",
    "\n",
    "    pairs_comparison_results = []\n",
    "\n",
    "    for extracted, inferred in pairs:\n",
    "        response = openAICaller.query(\n",
    "            \"\"\"\n",
    "You are going to help me compare if safety issues are the same.\n",
    "\n",
    "You will be given two safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that they are the same safety issue and maybe just worded differently. Whereas different will mean that they are fundamentally different safety issues.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "    Here is the first safety issues:\n",
    "    {safety_issues[extracted]}\n",
    "\n",
    "    Here is the second safety issues:\n",
    "    {inferred_safety_issues[inferred]}\n",
    "    \"\"\",\n",
    "            temp =  0,\n",
    "            model = \"gpt-4\"\n",
    "        ).lower()\n",
    "\n",
    "        if response != 'yes' and response != 'no':\n",
    "            print(f\"Error response in the wrong format {response}\")\n",
    "            response = 'undetermined'\n",
    "    \n",
    "        pairs_comparison_results.append({\n",
    "            'pair': (extracted, inferred),\n",
    "            'result': response\n",
    "        })\n",
    "    return pairs_comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt used to compare two lists of safety themes.\n",
    "```\n",
    "You are going to help me compare if these listed safety issues are the same.\n",
    "\n",
    "You will be given two lists of safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that for each safety issue in the set it will match a safety issue found in the other set albeit if worded slightly differently.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "Here is the first set of safety issues:\n",
    "{safety_issues}\n",
    "\n",
    "Here is the second set of safety issues:\n",
    "{inferred_safety_issues}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_safety_issues_df(df):\n",
    "    # Compare safety issues with the LLM\n",
    "    df['same_comparison_results'] = df.apply(\n",
    "        lambda x: compare_safety_issues(x['safety_issues'],\n",
    "                                        x['inferred_safety_issues']),\n",
    "        axis=1)\n",
    "    \n",
    "    df['inferred_safety_issue_quality'] = df['inferred_safety_issues_struct'].apply(lambda issues: next(iter(set([x['quality'] for x in issues]))))\n",
    "\n",
    "    df['same_quality'] = df['inferred_safety_issue_quality'] == df['expected_response_quality']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_safety_issues_from_cleaned_reports(df):\n",
    "    \"\"\"\n",
    "    This will call the inference function and extract the safety issues from the cleaned reports.\n",
    "    \"\"\"\n",
    "    df['inferred_safety_issues_struct'] = df.apply(lambda x: ReportExtracting.SafetyIssueExtractor(x['cleaned_report'], x['file'])._extract_safety_issues_with_inference(), axis=1)\n",
    "\n",
    "    # Extract the safety issues from the data structured returned from the inferences extraction function.\n",
    "    df['inferred_safety_issues'] = df['inferred_safety_issues_struct'].apply(lambda x: [issue['safety_issue'] for issue in x] if (x is not None and isinstance(x[0], dict)) else None if x is None else x)\n",
    "\n",
    "    df['number_inferred'] = df['inferred_safety_issues'].apply(lambda x: len(x) if x is not None else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import textwrap\n",
    "\n",
    "def make_match_visualization(df, safety_issue_text_dict, report_id):\n",
    "    '''\n",
    "    Create a picture that shows both the extracted and inferred safety issues and the matches between them as lines/arrows.\n",
    "    '''\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes for the 'extracted' and 'inferred' indices\n",
    "    extracted_indices = df['extracted'].unique()\n",
    "    inferred_indices = df['inferred'].unique()\n",
    "\n",
    "    NODE_WIDTH = 50\n",
    "\n",
    "    for i, index in enumerate(extracted_indices):\n",
    "        node_label = '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"extracted\"][index], width=NODE_WIDTH))\n",
    "        G.add_node(node_label, pos=(0, i))\n",
    "\n",
    "    for i, index in enumerate(inferred_indices):\n",
    "        node_label = '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"inferred\"][index], width=NODE_WIDTH))\n",
    "        G.add_node(node_label, pos=(3, i))\n",
    "\n",
    "    # Add edges between the matched indices\n",
    "    for _, row in df.iterrows():\n",
    "        if row['result'] == 'yes':\n",
    "            G.add_edge('\\n'.join(textwrap.wrap(safety_issue_text_dict[\"extracted\"][row[\"extracted\"]], width=NODE_WIDTH)), \n",
    "                        '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"inferred\"][row[\"inferred\"]], width=NODE_WIDTH)))\n",
    "\n",
    "    max_num_of_nodes_column = max(\n",
    "        len(safety_issue_text_dict['extracted']),\n",
    "        len(safety_issue_text_dict['inferred'])\n",
    "        )\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    plt.figure(figsize=(8, max_num_of_nodes_column * 2.5))  \n",
    "    nx.draw(G, pos, with_labels=False, node_color='lightblue', node_size=[len(node) * NODE_WIDTH for node in G.nodes()])\n",
    "    nx.draw_networkx_labels(G, pos, font_size=7)\n",
    "    \n",
    "    plt.xlim(-1, 4.5)  # Add buffer to the outside side edges\n",
    "    plt.ylim(-1, max_num_of_nodes_column*1)  # Add buffer to the outside top and bottom edges\n",
    "\n",
    "    # Add report ID and headers\n",
    "    plt.title(f'Report ID: {report_id}')\n",
    "    plt.text(0, max_num_of_nodes_column-0.2, 'Real', fontsize=12, ha='center')\n",
    "    plt.text(3, max_num_of_nodes_column-0.2, 'Inferred', fontsize=12, ha='center')\n",
    "    plt.text(1.5, max_num_of_nodes_column-0.5, 'Arrow indicates that real has been \"matched\" by LLM with inferred', fontsize=6, ha='center')\n",
    "\n",
    "    plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_comparison_of_reports_SI(row, with_visual_generation = True):\n",
    "    '''\n",
    "    Take a row of the DataFrame with the comparison results and interpret them by giving you a match percentage. This percentage is how 'similar' the extracted safety issues are to the inferred safety issues. However similar is hard to define so currently it is simply how many of the extracted safety issues have a match.\n",
    "    '''\n",
    "    result = row['same_comparison_results']\n",
    "\n",
    "    # Make it a DataFrame\n",
    "    result_df = pd.DataFrame(result)\n",
    "    # Split tuple into two columns\n",
    "    result_df['extracted'] = result_df['pair'].apply(lambda x: x[0])\n",
    "    result_df['inferred'] = result_df['pair'].apply(lambda x: x[1])\n",
    "\n",
    "    num_extracted_safety_issues = result_df['extracted'].unique().shape[0]\n",
    "    num_inferred_safety_issues = result_df['inferred'].unique().shape[0]\n",
    "\n",
    "    # Interpret the results\n",
    "    # Make sure that each exact safety issues is matched to only one inferred safety issue\n",
    "\n",
    "    inferred_safety_issues_matched = [False] * num_inferred_safety_issues\n",
    "    safety_issues_matched = [False] * num_extracted_safety_issues\n",
    "\n",
    "    for pair_comparison in result:\n",
    "        if pair_comparison['result'] == 'yes':\n",
    "            inferred_safety_issues_matched[pair_comparison['pair'][1]] = True\n",
    "            safety_issues_matched[pair_comparison['pair'][0]] = True\n",
    "\n",
    "    match_percent = sum(safety_issues_matched) / num_extracted_safety_issues\n",
    "\n",
    "    if with_visual_generation:\n",
    "        safety_issue_text ={\n",
    "            \"extracted\": [row['safety_issues'][index] for index in result_df['extracted'].unique()],\n",
    "            \"inferred\": [row['inferred_safety_issues'][index] for index in result_df['inferred'].unique()]\n",
    "        }\n",
    "\n",
    "        make_match_visualization(result_df, safety_issue_text, row['file'])\n",
    "        \n",
    "\n",
    "    return match_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_dataset_SI_comparison(dataset):\n",
    "    \"\"\"\n",
    "    The dataset is going to have a data on the safety issues extracted and inferred as well as the comparison results of these safety issues. This function will look at the while results and give you a summary of what is happening.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['SI_match_percent'] = dataset.apply(lambda row: interpret_comparison_of_reports_SI(row, True), axis = 1)\n",
    "    dataset['SI_match'] = dataset['SI_match_percent'] == 1\n",
    "\n",
    "    dataset['SI_count_match'] = dataset['number_extracted'] == dataset['number_inferred']\n",
    "    dataset['SI_count_difference'] = dataset['number_extracted'] - dataset['number_inferred']\n",
    "\n",
    "    print(f\"\"\"\n",
    "==============                                   ==============\n",
    "                Comparing Safety Issues results\n",
    "==============                                   ==============\n",
    "          \n",
    "There are {dataset.shape[0]} reports in the dataset.\n",
    "    \n",
    "Average match percentage: {round(dataset['SI_match_percent'].mean(), 2)}\n",
    "Amount of all matched safety issues: {dataset['SI_match'].sum()/dataset.shape[0]}\n",
    "    This is the percentage of extracted safety issues that have been matched to inferred safety issues.\n",
    "Information on the number of safety issues extracted and inferred:\n",
    "          \"\"\")\n",
    "    \n",
    "    display(\n",
    "        pd.DataFrame([\n",
    "            ['same number of extracted and inferred safety issues', dataset['SI_count_match'].mean()],\n",
    "            ['Average match of same count of SI', dataset[dataset['SI_count_match']]['SI_match_percent'].mean()],\n",
    "            ['Average difference in count of SI', dataset['SI_count_difference'].mean()],\n",
    "            ['Average number of extracted SI', dataset['number_extracted'].mean()],\n",
    "            ['Average number of inferred SI', dataset['number_inferred'].mean()]\n",
    "        ], columns=[\"description\", \"statistic\"]).style \\\n",
    "            .format(precision=2) \\\n",
    "            .hide(axis=\"index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a dataset\n",
    "I will compare the models responses with the exact extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(OpenAICaller)\n",
    "\n",
    "# Infer safety issues\n",
    "validation_df_with_SI_Inference = extract_safety_issues_from_cleaned_reports(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare safety issues\n",
    "validation_df_with_SI_Inference = compare_safety_issues_df(validation_df_with_SI_Inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how well they compared\n",
    "interpret_dataset_SI_comparison(validation_df_with_SI_Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there is some interesting performance going on.\n",
    "\n",
    "We have about 0.54 percent of the exact ones have a match. However there are problems where there is about half an extra safety issue inferred for every safety issues extracted. That is only about a 1/3 of the time does it generate the same amount of safety issues. However when it does there is a 60% chance they will all match up.\n",
    "\n",
    "I need to still do 2 things before the completion of the safety issue extraction upgrade:\n",
    "\n",
    "- [x] Make sure that the comparison of the extracted and inferred safety issues are reasonable.\n",
    "- [x] Check whether it does a good job getting the difference between inferred and exact safety issues\n",
    "To be completed with a re fine-tune of the model with giving it some new examples  that include ones where the safety issues are actaully exact because I havent redacted them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of safety issues reasonable\n",
    "\n",
    "I will start by just looking at the validation_df and the 15 examples there and see if I agree with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly will go through and make manual comparisons of the extracted and inferred safety issues\n",
    "\n",
    "with open('manual_comparison.txt', 'w') as file:\n",
    "    for index, row in validation_df_with_SI_Inference.iterrows():\n",
    "        file.write(f\"\"\"\n",
    "=============================================================================\n",
    "                   Report: {row['file']}\n",
    "=============================================================================\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Extracted Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Inferred Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['inferred_safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_comparison = [\n",
    "    {\n",
    "        \"file\": \"2013_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count extra\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_204\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Here the extracted safety issues have also captured a summary of hte safety issues at the top of the analysis. This means that there are 5 where there should only be 4 and the first one is currently a list of all of them.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_005\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The wording is different but it is similar enough. However the inferred one does not 'aviation industry practice'\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_108\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The first ones match however the inferred one is much longer and even gives a reference to a document.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_006\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        'notes': \"Completely different safety issues\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2017_203\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"The one inferred one is an exact match and I wonder if it was not removed prior to the inference\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_101\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"They are similar but not the same\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_003\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred safety issues are actually correct as there is a summary which it has read from.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_202\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"The inferred is more specific about what equipment but is more vague when it comes to defining what the problem to be mitigated was.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred are actually exact ones lifted from the report\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2012_103\",\n",
    "        \"same\": \"yes\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Two of the safety issues are exact however it should of extracted 4 exact issues.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made me noted that actaully I should have a look at which ones are be extracted exactly or inferred. But first I need to compare this with what my comparision function says"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df_with_SI_Inference['manual_same'] = validation_df_with_SI_Inference['file'].apply(lambda x: next((item['same'] for item in manual_comparison if item[\"file\"] == x), None))\n",
    "\n",
    "# Show cases where the manual comparison is different to the LLM comparison\n",
    "validation_df_with_SI_Inference[validation_df_with_SI_Inference['same'] != validation_df_with_SI_Inference['manual_same']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a look at this I have changed the comparision to be per safety issue.\n",
    "\n",
    "Looking at the charts that are produced it seems to do a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between exact and inferred\n",
    "\n",
    "The Model is going to be asked to read the important part of a report and extract the safety issues there will be two sorts of safety issues extracted.\n",
    "\n",
    "Firstly will be the exact safety issues. These are where the report explicitly states the safety issues. Then there will be inferred which is where no exact safety issues can be found. Then the model will \"invent\" some.\n",
    "\n",
    "This needs to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any reports that have exact safety issues that are inferred\n",
    "\n",
    "for _, row in validation_df_with_SI_Inference.iterrows():\n",
    "    \n",
    "    for issue in row['inferred_safety_issues_struct']:\n",
    "        if issue['quality'] == 'exact':\n",
    "            print(f\"Report {row.file} has an exact safety issue that is inferred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no exact safety issues from the validation which makes sense as the all the of the validation report have ha the safety issues removed.\n",
    "\n",
    "I could test this by seeing what happenings if I get a report that should have a exact safety issues in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "\n",
    "# Getting some examples of exact safety issues that are found using the model\n",
    "reports_path = \"output\"\n",
    "\n",
    "comparing_with_exact = validation_df.copy()\n",
    "\n",
    "comparing_with_exact['cleaned_report'] = comparing_with_exact['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n",
    "comparing_with_exact = extract_safety_issues_from_cleaned_reports(comparing_with_exact)\n",
    "\n",
    "comparing_with_exact = compare_safety_issues_df(comparing_with_exact)\n",
    "\n",
    "interpret_dataset_SI_comparison(comparing_with_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that it does a good job of extracted safety issues even if they are mentioned exactly. However two things. Some explicit safety issues are missing. Secondly the safety quality is not being correctly identified as exact.\n",
    "\n",
    "The problem of explicit safety issues missed are solved by: 735d98b1e122b66c229b93021c266424af5cd9d4\n",
    "\n",
    "Where as the incorrect labelling will be fixed by a fine tuning of the model with new data that has some \"exact\" safety issue examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to plain gpt-4-turbo\n",
    "\n",
    "As the fine tuning model is expensive to train I was interested in trying gpt 4 now with all the changes in the prompt.\n",
    "\n",
    "It is doing quite a good job and so I might try use it.\n",
    "\n",
    "I am getting about 77% completely matched reports (that is that all the safety issues in a report is matched with at least one of the inferred safety issues.)\n",
    "\n",
    "This was quite a surprise however now it is listing all of the safety issue qualities as exact. Therefore I will update the prompt and see what happens. After trying again and again with the prompts I have realized that i just cant ge it to do want and instead it is making it worse.\n",
    "\n",
    "I will fine-tune the data with the updated prompts and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(OpenAICaller)\n",
    "\n",
    "# For this unique test I will change the df so that it has the uncleaned reports in where the cleaned reports should be. THis is just to test that it can do both.\n",
    "\n",
    "validation_df_gpt4 = validation_df.copy()\n",
    "\n",
    "# Only replace if uncleaned_report exists\n",
    "validation_df_gpt4['cleaned_report'] = validation_df_gpt4.apply(lambda row: row['cleaned_report'] if row['expected_response_quality'] != \"exact\" else row['uncleaned_report'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Infer safety issues\n",
    "validation_df_gpt4_with_inference = extract_safety_issues_from_cleaned_reports(validation_df_gpt4)\n",
    "\n",
    "# Compare safety issues\n",
    "validation_df_gpt4_with_inference = compare_safety_issues_df(validation_df_gpt4_with_inference)\n",
    "\n",
    "# Understand how well they compared\n",
    "interpret_dataset_SI_comparison(validation_df_gpt4_with_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the safety issue quality with expected and results\n",
    "\n",
    "def find_mismatched_quality_issues(df):\n",
    "    df['inferred_safety_issue_quality'] = df['inferred_safety_issues_struct'].apply(lambda issues: next(iter(set([x['quality'] for x in issues]))))\n",
    "\n",
    "    return df[df['inferred_safety_issue_quality'] != df['expected_response_quality']]\n",
    "\n",
    "mismatched_reports = find_mismatched_quality_issues(validation_df_gpt4_with_inference)['file'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looking = validation_df_gpt4_with_inference[validation_df_gpt4_with_inference['file'].isin(mismatched_reports)]\n",
    "looking = looking[looking['expected_response_quality'] == \"inferred\"]\n",
    "\n",
    "interpret_dataset_SI_comparison(looking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a little problem with the fact that the data extraction of the safety issues is not as good as it should be.\n",
    "\n",
    "That means that the \"cleaned reports\" and number extracted etc can't actually be fully trusted.\n",
    "Therefore I think that it means there are better results than first thought.\n",
    "\n",
    "I am going to go through the validation set mismatches and list whether the extracted or the inferred ones are more correct. This is quite a manual process but needs to be done.\n",
    "\n",
    "| report_id | which one was correct | notes |\n",
    "| ------ | ----- | ---- |\n",
    "| 2011_003 | inferred | there was a summary in analysis that was not removed |\n",
    "| 2012_105  | exact | This one is confusing as in the analysis section it does state the safety issues arising are. This is where the first safety issue comes from the others are from the final \"findings\" section. Thus they are findings and not safety issues. |\n",
    "| 2020_002 | exact| It seemingly is grabbing randoms paragraphs and stating them as safety issues. |\n",
    "| 2015_102 | inferred | Getting its information from the analysis introduction |\n",
    "| 2012_103 | exact | The inferred is saying more or less the same thing. However it is just copied the text from the findings section. |\n",
    "| 2019_005 | exact | There is a summary in Analysis section but this misses out on two non causal safety issues | \n",
    "| 2017_202 | exact | There is a summary in the analysis section however it does not extract from there. Instead it pulls most of the them from the findings section. Also the text extraction did not work perfectly. |\n",
    "| 2011_204 | exact | The inferred is just grabbing from the findings section. |\n",
    "| 2013_106 | exact | There is a summary that outlines what the safety issues are. But it has just taken the findings. |\n",
    "| 2017_101 | inferred | Summarary in the analysis section |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After changing the prompts I will rerun it on just the reports that are mismatched\n",
    "\n",
    "importlib.reload(ReportExtracting)\n",
    "\n",
    "# Only get mismatched _reports\n",
    "mismatched_reports_df = validation_df_gpt4[validation_df_gpt4['file'].isin(mismatched_reports)]\n",
    "mismatched_reports_df = mismatched_reports_df[mismatched_reports_df['expected_response_quality'] == \"inferred\"]\n",
    "\n",
    "mismatched_reports_df_with_inference = extract_safety_issues_from_cleaned_reports(mismatched_reports_df)\n",
    "print(\"Safety issues extracted\")\n",
    "# Compare safety issues\n",
    "mismatched_reports_df_with_inference = compare_safety_issues_df(mismatched_reports_df_with_inference)\n",
    "print(\"Comparisons made\")\n",
    "# Understand how well they compared\n",
    "interpret_dataset_SI_comparison(mismatched_reports_df_with_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending off dataset to chris and ingrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "reports_path = \"output\"\n",
    "\n",
    "reports = os.listdir(reports_path)\n",
    "\n",
    "reports_to_ignore = [\n",
    "    \"2020_102\", # This report has a funny safety issue that is not formatted correctly so cant be picked up by the regex\n",
    "]\n",
    "reports = [report for report in reports if report not in reports_to_ignore]\n",
    "\n",
    "all_reports_df = clean_text_and_get_safety_issues(reports_path, reports)\n",
    "\n",
    "reports_to_send = all_reports_df.sample(50, random_state=42)\n",
    "\n",
    "reports_to_send['cleaned_report'] = reports_to_send['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reports_to_send_with_safety_issues = extract_safety_issues_from_cleaned_reports(reports_to_send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_df = reports_to_send_with_safety_issues[['file', 'inferred_safety_issues_struct']]\n",
    "\n",
    "# Make minimum_df longer by only having one safety issues per row. As currently safety issues are stored in a list\n",
    "\n",
    "minimum_df = minimum_df.explode('inferred_safety_issues_struct')\n",
    "\n",
    "#Remove the rows that have no safety issues\n",
    "minimum_df = minimum_df[minimum_df['inferred_safety_issues_struct'].notna()]\n",
    "\n",
    "minimum_df['quality'] = minimum_df['inferred_safety_issues_struct'].apply(lambda x: x['quality'])\n",
    "minimum_df['safety_issue'] = minimum_df['inferred_safety_issues_struct'].apply(lambda x: x['safety_issue'])\n",
    "\n",
    "minimum_df[['file', 'safety_issue', 'quality']].to_excel('inferred_safety_issues.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a meeting it was decided that it is good enough at its about 80% accuracy. Before I merge I just want to check a few things\n",
    "\n",
    "- That is still does a good job with the exact issue extraction\n",
    "- What the testing dataset has to say about accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that exact safety issue extraction is reasonable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will just use the validation set and add in the exact ones and see what it does\n",
    "\n",
    "validation_exact_check_df = validation_df.copy()\n",
    "\n",
    "validation_exact_check_df['cleaned_report'] = validation_exact_check_df['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n",
    "validation_exact_check_df['expected_response_quality'] = 'exact'\n",
    "\n",
    "validation_exact_check_df.drop(labels='uncleaned_report', axis=1, inplace=True)\n",
    "validation_exact_check_df.drop_duplicates(subset=['file'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_exact_check_df_with_inference = extract_safety_issues_from_cleaned_reports(validation_exact_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_exact_check_df_with_inference = compare_safety_issues_df(validation_exact_check_df_with_inference)\n",
    "\n",
    "\n",
    "interpret_dataset_SI_comparison(validation_exact_check_df_with_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this I have gotten very good results.\n",
    "\n",
    "Here are some of the stats\n",
    "\n",
    "same number of extracted and inferred safety issues\t0.71\n",
    "Average match of same count of SI\t1.00\n",
    "Average difference in count of SI\t-0.43\n",
    "Average number of extracted SI\t3.29\n",
    "Average number of inferred SI\t3.71\n",
    "\n",
    "But importantly there is 100% coverage. I can call this good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the test set to get a final expected accuracy\n",
    "\n",
    "Now that I have reached then end I will look at the test set and see what kind of numbers I get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['cleaned_report'] = test_df.apply(lambda row: row['cleaned_report'] if row['expected_response_quality'] == \"inferred\" else row['uncleaned_report'], axis=1)\n",
    "\n",
    "test_df_with_inference = extract_safety_issues_from_cleaned_reports(test_df)\n",
    "\n",
    "test_df_with_inference = compare_safety_issues_df(test_df_with_inference)\n",
    "\n",
    "interpret_dataset_SI_comparison(test_df_with_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the test set and got the results of 86% coverage with 83% of reports completely matched.\n",
    "\n",
    "That is pretty good so I will merge it in for now.\n",
    "\n",
    "description\tstatistic\n",
    "same number of extracted and inferred safety issues\t0.56\n",
    "Average match of same count of SI\t0.90\n",
    "Average difference in count of SI\t-1.28\n",
    "Average number of extracted SI\t2.44\n",
    "Average number of inferred SI\t3.72"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
