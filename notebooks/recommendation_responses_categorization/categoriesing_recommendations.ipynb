{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "As discussed in https://github.com/1jamesthompson1/TAIC-report-summary/issues/138 each recommendation that TAIC issues also has a response. These responses have a few categories.\n",
    "\n",
    "## How to do it\n",
    "\n",
    "I have a dataset file that are all of the recommendations from TAIC. \n",
    "\n",
    "I also have 10 random examples of categorized recommendations.\n",
    "\n",
    "I will start by just asking it and seeing how it does against this exmaples. Then I can use the examples as exmpaels within the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the modules needed\n",
    "\n",
    "To keep things as transparent as possible I will add all of the dependencies at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the engine\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "# Third party\n",
    "import pandas as pd\n",
    "import docx\n",
    "\n",
    "# Built in\n",
    "import os\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_categories = [\n",
    "    {\"category\": \"Accepted and Implemented\", \"definition\": \"The recommendation was accepted (wholly) and has been implemented\"},\n",
    "    {\"category\": \"Accepted\", \"definition\": \"The recommendation was accepted (wholly) and is being, or will be implemented\"},\n",
    "    {\"category\": \"Under consideration\", \"definition\": \"The recipient has acknowledged that the recommendation is received and will consider it.\"},\n",
    "    {\"category\": \"Rejected\", \"definition\": \"The recommendation will not be implemented\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting datasets\n",
    "\n",
    "I will get Ingrid from TAIC to give me some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAIC recommendations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample where recommendation and reply is not empty\n",
    "def clean_TAIC_recommendations(df):\n",
    "    df.dropna(subset=[\"Recommendation\", \"Inquiry\", \"Number\", \"Reply Text\"], inplace = True)\n",
    "    df = df[(df['Made'] >= '2010-01-01')]\n",
    "\n",
    "    # structure the inquiry to be able to match with rest of project\n",
    "    inquiry_regex = r'^(((AO)|(MO)|(RO))-[12][09][987012]\\d-[012]\\d{2})$'\n",
    "    df = df[df['Inquiry'].str.match(inquiry_regex)]\n",
    "    df['report_id'] = df['Inquiry'].apply(lambda x: \"_\".join(x.split('-')[1:3]))\n",
    "    return df\n",
    "\n",
    "taic_recommendations = pd.read_excel('TAIC_recommendations_04_04_2024.xlsx')\n",
    "\n",
    "cleaned_taic_recommendations = clean_TAIC_recommendations(taic_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingrid responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples from Ingrid\n",
    "\n",
    "ingrid_categories = pd.read_excel('example_recommendation_categories_ingrid_responses.xlsx')\n",
    "\n",
    "ingrid_categories['response_category'] = ingrid_categories['response_category'].apply(lambda x: response_categories[x+1]['category'])\n",
    "\n",
    "# Update 011/17 as N/A\n",
    "ingrid_categories.loc[ingrid_categories['Number'] == '011/17', 'response_category'] = \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAIC response categorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tables_to_dataframes(docx_file):\n",
    "    document = docx.Document(docx_file)\n",
    "    tables = document.tables\n",
    "    dataframes = []\n",
    "    for table in tables:\n",
    "        df = pd.DataFrame([[cell.text for cell in row.cells] for row in table.rows])\n",
    "        dataframes.append(df)\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "def process_tables(config):\n",
    "    tables = tables_to_dataframes(config['filename'])\n",
    "\n",
    "    all_tables_df = pd.DataFrame()\n",
    "    for table_idx in config['tables_to_read']:\n",
    "        table = tables[table_idx]\n",
    "\n",
    "        # Turn first row into column names\n",
    "        table.columns = table.iloc[0]\n",
    "\n",
    "        table_df = pd.DataFrame({\n",
    "            \"Number\": table[config['id']], \n",
    "            \"response_category\": table['Response'],\n",
    "                                })\n",
    "        \n",
    "        all_tables_df = pd.concat([all_tables_df, table_df])\n",
    "\n",
    "    # Clean up recommendation column\n",
    "\n",
    "    all_tables_df['Number'] = all_tables_df['Number'].apply(lambda x: re.sub(r'\\n', '', x))\n",
    "\n",
    "    # Remove all rows that dont match recommendation regex\n",
    "\n",
    "    all_tables_df = all_tables_df[all_tables_df['Number'].str.match(r'\\d{3}/\\d{2}')]\n",
    "\n",
    "    # Clean up response_category column\n",
    "\n",
    "    all_tables_df['response_category'] = all_tables_df['response_category'].apply(lambda x: re.sub(r'Accepted and being implemented', 'Accepted', x))\n",
    "\n",
    "\n",
    "    return all_tables_df\n",
    "\n",
    "# Rather than come up with a smart way of doing it I think it would be quicker to just quickly read this and come up with how to read the tables.\n",
    "\n",
    "export_config = [\n",
    "    {\n",
    "        \"filename\": \"2023-06-30.SR.Report.to.Minister.docx\",\n",
    "        \"tables_to_read\": [0, 2, 4, 6],\n",
    "        \"id\": \"Number\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"2022-06-30.SR.Report.to.Minister.docx\",\n",
    "        \"tables_to_read\": [1, 3, 5],\n",
    "        \"id\": \"Rec no.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "official_responses =pd.concat([process_tables(config) for config in export_config], ignore_index=True)\n",
    "\n",
    "# Drop empty cateories\n",
    "\n",
    "official_responses = official_responses[official_responses['response_category'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge exmaples and all recommendations togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ingrid and official responses\n",
    "\n",
    "example_categories = official_responses.merge(ingrid_categories[['Number', 'response_category']], how='outer', indicator='origin')\n",
    "\n",
    "example_categories['origin'] = example_categories['origin'].apply(lambda x: 'official' if x == 'left_only' else 'ingrid')\n",
    "\n",
    "example_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge example categories with TAIC recommendations\n",
    "\n",
    "recommendations_df = cleaned_taic_recommendations.merge(example_categories, how='left')\n",
    "\n",
    "recommendations_df = recommendations_df[['report_id', 'Number', 'Recommendation', 'Reply Text' ,'Made', 'Recipient', 'response_category', 'origin']]\n",
    "\n",
    "# change all columns to lower case and repalce spaces with _\n",
    "recommendations_df.columns = recommendations_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing categorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_response_category(response, recommendation, recommendation_num):\n",
    "    categories = '\\n'.join([f\"{element['category']} - {element['definition']}\" for element in response_categories])\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are helping me put responses into categories.\n",
    "\n",
    "These responses are to recommendations that were made in a transport accident investigation report. These recommendations are issued directly to a particular party.\n",
    "\n",
    "There are three categories:\n",
    "\n",
    "{categories}\n",
    "\n",
    "However if there are responses that don't fit into any of the categories then you can put them as N/A. These may be responses that request further information or want recommendation to be sent elsewhere.\n",
    "\n",
    "Your response should just be the name of the category with nothing else.\n",
    "\"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "Which category is this response in?\n",
    "\n",
    "\"\n",
    "{response}\n",
    "\"\n",
    "\n",
    "in regards to recommendation {recommendation_num}\n",
    "\"\"\"\n",
    "\n",
    "    # print(f\"system prompt is:\\n{system_prompt} and user prompt is:\\n{user_prompt}\")\n",
    "\n",
    "    openai_response = openAICaller.query(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        model = \"gpt-4\",\n",
    "        temp = 0\n",
    "    )\n",
    "\n",
    "    if openai_response in [category['category'] for category in response_categories] + ['N/A']:\n",
    "        return openai_response\n",
    "    else:\n",
    "        print(f\"Did not match any of the categories - {openai_response}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_recommendations_df = recommendations_df.sample(25, random_state=42)\n",
    "\n",
    "sample_recommendations_df['response_category'] = sample_recommendations_df.apply(lambda x: assign_response_category(x['reply_text'], x['number']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples_recommendations_df = recommendations_df[~recommendations_df['response_category'].isna()]\n",
    "examples_recommendations_df['response_category_inferred'] = examples_recommendations_df.apply(lambda x: assign_response_category(x['reply_text'], x['recommendation'], x['number']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results\n",
    "\n",
    "def printout_accuracy(df):\n",
    "    # Compare columns case insenstive\n",
    "\n",
    "    df['matching'] = df['response_category'].str.lower() == df['response_category_inferred'].str.lower()\n",
    "\n",
    "    changed_df = df[df['matching'] == False]\n",
    "\n",
    "    print(f\"{100* (len(df)-len(changed_df)) / len(df):.2f}% accuracy when comparing {len(df)} responses\")\n",
    "\n",
    "    changed_df['change'] = changed_df['response_category'] + ' -> ' + changed_df['response_category_inferred']\n",
    "\n",
    "    value_counts = changed_df['change'].value_counts()\n",
    "\n",
    "    # Sort this series by the value of the left side then the right in the response_categories list\n",
    "\n",
    "    display(value_counts)\n",
    "\n",
    "    return changed_df\n",
    "\n",
    "\n",
    "mismatched_categories = printout_accuracy(examples_recommendations_df)[['report_id', 'number', 'recommendation', 'reply_text', 'response_category', 'response_category_inferred', 'origin']]\n",
    "\n",
    "mismatched_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the good results I will do a complete run through for their sake\n",
    "\n",
    "recommendations_df['response_category'] = recommendations_df.apply(lambda x: (x['reply_text'], x['number']), axis=1)\n",
    "\n",
    "recommendations_df[[\"number\", \"report_id\", \"recommendation\", \"reply_text\", \"response_category\"]].to_excel('recommendation_categories.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df.groupby('response_category').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised classification\n",
    "\n",
    "I am going to try out some unsupervised classifications.\n",
    "\n",
    "I also want to try clustering of the responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lbl2Vec\n",
    "\n",
    "After working with this for a bit it didn't seem to be working. Due to this error https://github.com/sebischair/Lbl2Vec/issues/4. Until this is resolved I will have to move on to other techniques.\n",
    "\n",
    "This technique will involve me defining some categories based off keywords. It will then learn from unlabeled texts. This unlabeled text is exactly what I have here.\n",
    "\n",
    "Two concerns, I have a very small data set and I have quite short text in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame([\n",
    "    {\n",
    "        'class_name': \"Accepted\",\n",
    "        'keywords': ['accepted', 'implemented', 'accept', 'implement']\n",
    "    },\n",
    "    {\n",
    "        'class_name': \"Under consideration\",\n",
    "        'keywords': ['under', 'consideration', 'consider']\n",
    "    },\n",
    "    {\n",
    "        'class_name': 'Rejected',\n",
    "        'keywords': ['rejected']\n",
    "    }\n",
    "])\n",
    "\n",
    "labels['numbers_of_keywords'] = labels['keywords'].apply(lambda x: len(x))\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "importlib.reload(gensim)\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# doc: document text string\n",
    "# returns tokenized document\n",
    "# strip_tags removes meta tags from the text\n",
    "# simple preprocess converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long \n",
    "# simple preprocess also removes numerical values as well as punktuation characters\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "# Split into train and test\n",
    "test_df = cleaned_taic_recommendations.sample(10, random_state=42)\n",
    "train_df = cleaned_taic_recommendations[~cleaned_taic_recommendations['Number'].isin(sample_df['Number'])]\n",
    "\n",
    "test_df['data_set_type'] = \"test\"\n",
    "train_df['data_set_type'] = \"train\"\n",
    "\n",
    "# concat train and test data\n",
    "full_corpus = pd.concat([train_df,test_df]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# tokenize and tag documents for Lbl2Vec training\n",
    "full_corpus['tagged_responses'] = full_corpus.apply(lambda row: TaggedDocument(tokenize(row['Reply Text']), [str(row.name)]), axis=1)\n",
    "\n",
    "# add doc_key column\n",
    "full_corpus['doc_key'] = full_corpus.index.astype(str)\n",
    "\n",
    "display(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbl2vec import Lbl2Vec\n",
    "\n",
    "# init model with parameters\n",
    "Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels.keywords), tagged_documents=full_corpus['tagged_responses'][full_corpus['data_set_type'] == 'train'], label_names=list(labels.class_name), similarity_threshold=0.43, min_num_docs=5, epochs=10)\n",
    "\n",
    "# train model\n",
    "Lbl2Vec_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "I will just try to cluster them using normal techniques.\n",
    "\n",
    "I tried using clustering which was a bit of fun but not necessary as I have gotten good results with LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your responses are stored in a list called 'responses'\n",
    "# Clean the responses by removing punctuation and converting to lowercase\n",
    "clustered_recommendations = taic_recommendations.copy()\n",
    "\n",
    "clustered_recommendations = clustered_recommendations[clustered_recommendations['Made'] >= '2010-01-01']\n",
    "\n",
    "\n",
    "cleaned_responses = [response.lower().replace('.', '').replace(',', '') for response in clustered_recommendations['Reply Text']]\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_responses = [word_tokenize(response) for response in cleaned_responses]\n",
    "filtered_responses = [[word for word in tokenized_response if word not in stop_words] for tokenized_response in tokenized_responses]\n",
    "\n",
    "\n",
    "preprocessed_responses = [' '.join(filtered_response) for filtered_response in filtered_responses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the transform\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_clusters = 3\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "print(\"Average silhouette_score:\", silhouette_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clustered_recommendations['cluster'] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_matrix_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Visualize clusters\n",
    "plt.scatter(tfidf_matrix_pca[:, 0], tfidf_matrix_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('Clustering of Responses')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with the examples from Ingrid\n",
    "\n",
    "ingrid_categories['cluster'] = ingrid_categories['Reply Text'].apply(lambda x: kmeans.predict(tfidf_vectorizer.transform([x]).toarray()))\n",
    "\n",
    "ingrid_categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
