{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "As discussed in https://github.com/1jamesthompson1/TAIC-report-summary/issues/138 each recommendation that TAIC issues also has a response. These responses have a few categories.\n",
    "\n",
    "## How to do it\n",
    "\n",
    "I have a dataset file that are all of the recommendations from TAIC. \n",
    "\n",
    "I also have 10 random examples of categorized recommendations.\n",
    "\n",
    "I will start by just asking it and seeing how it does against this exmaples. Then I can use the examples as exmpaels within the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the modules needed\n",
    "\n",
    "To keep things as transparent as possible I will add all of the dependencies at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the engine\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "# Third party\n",
    "import pandas as pd\n",
    "\n",
    "# Built in\n",
    "import os\n",
    "import re\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_categories = [\n",
    "    {\"category\": \"Accepted and Implemented\", \"definition\": \"The recommendation was accepted (wholly) and has been implemented\"},\n",
    "    {\"category\": \"Accepted\", \"definition\": \"The recommendation was accepted (wholly) and is being, or will be implemented\"},\n",
    "    {\"category\": \"Under consideration\", \"definition\": \"The recipient has acknowledged that the recommendation is received and will consider it.\"},\n",
    "    {\"category\": \"Rejected\", \"definition\": \"The recommendation will not be implemented\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting example dataset\n",
    "\n",
    "I will get Ingrid from TAIC to give me some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Recommendations 2024-04-04.xlsx into dataframe\n",
    "\n",
    "taic_recommendations = pd.read_excel('TAIC_recommendations_04_04_2024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample where recommendation and reply is not empty\n",
    "def clean_TAIC_recommendations(df):\n",
    "    df.dropna(subset=[\"Recommendation\", \"Inquiry\", \"Number\", \"Reply Text\"], inplace = True)\n",
    "    df = df[(df['Made'] >= '2010-01-01')]\n",
    "\n",
    "    # structure the inquiry to be able to match with rest of project\n",
    "    inquiry_regex = r'^(((AO)|(MO)|(RO))-[12][09][987012]\\d-[012]\\d{2})$'\n",
    "    df = df[df['Inquiry'].str.match(inquiry_regex)]\n",
    "    df['report_id'] = df['Inquiry'].apply(lambda x: \"_\".join(x.split('-')[1:3]))\n",
    "    return df\n",
    "\n",
    "cleaned_taic_recommendations = clean_TAIC_recommendations(taic_recommendations)\n",
    "\n",
    "sample_df = cleaned_taic_recommendations.sample(10, random_state=42)\n",
    "\n",
    "sample_df['response_category'] = \"To be filled out\"\n",
    "\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get defintions read\n",
    "\n",
    "df_definitions = pd.DataFrame(response_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printout sample df to excel to be sent to Ingrid\n",
    "with pd.ExcelWriter('example_recommendation_categories_ingrid.xlsx', engine='openpyxl') as writer:\n",
    "    sample_df.to_excel(writer, sheet_name='Recommendations')\n",
    "    df_definitions.to_excel(writer, sheet_name='Definitions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing categorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples from Ingrid\n",
    "\n",
    "ingrid_categories = pd.read_excel('example_recommendation_categories_ingrid_responses.xlsx')\n",
    "\n",
    "ingrid_categories['response_category_example'] = ingrid_categories['response_category'].apply(lambda x: response_categories[x+1]['category'])\n",
    "\n",
    "ingrid_categories.drop(columns=['response_category'], inplace=True)\n",
    "\n",
    "# Update 011/17 as N/A\n",
    "ingrid_categories.loc[ingrid_categories['Number'] == '011/17', 'response_category_example'] = \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all recommendations from TAIC\n",
    "\n",
    "taic_recommendations_cleaned = clean_TAIC_recommendations(taic_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets together\n",
    "\n",
    "recommendations_df = taic_recommendations_cleaned.merge(ingrid_categories, how='outer')\n",
    "\n",
    "recommendations_df = recommendations_df[['report_id', 'Number', 'Recommendation', 'Reply Text' ,'Made', 'Recipient', 'response_category_example']]\n",
    "\n",
    "# change all columsn to lower case and repalce spaces with _\n",
    "recommendations_df.columns = recommendations_df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_response_category(response, recommendation_num):\n",
    "    categories = '\\n'.join([f\"{element['category']} - {element['definition']}\" for element in response_categories])\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are helping me put responses into categories.\n",
    "\n",
    "These responses are to recommendations that were made in a transport accident investigation report. These recommendations are issued directly to a particular party.\n",
    "\n",
    "There are three categories:\n",
    "\n",
    "{categories}\n",
    "\n",
    "However if there are responses that don't fit into any of the categories then you can put them as N/A. These may be responses that request further information or want recommendation to be sent elsewhere.\n",
    "\n",
    "Your response should just be the name of the category with nothing else.\n",
    "\"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "Which category is this response in?\n",
    "\n",
    "\"\n",
    "{response}\n",
    "\"\n",
    "\n",
    "in regards to recommendation number {recommendation_num}\n",
    "\"\"\"\n",
    "\n",
    "    # print(f\"system propmt is:\\n{system_prompt} and user prompt is:\\n{user_prompt}\")\n",
    "\n",
    "    openai_response = openAICaller.query(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        model = \"gpt-4\",\n",
    "        temp = 0\n",
    "    )\n",
    "\n",
    "    if openai_response in [category['category'] for category in response_categories] + ['N/A']:\n",
    "        return openai_response\n",
    "    else:\n",
    "        print(f\"Did not match any of the categories - {openai_response}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_recommendations_df = recommendations_df.sample(25, random_state=42)\n",
    "\n",
    "sample_recommendations_df['response_category'] = sample_recommendations_df.apply(lambda x: assign_response_category(x['reply_text'], x['number']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples_recommendations_df = recommendations_df[~recommendations_df['response_category_example'].isna()]\n",
    "examples_recommendations_df['response_category'] = examples_recommendations_df.apply(lambda x: assign_response_category(x['reply_text'], x['number']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results\n",
    "\n",
    "comparison = examples_recommendations_df['response_category'] == examples_recommendations_df['response_category_example']\n",
    "\n",
    "sum(comparison)/len(comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the good results I will do a complete run through for their sake\n",
    "\n",
    "recommendations_df['response_category'] = recommendations_df.apply(lambda x: (x['reply_text'], x['number']), axis=1)\n",
    "\n",
    "recommendations_df[[\"number\", \"report_id\", \"recommendation\", \"reply_text\", \"response_category\"]].to_excel('recommendation_categories.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df.groupby('response_category').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised classification\n",
    "\n",
    "I am going to try out some unsupervised classifications.\n",
    "\n",
    "I also want to try clustering of the responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lbl2Vec\n",
    "\n",
    "After working with this for a bit it didn't seem to be working. Due to this error https://github.com/sebischair/Lbl2Vec/issues/4. Until this is resolved I will have to move on to other techniques.\n",
    "\n",
    "This technique will involve me defining some categories based off keywords. It will then learn from unlabeled texts. This unlabeled text is exactly what I have here.\n",
    "\n",
    "Two concerns, I have a very small data set and I have quite short text in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame([\n",
    "    {\n",
    "        'class_name': \"Accepted\",\n",
    "        'keywords': ['accepted', 'implemented', 'accept', 'implement']\n",
    "    },\n",
    "    {\n",
    "        'class_name': \"Under consideration\",\n",
    "        'keywords': ['under', 'consideration', 'consider']\n",
    "    },\n",
    "    {\n",
    "        'class_name': 'Rejected',\n",
    "        'keywords': ['rejected']\n",
    "    }\n",
    "])\n",
    "\n",
    "labels['numbers_of_keywords'] = labels['keywords'].apply(lambda x: len(x))\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "importlib.reload(gensim)\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# doc: document text string\n",
    "# returns tokenized document\n",
    "# strip_tags removes meta tags from the text\n",
    "# simple preprocess converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long \n",
    "# simple preprocess also removes numerical values as well as punktuation characters\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "# Split into train and test\n",
    "test_df = cleaned_taic_recommendations.sample(10, random_state=42)\n",
    "train_df = cleaned_taic_recommendations[~cleaned_taic_recommendations['Number'].isin(sample_df['Number'])]\n",
    "\n",
    "test_df['data_set_type'] = \"test\"\n",
    "train_df['data_set_type'] = \"train\"\n",
    "\n",
    "# concat train and test data\n",
    "full_corpus = pd.concat([train_df,test_df]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# tokenize and tag documents for Lbl2Vec training\n",
    "full_corpus['tagged_responses'] = full_corpus.apply(lambda row: TaggedDocument(tokenize(row['Reply Text']), [str(row.name)]), axis=1)\n",
    "\n",
    "# add doc_key column\n",
    "full_corpus['doc_key'] = full_corpus.index.astype(str)\n",
    "\n",
    "display(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbl2vec import Lbl2Vec\n",
    "\n",
    "# init model with parameters\n",
    "Lbl2Vec_model = Lbl2Vec(keywords_list=list(labels.keywords), tagged_documents=full_corpus['tagged_responses'][full_corpus['data_set_type'] == 'train'], label_names=list(labels.class_name), similarity_threshold=0.43, min_num_docs=5, epochs=10)\n",
    "\n",
    "# train model\n",
    "Lbl2Vec_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "I will just try to cluster them using normal techniques.\n",
    "\n",
    "I tried using clustering which was a bit of fun but not necessary as I have gotten good results with LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your responses are stored in a list called 'responses'\n",
    "# Clean the responses by removing punctuation and converting to lowercase\n",
    "clustered_recommendations = taic_recommendations.copy()\n",
    "\n",
    "clustered_recommendations = clustered_recommendations[clustered_recommendations['Made'] >= '2010-01-01']\n",
    "\n",
    "\n",
    "cleaned_responses = [response.lower().replace('.', '').replace(',', '') for response in clustered_recommendations['Reply Text']]\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_responses = [word_tokenize(response) for response in cleaned_responses]\n",
    "filtered_responses = [[word for word in tokenized_response if word not in stop_words] for tokenized_response in tokenized_responses]\n",
    "\n",
    "\n",
    "preprocessed_responses = [' '.join(filtered_response) for filtered_response in filtered_responses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the transform\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_clusters = 3\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "print(\"Average silhouette_score:\", silhouette_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clustered_recommendations['cluster'] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_matrix_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Visualize clusters\n",
    "plt.scatter(tfidf_matrix_pca[:, 0], tfidf_matrix_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('Clustering of Responses')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with the examples from Ingrid\n",
    "\n",
    "ingrid_categories['cluster'] = ingrid_categories['Reply Text'].apply(lambda x: kmeans.predict(tfidf_vectorizer.transform([x]).toarray()))\n",
    "\n",
    "ingrid_categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
