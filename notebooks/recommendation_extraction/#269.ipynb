{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What \n",
    "\n",
    "This is related to #269.\n",
    "\n",
    "To do recommendation exrtaction I need to complete a few tasks:\n",
    "\n",
    "- [x] Web scraping of TSB for its recommendations\n",
    "- [x] Safety action section extraction for ATSB and TSB (depending on if TSB recommendation dataset is in fact complete)\n",
    "- [x] Safety action section parsing to gedt the recommendations\n",
    "- [ ] Integration into the rest of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import engine.gather.WebsiteScraping as WebsiteScraping\n",
    "import engine.extract.ReportExtracting as ReportExtracting\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "importlib.reload(WebsiteScraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping of TSB\n",
    "\n",
    "Firstly I need to scrape the recommendations of the TSB website.\n",
    "\n",
    "This will be done in the `WebsiteScraping` module.\n",
    "\n",
    "I am not sure if this recommendations dataset is complete or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(WebsiteScraping)\n",
    "\n",
    "recommendation_scraper = WebsiteScraping.TSBRecommendationsScraper('tsb_recommendations', refresh=False)\n",
    "\n",
    "recommendation_scraper.extract_recommendations_from_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_recommendations = pd.read_pickle('tsb_recommendations')\n",
    "tsb_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the current recommendations looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taic_recommendations =  pd.read_pickle('../../output/taic_recommendations.pkl')\n",
    "\n",
    "example_taic_recommendation = taic_recommendations.loc[213, 'recommendations']\n",
    "example_taic_recommendation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through the recommendations. I think that they are a compelte amount. Even if there are some larger gaps.\n",
    "\n",
    "I have spot checked the rail and the gaps there seem to check out as the none of the reports in that few years had any recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety Action section extraction\n",
    "\n",
    "As found above we have a complete recommendation dataset for TSB. So we only need to scrape the ATSB reports for recommendations.recommendation_scraper\n",
    "\n",
    "This can be done by reading the safety actions section I believe.\n",
    "\n",
    "Itm ight even be possible to do it with just regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles = pd.read_pickle('../../output/report_titles.pkl')\n",
    "parsed_reports = pd.read_pickle('../../output/parsed_reports.pkl')\n",
    "all_reports = report_titles.merge(parsed_reports, how='right', on='report_id')\n",
    "atsb_reports = all_reports[all_reports['report_id'].str.startswith('ATSB') & (all_reports['investigation_type'] != 'short')]\n",
    "atsb_reports.set_index('report_id', inplace=True)\n",
    "atsb_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_atsb = atsb_reports.sample(frac=0.1, random_state=42)\n",
    "\n",
    "\n",
    "sample_dir = 'sample_atsb'\n",
    "\n",
    "if os.path.exists(sample_dir):\n",
    "    shutil.rmtree(sample_dir)\n",
    "\n",
    "os.mkdir(sample_dir)\n",
    "for report_id in sample_atsb['report_id']:\n",
    "    if not os.path.exists(f'../../output/report_pdfs/{report_id}.pdf'):\n",
    "        continue\n",
    "    shutil.copy(f'../../output/report_pdfs/{report_id}.pdf', f'{sample_dir}/{report_id}.pdf')\n",
    "\n",
    "sample_atsb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through the entire sample it seems that only abotu 2 reports ahve recommendations. I dont know if this is just because the didnt have recommendation or they are listed elsewhere.else\n",
    "\n",
    "I am going to have a look into TAIC and ATSBs recommendations datasets to see how many there are per report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tsb_recommendations\n",
    "report_groups = [v.reset_index(drop=True) for k, v in df.groupby(\"report_id\")]\n",
    "\n",
    "widened_df = pd.DataFrame(\n",
    "    {\n",
    "        \"report_id\": df.groupby(\"report_id\").groups.keys(),\n",
    "        \"recommendations\": report_groups,\n",
    "    }\n",
    ")\n",
    "\n",
    "all_recommendations = pd.concat([widened_df, taic_recommendations], axis=0)\n",
    "\n",
    "all_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles = pd.read_pickle('../../output/report_titles.pkl')\n",
    "report_titles['agency'] = report_titles['report_id'].str[:4]\n",
    "report_titles.value_counts('agency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the TSB only has about 100 reports which have recommendations. However TAIC has about 300 reports in that same time period. This means that TAIC is releasing alot more recomendations. This is equally curious as TAIC has only released about 545 reports whereas TSB has done 1500 in that same time period. Meaning that 1/15 TSB reprots has recommendations but taic is about 1/2. About 8 times more. I know that alot of tSB reports are short rpeorts so that will account for some but not all of this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_atsb['content_section'] = sample_atsb.apply(\n",
    "    lambda x: ReportExtracting.ReportExtractor(x['text'], x['report_id'], x['headers']).extract_contents_section(), axis=1    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "sample_atsb['recommendation_sections'] = sample_atsb.progress_apply(\n",
    "    lambda x: ReportExtracting.RecommendationsExtractor(x['text'], x['report_id'], x['headers'])._extract_recommendation_section_text(), axis=1\n",
    ")\n",
    "sample_atsb.to_pickle(\"sample_atsb_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost ot read all of safety actions sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.encoding_for_model('gpt-4o')\n",
    "\n",
    "sample_atsb['recommendation_section_tokens'] = sample_atsb['recommendation_sections'].progress_apply(\n",
    "    lambda x: len(encoder.encode(x)) if x else 0\n",
    ")\n",
    "\n",
    "sample_atsb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(atsb_reports) * sample_atsb['recommendation_section_tokens'].mean() / 1_000) * 0.00250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the safety action section\n",
    "Now that the safety action sections have been extracted I can read them with the use of an LLM. It will be a simple query just to extract the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "sample_atsb['recommendations'] = sample_atsb.progress_apply(\n",
    "    lambda x:  ReportExtracting.RecommendationsExtractor(x['text'], x['report_id'], x['headers'])._extract_recommendations_from_text(x['recommendation_sections']) if x['recommendation_sections'] else None, axis=1\n",
    ")\n",
    "sample_atsb.to_pickle(\"sample_atsb_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_atsb = pd.read_pickle(\"sample_atsb_df\")\n",
    "sample_atsb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(\"recommendation_test_data.pkl\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sample_atsb.set_index('report_id').loc[\n",
    "    [\"ATSB_a_2002_780\", \"ATSB_m_2005_215\", \"ATSB_a_2021_005\", \"ATSB_m_2008_012\", \"ATSB_r_2015_007\", # For reading recommendation_sections\n",
    "     \"ATSB_m_2001_163\", \"ATSB_a_2002_710\", \"ATSB_r_2014_024\", \"ATSB_m_2006_234\", \"ATSB_r_2004_004\", \"ATSB_a_2003_980\", \"ATSB_a_2017_105\"] # For extracting pages to read\n",
    "    ][[\"text\", \"headers\", \"content_section\", \"recommendation_sections\", \"recommendations\"]]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc['ATSB_m_2008_012', 'recommendations'] = None\n",
    "test_data.loc['ATSB_a_2002_780', 'recommendations'] = None\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.rename(columns={\"recommendation_sections\": \"recommendation_section\"}).to_pickle(\"recommendation_test_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete run through for ATSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "atsb_reports['recommendations'] = atsb_reports.progress_apply(\n",
    "    lambda x:  ReportExtracting.RecommendationsExtractor(x['text'], x.name, x['headers']).extract_recommendations(), axis=1\n",
    ")\n",
    "atsb_reports.to_pickle(\"atsb_recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atsb_recommendations = pd.read_pickle(\"atsb_recommendations\")\n",
    "\n",
    "atsb_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "atsb_recommendations['recommendations'].dropna().apply(lambda x: len(x) if isinstance(x, list) else 0).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a look at this I need to compare it to the TAIC and the TSB reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb_recommendations = pd.read_pickle('../../output/tsb_website_recommendations.pkl')\n",
    "tsb_recommendations = pd.DataFrame({\n",
    "    'report_id': tsb_recommendations.groupby('report_id').groups.keys(),\n",
    "    'recommendations': [group.reset_index(drop=True) for _, group in tsb_recommendations.groupby('report_id')]\n",
    "}\n",
    ")\n",
    "display(tsb_recommendations['recommendations'].map(len).describe())\n",
    "taic_recommendations = pd.read_pickle('../../output/taic_website_recommendations.pkl')\n",
    "taic_recommendations['year'] = taic_recommendations['report_id'].map(lambda x: int(x.split('_')[2]))\n",
    "taic_recommendations = taic_recommendations[taic_recommendations['year'] >= 2000]\n",
    "display(taic_recommendations['recommendations'].map(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note here is that TAIC has a much higher amount of recommendations. It averages about the same amount per report that ARE making recommendations.\n",
    "\n",
    "However it is much more likely for a report to make a recommendation than for TSB or ATSB to do so. This is partly because all of TAICs reports are full reports where alot of ATSB and TSB are actually short reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles = pd.read_pickle('../../output/report_titles.pkl')\n",
    "agency_report_counts = report_titles.query(\"investigation_type != 'short'\")['report_id'].map(lambda x: x.split('_')[0]).value_counts()\n",
    "print(f\"TAIC has {agency_report_counts['TAIC']} TAIC reports and {len(taic_recommendations)} reports with recommendations which means it has a make recommendations percentage of {len(taic_recommendations)/agency_report_counts['TAIC']*100:.2f}%\")\n",
    "print(f\"TSB has {agency_report_counts['TSB']} TSB reports and {len(tsb_recommendations)} reports with recommendations which means it has a make recommendations percentage of {len(tsb_recommendations)/agency_report_counts['TSB']*100:.2f}%\")\n",
    "print(f\"ATSB has {agency_report_counts['ATSB']} ATSB reports  and {len(atsb_recommendations['recommendations'].dropna())} reports with recommendations which means it has a make recommendations percentage of {len(atsb_recommendations['recommendations'].dropna())/agency_report_counts['ATSB']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_per_year = taic_recommendations.groupby('year')['num'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "old_report_titles = pd.read_pickle('../../output/old_report_titles.pkl')\n",
    "old_report_titles['agency'] = old_report_titles['report_id'].map(lambda x: x.split('_')[0])\n",
    "taic_reports = old_report_titles[old_report_titles['agency'] == 'TAIC']\n",
    "taic_reports['year'] = taic_reports['report_id'].map(lambda x: int(x.split('_')[2]))\n",
    "\n",
    "reports_per_year = taic_reports.groupby('year')['report_id'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the recommendations per report by combing teh taic_reports and taic_recommendaitons\n",
    "\n",
    "recs_per_report = {\n",
    "    year: recs_per_year[year] / reports_per_year[year] for year in range(2000,2024)\n",
    "}\n",
    "df = pd.DataFrame(zip(recs_per_report.keys(), recs_per_report.values()), columns=['year', 'recommendations_per_report'])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_bar, theme_minimal, labs\n",
    "\n",
    "chart = (ggplot(, aes(x='year', y='recommendation_count')) +\n",
    "         geom_bar(stat='identity') +\n",
    "         theme_minimal() +\n",
    "         labs(title='Recommendations per Report by Year', x='Year', y='Recommendations per Report'))\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intergrating it with the report extracting processor\n",
    "\n",
    "The reommendation extraction process needs to be updated to be included into the pipeline.\n",
    "\n",
    "It needs to still output a recommendation file that is wide with one row per report that has recommendations.recommendation_scraper\n",
    "\n",
    "TSB still hasnt got oteh code to convert the agnecy_id to report_id. This is because the new report_titles scrape hasnt been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles = pd.read_pickle('../../output/report_titles.pkl')\n",
    "report_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reports = pd.read_pickle('../../output/parsed_reports.pkl')\n",
    "\n",
    "importlib.reload(ReportExtracting)\n",
    "processor = ReportExtracting.ReportExtractingProcessor('../../output/parsed_reports.pkl', refresh=True)\n",
    "\n",
    "processor.extract_recommendations(\"recommendations.pkl\", \"temp.pkl\", \"../../output/taic_website_recommendations.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
