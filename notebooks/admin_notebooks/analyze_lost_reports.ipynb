{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "As per [#286](https://github.com/1jamesthompson1/TAIC-report-summary/issues/286) there is a problem where I dont currently know where the reports are going.\n",
    "\n",
    "I want to start by looking at the various steps and figuring out which reports are lost at which step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def output_file(path):\n",
    "    return os.path.join('../../output', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting datasets\n",
    "\n",
    "The data flows through the engine as pandas dataframes.\n",
    "\n",
    "In theory by just looking at `report_titles.pkl`, `extracted_reports.pkl` we will be able to know what reports were dropped off in the gather and extract phases. Then the last step is the analyze which is mostly embedding as the other two datasets are used as much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles = pd.read_pickle(output_file('report_titles.pkl'))\n",
    "extracted_reports = pd.read_pickle(output_file('extracted_reports.pkl'))\n",
    "report_pdfs = pd.DataFrame(map(lambda x: x[:-4], os.listdir(output_file('report_pdfs'))), columns = ['report_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_pdfs['pdf_download'] = True\n",
    "report_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = report_titles.merge(report_pdfs, on='report_id', how='left').merge(extracted_reports[[\"report_id\", \"text\", \"toc\", \"recommendations\", \"safety_issues\", \"sections\"]], on='report_id', how='left')\n",
    "all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info[\"mode\"] = all_info[\"report_id\"].apply(lambda x: x.split(\"_\")[1])\n",
    "all_info[\"year\"] = all_info[\"report_id\"].apply(lambda x: x.split(\"_\")[2])\n",
    "all_info[\"agency\"] = all_info[\"report_id\"].apply(lambda x: x.split(\"_\")[0])\n",
    "all_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating outcome dataset\n",
    "\n",
    "The report titles are the list of all reports that were web scraped. It should line up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to instead do it widers o that each column is for its own stage\n",
    "\n",
    "outcome = all_info.copy()\n",
    "\n",
    "outcome[\"found_on_website\"] = True\n",
    "\n",
    "outcome['pdf_download'] = outcome['pdf_download'].fillna(False)\n",
    "\n",
    "outcome[\"text_extracted\"] = outcome['text'].map(lambda x: True if isinstance(x, str) else False)\n",
    "\n",
    "outcome[\"toc_extracted\"] = outcome['toc'].map(lambda x: True if isinstance(x, str) else False)\n",
    "\n",
    "outcome[\"safety_issues_extracted\"] = outcome['safety_issues'].map(lambda x: True if isinstance(x, pd.DataFrame) and len(x) > 0 else False)\n",
    "\n",
    "outcome[\"recommendations_extracted\"] = outcome['recommendations'].map(lambda x: True if isinstance(x, pd.DataFrame) and len(x) > 0 else False)\n",
    "\n",
    "outcome[\"safety_issues and/or recommendations extracted\"] = outcome[\"safety_issues_extracted\"] | outcome[\"recommendations_extracted\"]\n",
    "\n",
    "\n",
    "counts = outcome[[\"found_on_website\", \"pdf_download\", \"text_extracted\", \"safety_issues and/or recommendations extracted\"]].apply(sum, axis = 0 )\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where did the reports go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "nodes = [\"found_on_website\", \"pdf_download\", \"text_extracted\", \"safety_issues_and/or recommendations extracted\", \"nothing_extracted\",  \"could_not_get_pdf\", \"no_text_extraction\"]\n",
    "\n",
    "links = [\n",
    "    {\"source\": 0, \"target\": 1, \"value\": counts[\"pdf_download\"]},\n",
    "    {\"source\": 0, \"target\": 5, \"value\": counts[\"found_on_website\"] - counts[\"pdf_download\"]},\n",
    "    {\"source\": 1, \"target\": 2, \"value\": counts[\"text_extracted\"]},\n",
    "    {\"source\": 1, \"target\": 6, \"value\": counts[\"pdf_download\"] - counts[\"text_extracted\"]},\n",
    "    {\"source\": 2, \"target\": 3, \"value\": counts[\"safety_issues and/or recommendations extracted\"]},\n",
    "    {\"source\": 2, \"target\": 4, \"value\": counts[\"text_extracted\"] - counts[\"safety_issues and/or recommendations extracted\"]},\n",
    "]\n",
    "    \n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),\n",
    "        label = nodes,\n",
    "        color = \"blue\",\n",
    "        align=\"left\"\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = [link[\"source\"] for link in links],\n",
    "        target = [link[\"target\"] for link in links],\n",
    "        value = [link[\"value\"] for link in links]\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Report Extraction Pipeline\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of different outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.histogram(outcome, x=\"year\", color=\"agency\", facet_col=\"mode\", \n",
    "                   barmode='group', title=\"Count of Records by Year, Agency, and Mode\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could not get PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "could_not_get_pdf = outcome[outcome['pdf_download'] == False]\n",
    "\n",
    "fig = px.histogram(could_not_get_pdf, x=\"year\", color=\"agency\", facet_col=\"mode\", \n",
    "                   barmode='group', title=\"Count of reports with missing PDFs by Year, Agency, and Mode\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nothing extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nothing_extracted = outcome[(outcome['text_extracted'] == True) & (outcome[\"safety_issues and/or recommendations extracted\"] == False)]\n",
    "\n",
    "\n",
    "fig = px.histogram(nothing_extracted, x=\"year\", color=\"agency\", facet_col=\"mode\", \n",
    "                   barmode='group', title=\"Count of Records by Year, Agency, and Mode\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
