{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting safety issues from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem introduction\n",
    "Safety issues are being extracted from the reports.\n",
    "\n",
    "There are two types of safety issues being extracted.\n",
    "Firstly are exact extraction. These are safety issues that are explicitly mentioned in the report. They will follow something like \"safety issue: ...\" or \"safety issue - ...\". Exact safety issue extraction is not too much of a worry and just needs to be tested and could be done so with unit tests.\n",
    "\n",
    "Secondly however are inferred safety issues. These safety issues are supposedly present but are not explicitly stated. This requires a little bit more validation then a simple unit test. Fortunately I can get a good test set.\n",
    "\n",
    "All the reports that have the safety issues explicitly mentioned will show me what the correct answer is. So all I need to do is remove the explicitly mention of the safety issue from the report then I can run it as an inferred and see if it comes up with the same safety issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution story\n",
    "\n",
    "I have peformed some testing on fine-tuning gpt 3.5 turbo.\n",
    "\n",
    "It was about 56% accurate on the training set (about 15 reports)\n",
    "However on a testing set it was only 17% accurate. This was becuase about 70% of the items had mismatched number of safety issues extracted.\n",
    "\n",
    "This means that I wanted to try with more data. There are about 70 reports that I have meaning I can split it between a training, validation and test set. This is what the code does below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'engine.Extract_Analyze.ReportExtracting' from '/home/james/code/TAIC-report-summary/engine/Extract_Analyze/ReportExtracting.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "import importlib\n",
    "from engine.Extract_Analyze import ReportExtracting\n",
    "importlib.reload(ReportExtracting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  read_or_create_important_text_file(path, report_id, text):\n",
    "    important_text_path = os.path.join(path, report_id, f'{report_id}_important_text.txt')\n",
    "\n",
    "    if not os.path.isfile(important_text_path):\n",
    "        important_text, pages = ReportExtracting.ReportExtractor(text, report_id).extract_important_text()\n",
    "\n",
    "        important_text = \"\" if important_text == None else important_text\n",
    "\n",
    "        with open(important_text_path, 'w') as stream:\n",
    "            stream.write(important_text)\n",
    "\n",
    "        return important_text\n",
    "    \n",
    "    with open(important_text_path, 'r') as stream:\n",
    "        important_text = stream.read()\n",
    "        \n",
    "    important_text = None if important_text == \"\" else important_text\n",
    "\n",
    "    return important_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text_and_get_safety_issues(path, reports_ids):\n",
    "    \"\"\"\n",
    "    This will go through a folder and extract all of the safety issues that it can from the reports using regex. Then it will also remove those extracted safety issues from the report text and report a DataFrame that has the report_id, the safety issues extracted, and the cleaned report text.\n",
    "    \"\"\"\n",
    "    safety_issues_raw = []\n",
    "\n",
    "    for report in reports_ids:\n",
    "        with open(os.path.join(path, report, f'{report}.txt'), 'r') as stream:\n",
    "            text = stream.read()\n",
    "\n",
    "        # Check to see if the important text exists\n",
    "\n",
    "        important_text = read_or_create_important_text_file(path, report, text)\n",
    "\n",
    "        if important_text == None:\n",
    "            print(f'Could not extract important text from {report}')\n",
    "            continue\n",
    "        \n",
    "        safety_regex = lambda x: fr's ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s? ?{x} ?'\n",
    "        end_regex = r'([\\s\\S]+?)(?=(?:\\d+\\.(?:\\d+\\.)?(?:\\d+)?)|(?:^ [A-Z])|(?:s ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s?))'\n",
    "\n",
    "        uncompiled_regexes = [\"(\" + safety_regex(sep) + end_regex + \")\" for sep in [\"-\", \":\"]]\n",
    "\n",
    "        safety_issue_regexes = [re.compile(regex , re.MULTILINE | re.IGNORECASE) for regex in uncompiled_regexes]\n",
    "\n",
    "        safety_issues_for_report = []\n",
    "\n",
    "        cleaned_text = text\n",
    "\n",
    "        matches = [regex.findall(important_text) for regex in safety_issue_regexes]\n",
    "\n",
    "        # Choose one of the matches that has the most matches\n",
    "        matches = max(matches, key=lambda x: len(x))\n",
    "\n",
    "        for full_match, safety_issue_match in matches:\n",
    "            safety_issues_for_report.append(safety_issue_match)\n",
    "            cleaned_text = re.sub(\n",
    "                full_match, '',\n",
    "                cleaned_text,\n",
    "                flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "        safety_issues_raw.append({\n",
    "            \"file\": report,\n",
    "            \"safety_issues\": safety_issues_for_report,\n",
    "            'cleaned_report': cleaned_text})\n",
    "\n",
    "    safety_issues_df = pd.DataFrame(safety_issues_raw)\n",
    "\n",
    "    safety_issues_df['number_extracted'] = safety_issues_df['safety_issues'].apply(lambda x: len(x))\n",
    "\n",
    "    return safety_issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract important text from 2010_207\n",
      "Could not extract important text from 2010_205\n",
      "Could not extract important text from 2016_002\n",
      "Could not extract important text from 2019_003\n"
     ]
    }
   ],
   "source": [
    "reports_path = \"output\"\n",
    "\n",
    "reports = os.listdir(reports_path)\n",
    "\n",
    "reports_to_ignore = [\n",
    "    \"2020_102\", # This report has a funny safety issue that is not formatted correctly so cant be picked up by the regex\n",
    "]\n",
    "reports = [report for report in reports if report not in reports_to_ignore]\n",
    "\n",
    "all_reports_df = clean_text_and_get_safety_issues(reports_path, reports)\n",
    "\n",
    "# Remove all reports that have no found safety issues\n",
    "all_reports_df = all_reports_df[all_reports_df['number_extracted'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get split of 60% train, 20% validation, 20% test\n",
    "seed = 42\n",
    "\n",
    "train_df = all_reports_df.sample(frac=0.6, random_state=seed)\n",
    "\n",
    "validation_df = all_reports_df.drop(train_df.index).sample(frac=0.5, random_state=seed)\n",
    "\n",
    "test_df = all_reports_df.drop(train_df.index).drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data is in three distinct categories it would be prudent to make sure that the resultant data has similar distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>mode</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>validation</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>validation</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>validation</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        split mode  count\n",
       "0        test    0      4\n",
       "1        test    1      5\n",
       "2        test    2      6\n",
       "3       train    0     14\n",
       "4       train    1     17\n",
       "5       train    2     15\n",
       "6  validation    0      5\n",
       "7  validation    1      7\n",
       "8  validation    2      3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the three dataframes into one and add a column to indicate the split\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "validation_df['split'] = 'validation'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "all_datasets_df = pd.concat([train_df, validation_df, test_df])\n",
    "\n",
    "all_datasets_df['mode'] = all_datasets_df['file'].apply(lambda x: x.split('_')[1][0])\n",
    "\n",
    "# Group by split and get distribution of mode in each split\n",
    "all_datasets_df.groupby(['split', 'mode']).count().reset_index()[['split', 'mode', 'file']].rename(columns={'file': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not perfect but we can see that it is pretty darn close. The validation set is low on reports. However this is not something that can be completely fixed and aught not to affect the final outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fine tuned model\n",
    "\n",
    "Because of the dismal success of just asking it I am going to give it a couple of examples and then see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions for formatting the data\n",
    "def format_safety_issue_to_yaml(safety_issue):\n",
    "    safety_issues_dict = []\n",
    "    for issue in safety_issue:\n",
    "        safety_issues_dict.append({\"safety_issue\": issue, \"quality\": \"inferred\"})\n",
    "\n",
    "    return yaml.dump(safety_issues_dict, sort_keys=False)\n",
    "\n",
    "def format_data(data):\n",
    "  formatted_data = []\n",
    "  for index, row in data.iterrows():\n",
    "    formatted_data.append({\"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "You are going help me read a transport accident investigation report.\n",
    "\n",
    " I want you to please read the report and respond with the safety issues identified in the report.\n",
    "\n",
    "Please only respond with safety issues that are quite clearly stated and/or implied.\n",
    "\n",
    "It should be noted that the number of safety issues in a report has a minimum of 1 an 0.25 quantile of 1, median of 2, 0.75 quantile of 3 and a maximum of 13. You should try make your answers match this distribution and on average a report will have only 2 safety issues.\n",
    "\n",
    "Remember the definitions give\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "• can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "• is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "{ReportExtracting.ReportExtractor(row['cleaned_report'], row['file']).extract_important_text()[0]}\n",
    "        \n",
    "=Instructions=\n",
    "\n",
    "I want to know the safety issues which this investigation has found.\n",
    "\n",
    "I also ned to know what is the quality of this safety issue. Some reports will have safety issues explicitly stated with something like \"safety issue - ...\" or \"safety issue: ...\", these are \"exact\" safety issues. Other reports however wont have these yet safety issues may still be present in this case they are \"inferred\" safety issues.\n",
    "\n",
    "Can your response please be in yaml format as shown below.\n",
    "\n",
    "- safety_issue: \"bla bla talking about this and that bla bla bla\"\n",
    "  quality: exact\n",
    "- safety_issue: \"bla bla talking about this and that bla bla bla\"\n",
    "  quality: exact\n",
    "\n",
    "\n",
    "There is no need to enclose the yaml in any tags.\n",
    "\n",
    "=Here are some definitions=\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "• can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "• is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"{format_safety_issue_to_yaml(row['safety_issues'])}\"\"\"\n",
    "        }\n",
    "    ]})\n",
    "\n",
    "  return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15]\n",
      "  Could not find text between pages 15 and 16\n",
      "  Could not extract text from page 15\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  Could not find text between pages 10 and 11\n",
      "  Could not extract text from page 10\n",
      "  Could not find text between pages 11 and 12\n",
      "  Could not extract text from page 11\n",
      "  Could not find text between pages 13 and 14\n",
      "  Could not extract text from page 13\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
      "  I am going to be reading these pages: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "  I am going to be reading these pages: [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "  I am going to be reading these pages: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
      "  I am going to be reading these pages: [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "  Could not find text between pages 23 and 24\n",
      "  Could not extract text from page 23\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "  Could not find text between pages 25 and 26\n",
      "  Could not extract text from page 25\n",
      "  Could not find text between pages 29 and 30\n",
      "  Could not extract text from page 29\n",
      "  Could not find text between pages 31 and 32\n",
      "  Could not extract text from page 31\n",
      "  Could not find text between pages 35 and 36\n",
      "  Could not extract text from page 35\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  Could not find text between pages 13 and 14\n",
      "  Could not extract text from page 13\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "# Format and save dataset\n",
    "\n",
    "import json\n",
    "\n",
    "training_data = format_data(train_df)\n",
    "\n",
    "validation_data = format_data(validation_df)\n",
    "\n",
    "\n",
    "\n",
    "with open('training_data.jsonl', 'w') as outfile:\n",
    "    for entry in training_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "with open('validation_data.jsonl', 'w') as outfile:\n",
    "    for entry in validation_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to OpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai_file = client.files.create(\n",
    "  file=open(\"training_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "openai_file_val = client.files.create(\n",
    "  file=open(\"validation_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-RXt3Jit0q6UnJda8QRaV3LrZ', created_at=1712098644, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-imErdBW1EpSlnDOl8RluwTmE', result_files=[], status='validating_files', trained_tokens=None, training_file='file-BifhlnCFv2RiWnT4pm6AVTL3', validation_file='file-7LJJlx8HUoHGmBpLcucpK16I', user_provided_suffix=None, seed=141041653)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the fine-tune job\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=openai_file.id, \n",
    "  validation_file=openai_file_val.id,\n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this I am working on fine tuning a model so that I can accurately extract the safety issues from the report.\n",
    "\n",
    "I am getting to the point of thinking that it might not be possible to extract them 100% accurately.\n",
    "\n",
    "Instead maybe I should just focus on making a good distinction between exact and inferred.\n",
    "\n",
    "False news!\n",
    "\n",
    "False news!\n",
    "\n",
    "I had made a mistake and moved over the wrong query request to the new ft model.\n",
    "\n",
    "After checking it on the training data it is about 50% accurate which is pretty good.\n",
    "I will now check it on some new data that the training hasn't seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fine-tuned model\n",
    "\n",
    "I will now test the fine-tuned model and see how it fares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and comparing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine.OpenAICaller as OpenAICaller\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "def compare_safety_issues(safety_issues, inferred_safety_issues):\n",
    "    \"\"\"\n",
    "    This will receive two lists of safety issues and then ask the LLM to compare each safety issue and see if they are the same or different.\n",
    "    \"\"\"\n",
    "\n",
    "    pairs = [(i, j) for i, _ in enumerate(safety_issues) for j, _ in enumerate(inferred_safety_issues)]\n",
    "\n",
    "    pairs_comparison_results = []\n",
    "\n",
    "    for extracted, inferred in pairs:\n",
    "        response = openAICaller.query(\n",
    "            \"\"\"\n",
    "You are going to help me compare if safety issues are the same.\n",
    "\n",
    "You will be given two safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that they are the same safety issue and maybe just worded differently. Whereas different will mean that they are fundamentally different safety issues.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "    Here is the first safety issues:\n",
    "    {safety_issues[extracted]}\n",
    "\n",
    "    Here is the second safety issues:\n",
    "    {inferred_safety_issues[inferred]}\n",
    "    \"\"\",\n",
    "            temp =  0,\n",
    "            model = \"gpt-4\"\n",
    "        ).lower()\n",
    "\n",
    "        if response != 'yes' and response != 'no':\n",
    "            print(f\"Error response in the wrong format {response}\")\n",
    "            response = 'undetermined'\n",
    "    \n",
    "        pairs_comparison_results.append({\n",
    "            'pair': (extracted, inferred),\n",
    "            'result': response\n",
    "        })\n",
    "    return pairs_comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt used to compare two lists of safety themes.\n",
    "```\n",
    "You are going to help me compare if these listed safety issues are the same.\n",
    "\n",
    "You will be given two lists of safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that for each safety issue in the set it will match a safety issue found in the other set albeit if worded slightly differently.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "Here is the first set of safety issues:\n",
    "{safety_issues}\n",
    "\n",
    "Here is the second set of safety issues:\n",
    "{inferred_safety_issues}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_safety_issues_df(df):\n",
    "    # Compare safety issues with the LLM\n",
    "    df['same_comparison_results'] = df.apply(\n",
    "        lambda x: compare_safety_issues(x['safety_issues'],\n",
    "                                        x['inferred_safety_issues']),\n",
    "        axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_safety_issues_from_cleaned_reports(df):\n",
    "    \"\"\"\n",
    "    This will call the inference function and extract the safety issues from the cleaned reports.\n",
    "    \"\"\"\n",
    "    df['inferred_safety_issues_struct'] = df.apply(lambda x: ReportExtracting.SafetyIssueExtractor(x['cleaned_report'], x['file'])._extract_safety_issues_with_inference(), axis=1)\n",
    "\n",
    "    # Extract the safety issues from the data structured returned from the inferences extraction function.\n",
    "    df['inferred_safety_issues'] = df['inferred_safety_issues_struct'].apply(lambda x: [issue['safety_issue'] for issue in x] if (x is not None and isinstance(x[0], dict)) else None if x is None else x)\n",
    "\n",
    "    df['number_inferred'] = df['inferred_safety_issues'].apply(lambda x: len(x) if x is not None else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import textwrap\n",
    "\n",
    "def make_match_visualization(df, safety_issue_text_dict, report_id):\n",
    "    '''\n",
    "    Create a picture that shows both the extracted and inferred safety issues and the matches between them as lines/arrows.\n",
    "    '''\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes for the 'extracted' and 'inferred' indices\n",
    "    extracted_indices = df['extracted'].unique()\n",
    "    inferred_indices = df['inferred'].unique()\n",
    "\n",
    "    NODE_WIDTH = 50\n",
    "\n",
    "    for i, index in enumerate(extracted_indices):\n",
    "        node_label = '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"extracted\"][index], width=NODE_WIDTH))\n",
    "        G.add_node(node_label, pos=(0, i))\n",
    "\n",
    "    for i, index in enumerate(inferred_indices):\n",
    "        node_label = '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"inferred\"][index], width=NODE_WIDTH))\n",
    "        G.add_node(node_label, pos=(3, i))\n",
    "\n",
    "    # Add edges between the matched indices\n",
    "    for _, row in df.iterrows():\n",
    "        if row['result'] == 'yes':\n",
    "            G.add_edge('\\n'.join(textwrap.wrap(safety_issue_text_dict[\"extracted\"][row[\"extracted\"]], width=NODE_WIDTH)), \n",
    "                        '\\n'.join(textwrap.wrap(safety_issue_text_dict[\"inferred\"][row[\"inferred\"]], width=NODE_WIDTH)))\n",
    "\n",
    "    max_num_of_nodes_column = max(\n",
    "        len(safety_issue_text_dict['extracted']),\n",
    "        len(safety_issue_text_dict['inferred'])\n",
    "        )\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    plt.figure(figsize=(8, max_num_of_nodes_column * 2.5))  \n",
    "    nx.draw(G, pos, with_labels=False, node_color='lightblue', node_size=[len(node) * NODE_WIDTH for node in G.nodes()])\n",
    "    nx.draw_networkx_labels(G, pos, font_size=7)\n",
    "    \n",
    "    plt.xlim(-1, 4.5)  # Add buffer to the outside side edges\n",
    "    plt.ylim(-1, max_num_of_nodes_column*1)  # Add buffer to the outside top and bottom edges\n",
    "\n",
    "    # Add report ID and headers\n",
    "    plt.title(f'Report ID: {report_id}')\n",
    "    plt.text(0, max_num_of_nodes_column-0.2, 'Real', fontsize=12, ha='center')\n",
    "    plt.text(3, max_num_of_nodes_column-0.2, 'Inferred', fontsize=12, ha='center')\n",
    "    plt.text(1.5, max_num_of_nodes_column-0.5, 'Arrow indicates that real has been \"matched\" by LLM with inferred', fontsize=6, ha='center')\n",
    "\n",
    "    plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_comparison_of_reports_SI(row, with_visual_generation = True):\n",
    "    '''\n",
    "    Take a row of the DataFrame with the comparison results and interpret them by giving you a match percentage. This percentage is how 'similar' the extracted safety issues are to the inferred safety issues. However similar is hard to define so currently it is simply how many of the extracted safety issues have a match.\n",
    "    '''\n",
    "    result = row['same_comparison_results']\n",
    "\n",
    "    # Make it a DataFrame\n",
    "    result_df = pd.DataFrame(result)\n",
    "    # Split tuple into two columns\n",
    "    result_df['extracted'] = result_df['pair'].apply(lambda x: x[0])\n",
    "    result_df['inferred'] = result_df['pair'].apply(lambda x: x[1])\n",
    "\n",
    "    num_extracted_safety_issues = result_df['extracted'].unique().shape[0]\n",
    "    num_inferred_safety_issues = result_df['inferred'].unique().shape[0]\n",
    "\n",
    "    # Interpret the results\n",
    "    # Make sure that each exact safety issues is matched to only one inferred safety issue\n",
    "\n",
    "    inferred_safety_issues_matched = [False] * num_inferred_safety_issues\n",
    "    safety_issues_matched = [False] * num_extracted_safety_issues\n",
    "\n",
    "    for pair_comparison in result:\n",
    "        if pair_comparison['result'] == 'yes':\n",
    "            inferred_safety_issues_matched[pair_comparison['pair'][1]] = True\n",
    "            safety_issues_matched[pair_comparison['pair'][0]] = True\n",
    "\n",
    "    match_percent = sum(safety_issues_matched) / num_extracted_safety_issues\n",
    "\n",
    "    if with_visual_generation:\n",
    "        safety_issue_text ={\n",
    "            \"extracted\": [row['safety_issues'][index] for index in result_df['extracted'].unique()],\n",
    "            \"inferred\": [row['inferred_safety_issues'][index] for index in result_df['inferred'].unique()]\n",
    "        }\n",
    "\n",
    "        make_match_visualization(result_df, safety_issue_text, row['file'])\n",
    "        \n",
    "\n",
    "    return match_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_dataset_SI_comparison(dataset):\n",
    "    \"\"\"\n",
    "    The dataset is going to have a data on the safety issues extracted and inferred as well as the comparison results of these safety issues. This function will look at the while results and give you a summary of what is happening.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['SI_match_percent'] = dataset.apply(lambda row: interpret_comparison_of_reports_SI(row, True), axis = 1)\n",
    "    dataset['SI_match'] = dataset['SI_match_percent'] == 1\n",
    "\n",
    "    dataset['SI_count_match'] = dataset['number_extracted'] == dataset['number_inferred']\n",
    "    dataset['SI_count_difference'] = dataset['number_extracted'] - dataset['number_inferred']\n",
    "\n",
    "    print(f\"\"\"\n",
    "==============                                   ==============\n",
    "                Comparing Safety Issues results\n",
    "==============                                   ==============\n",
    "          \n",
    "There are {dataset.shape[0]} reports in the dataset.\n",
    "    \n",
    "Average match percentage: {round(dataset['SI_match_percent'].mean(), 2)}\n",
    "    This is the percentage of extracted safety issues that have been matched to inferred safety issues.\n",
    "Information on the number of safety issues extracted and inferred:\n",
    "          \"\"\")\n",
    "    \n",
    "    display(\n",
    "        pd.DataFrame([\n",
    "            ['same number of extracted and inferred safety issues', dataset['SI_count_match'].mean()],\n",
    "            ['Average match of same count of SI', dataset[dataset['SI_count_match']]['SI_match_percent'].mean()],\n",
    "            ['Average difference in count of SI', dataset['SI_count_difference'].mean()],\n",
    "            ['Average number of extracted SI', dataset['number_extracted'].mean()],\n",
    "            ['Average number of inferred SI', dataset['number_inferred'].mean()]\n",
    "        ], columns=[\"description\", \"statistic\"]).style \\\n",
    "            .format(precision=2) \\\n",
    "            .hide(axis=\"index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a dataset\n",
    "I will compare the models responses with the exact extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(OpenAICaller)\n",
    "\n",
    "# Infer safety issues\n",
    "validation_df_with_SI_Inference = extract_safety_issues_from_cleaned_reports(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare safety issues\n",
    "validation_df_with_SI_Inference = compare_safety_issues_df(validation_df_with_SI_Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19329/1421364681.py:50: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n",
      "/tmp/ipykernel_19329/1421364681.py:50: UserWarning: Glyph 61623 (\\uf0b7) missing from current font.\n",
      "  plt.savefig(os.path.join('visualizations_of_matches', f'{report_id}_comparison_visualization.png'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============                                   ==============\n",
      "                Comparing Safety Issues results\n",
      "==============                                   ==============\n",
      "          \n",
      "There are 15 reports in the dataset.\n",
      "    \n",
      "Average match percentage: 0.54\n",
      "    This is the percentage of extracted safety issues that have been matched to inferred safety issues.\n",
      "Information on the number of safety issues extracted and inferred:\n",
      "          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_04f03\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_04f03_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n",
       "      <th id=\"T_04f03_level0_col1\" class=\"col_heading level0 col1\" >statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_04f03_row0_col0\" class=\"data row0 col0\" >same number of extracted and inferred safety issues</td>\n",
       "      <td id=\"T_04f03_row0_col1\" class=\"data row0 col1\" >0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_04f03_row1_col0\" class=\"data row1 col0\" >Average match of same count of SI</td>\n",
       "      <td id=\"T_04f03_row1_col1\" class=\"data row1 col1\" >0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_04f03_row2_col0\" class=\"data row2 col0\" >Average difference in count of SI</td>\n",
       "      <td id=\"T_04f03_row2_col1\" class=\"data row2 col1\" >0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_04f03_row3_col0\" class=\"data row3 col0\" >Average number of extracted SI</td>\n",
       "      <td id=\"T_04f03_row3_col1\" class=\"data row3 col1\" >2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_04f03_row4_col0\" class=\"data row4 col0\" >Average number of inferred SI</td>\n",
       "      <td id=\"T_04f03_row4_col1\" class=\"data row4 col1\" >1.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x79e33e4fe1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Understand how well they compared\n",
    "interpret_dataset_SI_comparison(validation_df_with_SI_Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there is some interesting performance going on.\n",
    "\n",
    "We have about 0.54 percent of the exact ones have a match. However there are problems where there is about half an extra safety issue inferred for every safety issues extracted. That is only about a 1/3 of the time does it generate the same amount of safety issues. However when it does there is a 60% chance they will all match up.\n",
    "\n",
    "I need to still do 2 things before the completion of the safety issue extraction upgrade:\n",
    "\n",
    "- [ ] Make sure that the comparison of the extracted and inferred safety issues are reasonable.\n",
    "- [ ] Check whether it does a good job getting the difference between inferred and exact safety issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of safety issues reasonable\n",
    "\n",
    "I will start by just looking at the validation_df and the 15 examples there and see if I agree with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly will go through and make manual comparisons of the extracted and inferred safety issues\n",
    "\n",
    "with open('manual_comparison.txt', 'w') as file:\n",
    "    for index, row in validation_df_with_SI_Inference.iterrows():\n",
    "        file.write(f\"\"\"\n",
    "=============================================================================\n",
    "                   Report: {row['file']}\n",
    "=============================================================================\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Extracted Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Inferred Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['inferred_safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_comparison = [\n",
    "    {\n",
    "        \"file\": \"2013_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count extra\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_204\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Here the extracted safety issues have also caputred a summarty of hte safety issues at the top of the analysis. This means that there are 5 where there should only be 4 and the first one is curently a list of all of them.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_005\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The wording is different but it is similar enough. However the inferred one does not 'aviation industry practice'\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_108\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The first ones match however the inferred one is much longer and even gives a reference to a document.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_006\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        'notes': \"Completely different safety issues\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2017_203\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"The one inferred one is an exact match and I wonder if it was not removed prior to the inference\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_101\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"They are similar but not the same\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_003\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred safety issues are actually correct as there is a summary which it has read from.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_202\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"The inferred is more specific about what equipment but is more vague when it comes to defining what the problem to be mitigated was.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred are actaully exact ones lifted from the report\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2012_103\",\n",
    "        \"same\": \"yes\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Two of the safety issues are exact however it should of exracted 4 exact issues.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made me noted that actaully I should have a look at which ones are be extracted exactly or inferred. But first I need to compare this with what my comparision function says"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>safety_issues</th>\n",
       "      <th>cleaned_report</th>\n",
       "      <th>number_extracted</th>\n",
       "      <th>split</th>\n",
       "      <th>inferred_safety_issues</th>\n",
       "      <th>number_inferred</th>\n",
       "      <th>same</th>\n",
       "      <th>manual_same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2014_101</td>\n",
       "      <td>[The sighting distance available to drivers of...</td>\n",
       "      <td>﻿ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>validation</td>\n",
       "      <td>[There was  an insufficient sighting distance ...</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                      safety_issues  \\\n",
       "60  2014_101  [The sighting distance available to drivers of...   \n",
       "\n",
       "                                       cleaned_report  number_extracted  \\\n",
       "60  ﻿ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...                 2   \n",
       "\n",
       "         split                             inferred_safety_issues  \\\n",
       "60  validation  [There was  an insufficient sighting distance ...   \n",
       "\n",
       "    number_inferred same manual_same  \n",
       "60                2  yes          no  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df_with_SI_Inference['manual_same'] = validation_df_with_SI_Inference['file'].apply(lambda x: next((item['same'] for item in manual_comparison if item[\"file\"] == x), None))\n",
    "\n",
    "# Show cases where the manual comparison is different to the LLM comparison\n",
    "validation_df_with_SI_Inference[validation_df_with_SI_Inference['same'] != validation_df_with_SI_Inference['manual_same']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between exact and inferred\n",
    "\n",
    "The Model is going to be asked to read the important part of a report and extract the safety issues there will be two sorts of safety issues extracted.\n",
    "\n",
    "Firstly will be the exact safety issues. These are where the report explicitly states the safety issues. Then there will be inferred which is where no exact safety issues can be found. Then the model will \"invent\" some.\n",
    "\n",
    "This needs to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any reports that have exact safety issues that are inferred\n",
    "\n",
    "for _, row in validation_df_with_SI_Inference.iterrows():\n",
    "    \n",
    "    for issue in row['inferred_safety_issues_struct']:\n",
    "        if issue['quality'] == 'exact':\n",
    "            print(f\"Report {row.file} has an exact safety issue that is inferred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no exact safety issues from the validation which makes sense as the all the of the validation report have ha the safety issues removed.\n",
    "\n",
    "I could test this by seeing what happenings if I get a report that should have a exact safety issues in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "# Getting some examples of exact safety issues that are found using the model\n",
    "reports_path = \"output\"\n",
    "\n",
    "comparing_with_exact = validation_df.copy()\n",
    "\n",
    "comparing_with_exact['cleaned_report'] = comparing_with_exact['file'].apply(lambda x: open(os.path.join(reports_path, x, f'{x}.txt'), 'r').read())\n",
    "\n",
    "comparing_with_exact = extract_safety_issues_from_cleaned_reports(comparing_with_exact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
