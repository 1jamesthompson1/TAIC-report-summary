{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting safety issues from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem introduction\n",
    "Safety issues are being extracted from the reports.\n",
    "\n",
    "There are two types of safety issues being extracted.\n",
    "Firstly are exact extraction. These are safety issues that are explicitly mentioned in the report. They will follow something like \"safety issue: ...\" or \"safety issue - ...\". Exact safety issue extraction is not too much of a worry and just needs to be tested and could be done so with unit tests.\n",
    "\n",
    "Secondly however are inferred safety issues. These safety issues are supposedly present but are not explicitly stated. This requires a little bit more validation then a simple unit test. Fortunately I can get a good test set.\n",
    "\n",
    "All the reports that have the safety issues explicitly mentioned will show me what the correct answer is. So all I need to do is remove the explicitly mention of the safety issue from the report then I can run it as an inferred and see if it comes up with the same safety issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution story\n",
    "\n",
    "I have peformed some testing on fine-tuning gpt 3.5 turbo.\n",
    "\n",
    "It was about 56% accurate on the training set (about 15 reports)\n",
    "However on a testing set it was only 17% accurate. This was becuase about 70% of the items had mismatched number of safety issues extracted.\n",
    "\n",
    "This means that I wanted to try with more data. There are about 70 reports that I have meaning I can split it between a training, validation and test set. This is what the code does below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'engine.Extract_Analyze.ReportExtracting' from '/home/james/code/TAIC-report-summary/engine/Extract_Analyze/ReportExtracting.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "import importlib\n",
    "from engine.Extract_Analyze import ReportExtracting\n",
    "importlib.reload(ReportExtracting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  read_or_create_important_text_file(path, report_id, text):\n",
    "    important_text_path = os.path.join(path, report_id, f'{report_id}_important_text.txt')\n",
    "\n",
    "    if not os.path.isfile(important_text_path):\n",
    "        important_text, pages = ReportExtracting.ReportExtractor(text, report_id).extract_important_text()\n",
    "\n",
    "        important_text = \"\" if important_text == None else important_text\n",
    "\n",
    "        with open(important_text_path, 'w') as stream:\n",
    "            stream.write(important_text)\n",
    "\n",
    "        return important_text\n",
    "    \n",
    "    with open(important_text_path, 'r') as stream:\n",
    "        important_text = stream.read()\n",
    "        \n",
    "    important_text = None if important_text == \"\" else important_text\n",
    "\n",
    "    return important_text\n",
    "\n",
    "def clean_text_and_get_safety_issues(path, reports_ids):\n",
    "    \"\"\"\n",
    "    This will go through a folder and extract all of the safety issues that it can from the reports using regex. Then it will also remove those extracted safety issues from the report text and report a DataFrame that has the report_id, the safety issues extracted, and the cleaned report text.\n",
    "    \"\"\"\n",
    "    safety_issues_raw = []\n",
    "\n",
    "    for report in reports_ids:\n",
    "        with open(os.path.join(path, report, f'{report}.txt'), 'r') as stream:\n",
    "            text = stream.read()\n",
    "\n",
    "        # Check to see if the important text exists\n",
    "\n",
    "        important_text = read_or_create_important_text_file(path, report, text)\n",
    "\n",
    "        if important_text == None:\n",
    "            print(f'Could not extract important text from {report}')\n",
    "            continue\n",
    "        \n",
    "        safety_regex = lambda x: fr's ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s? ?{x} ?'\n",
    "        end_regex = r'([\\s\\S]+?)(?=(?:\\d+\\.(?:\\d+\\.)?(?:\\d+)?)|(?:^ [A-Z])|(?:s ?a ?f ?e ?t ?y ? ?i ?s ?s ?u ?e ?s?))'\n",
    "\n",
    "        uncompiled_regexes = [\"(\" + safety_regex(sep) + end_regex + \")\" for sep in [\"-\", \":\"]]\n",
    "\n",
    "        safety_issue_regexes = [re.compile(regex , re.MULTILINE | re.IGNORECASE) for regex in uncompiled_regexes]\n",
    "\n",
    "        safety_issues_for_report = []\n",
    "\n",
    "        cleaned_text = text\n",
    "\n",
    "        matches = [regex.findall(important_text) for regex in safety_issue_regexes]\n",
    "\n",
    "        # Choose one of the matches that has the most matches\n",
    "        matches = max(matches, key=lambda x: len(x))\n",
    "\n",
    "        for full_match, safety_issue_match in matches:\n",
    "            safety_issues_for_report.append(safety_issue_match)\n",
    "            cleaned_text = re.sub(\n",
    "                full_match, '',\n",
    "                cleaned_text,\n",
    "                flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "        safety_issues_raw.append({\n",
    "            \"file\": report,\n",
    "            \"safety_issues\": safety_issues_for_report,\n",
    "            'cleaned_report': cleaned_text})\n",
    "\n",
    "    safety_issues_df = pd.DataFrame(safety_issues_raw)\n",
    "\n",
    "    safety_issues_df['number_extracted'] = safety_issues_df['safety_issues'].apply(lambda x: len(x))\n",
    "\n",
    "    return safety_issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract important text from 2010_207\n",
      "Could not extract important text from 2010_205\n",
      "Could not extract important text from 2016_002\n",
      "Could not extract important text from 2019_003\n"
     ]
    }
   ],
   "source": [
    "reports_path = \"output\"\n",
    "\n",
    "reports = os.listdir(reports_path)\n",
    "\n",
    "reports_to_ignore = [\n",
    "    \"2020_102\", # This report has a funny safety issue that is not formatted correctly so cant be picked up by the regex\n",
    "]\n",
    "reports = [report for report in reports if report not in reports_to_ignore]\n",
    "\n",
    "all_reports_df = clean_text_and_get_safety_issues(reports_path, reports)\n",
    "\n",
    "# Remove all reports that have no found safety issues\n",
    "all_reports_df = all_reports_df[all_reports_df['number_extracted'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get split of 60% train, 20% validation, 20% test\n",
    "seed = 42\n",
    "\n",
    "train_df = all_reports_df.sample(frac=0.6, random_state=seed)\n",
    "\n",
    "validation_df = all_reports_df.drop(train_df.index).sample(frac=0.5, random_state=seed)\n",
    "\n",
    "test_df = all_reports_df.drop(train_df.index).drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data is in three distinct categories it would be prudent to make sure that the resultant data has similar distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>mode</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>validation</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>validation</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>validation</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        split mode  count\n",
       "0        test    0      4\n",
       "1        test    1      5\n",
       "2        test    2      6\n",
       "3       train    0     14\n",
       "4       train    1     17\n",
       "5       train    2     15\n",
       "6  validation    0      5\n",
       "7  validation    1      7\n",
       "8  validation    2      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the three dataframes into one and add a column to indicate the split\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "validation_df['split'] = 'validation'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "all_datasets_df = pd.concat([train_df, validation_df, test_df])\n",
    "\n",
    "all_datasets_df['mode'] = all_datasets_df['file'].apply(lambda x: x.split('_')[1][0])\n",
    "\n",
    "# Group by split and get distribution of mode in each split\n",
    "all_datasets_df.groupby(['split', 'mode']).count().reset_index()[['split', 'mode', 'file']].rename(columns={'file': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not perfect but we can see that it is pretty darn close. The validation set is low on reports. However this is not something that can be completely fixed and aught not to affect the final outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fine tuned model\n",
    "\n",
    "Because of the dismal success of just asking it I am going to give it a couple of examples and then see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions for formatting the data\n",
    "def format_safety_issue_to_yaml(safety_issue):\n",
    "    safety_issues_dict = []\n",
    "    for issue in safety_issue:\n",
    "        safety_issues_dict.append({\"safety_issue\": issue, \"quality\": \"inferred\"})\n",
    "\n",
    "    return yaml.dump(safety_issues_dict, sort_keys=False)\n",
    "\n",
    "def format_data(data):\n",
    "  formatted_data = []\n",
    "  for index, row in data.iterrows():\n",
    "    formatted_data.append({\"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "You are going help me read a transport accident investigation report.\n",
    "\n",
    " I want you to please read the report and respond with the safety issues identified in the report.\n",
    "\n",
    "Please only respond with safety issues that are quite clearly stated and/or implied.\n",
    "\n",
    "It should be noted that the number of safety issues in a report has a minimum of 1 an 0.25 quantile of 1, median of 2, 0.75 quantile of 3 and a maximum of 13. You should try make your answers match this distribution and on average a report will have only 2 safety issues.\n",
    "\n",
    "Remember the definitions give\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "• can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "• is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "{ReportExtracting.ReportExtractor(row['cleaned_report'], row['file']).extract_important_text()[0]}\n",
    "        \n",
    "=Instructions=\n",
    "\n",
    "I want to know the safety issues which this investigation has found.\n",
    "\n",
    "I also ned to know what is the quality of this safety issue. Some reports will have safety issues explicitly stated with something like \"safety issue - ...\" or \"safety issue: ...\", these are \"exact\" safety issues. Other reports however wont have these yet safety issues may still be present in this case they are \"inferred\" safety issues.\n",
    "\n",
    "Can your response please be in yaml format as shown below.\n",
    "\n",
    "- safety_issue: \"bla bla talking about this and that bla bla bla\"\n",
    "  quality: exact\n",
    "- safety_issue: \"bla bla talking about this and that bla bla bla\"\n",
    "  quality: exact\n",
    "\n",
    "\n",
    "There is no need to enclose the yaml in any tags.\n",
    "\n",
    "=Here are some definitions=\n",
    "\n",
    "Safety factor - Any (non-trivial) events or conditions, which increases safety risk. If they occurred in the future, these would\n",
    "increase the likelihood of an occurrence, and/or the\n",
    "severity of any adverse consequences associated with the\n",
    "occurrence.\n",
    "\n",
    "Safety issue - A safety factor that:\n",
    "• can reasonably be regarded as having the\n",
    "potential to adversely affect the safety of future\n",
    "operations, and\n",
    "• is characteristic of an organisation, a system, or an\n",
    "operational environment at a specific point in time.\n",
    "Safety Issues are derived from safety factors classified\n",
    "either as Risk Controls or Organisational Influences.\n",
    "\n",
    "Safety theme - Indication of recurring circumstances or causes, either across transport modes or over time. A safety theme may\n",
    "cover a single safety issue, or two or more related safety\n",
    "issues.\n",
    "\"\"\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"{format_safety_issue_to_yaml(row['safety_issues'])}\"\"\"\n",
    "        }\n",
    "    ]})\n",
    "\n",
    "  return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15]\n",
      "  Could not find text between pages 15 and 16\n",
      "  Could not extract text from page 15\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  Could not find text between pages 10 and 11\n",
      "  Could not extract text from page 10\n",
      "  Could not find text between pages 11 and 12\n",
      "  Could not extract text from page 11\n",
      "  Could not find text between pages 13 and 14\n",
      "  Could not extract text from page 13\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
      "  I am going to be reading these pages: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "  I am going to be reading these pages: [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "  I am going to be reading these pages: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11, 12]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  I am going to be reading these pages: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
      "  I am going to be reading these pages: [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "  Could not find text between pages 23 and 24\n",
      "  Could not extract text from page 23\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "  I am going to be reading these pages: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "  Could not find text between pages 25 and 26\n",
      "  Could not extract text from page 25\n",
      "  Could not find text between pages 29 and 30\n",
      "  Could not extract text from page 29\n",
      "  Could not find text between pages 31 and 32\n",
      "  Could not extract text from page 31\n",
      "  Could not find text between pages 35 and 36\n",
      "  Could not extract text from page 35\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  Could not find text between pages 13 and 14\n",
      "  Could not extract text from page 13\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "# Format and save dataset\n",
    "\n",
    "import json\n",
    "\n",
    "training_data = format_data(train_df)\n",
    "\n",
    "validation_data = format_data(validation_df)\n",
    "\n",
    "\n",
    "\n",
    "with open('training_data.jsonl', 'w') as outfile:\n",
    "    for entry in training_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "with open('validation_data.jsonl', 'w') as outfile:\n",
    "    for entry in validation_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to OpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai_file = client.files.create(\n",
    "  file=open(\"training_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "openai_file_val = client.files.create(\n",
    "  file=open(\"validation_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-RXt3Jit0q6UnJda8QRaV3LrZ', created_at=1712098644, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-imErdBW1EpSlnDOl8RluwTmE', result_files=[], status='validating_files', trained_tokens=None, training_file='file-BifhlnCFv2RiWnT4pm6AVTL3', validation_file='file-7LJJlx8HUoHGmBpLcucpK16I', user_provided_suffix=None, seed=141041653)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the fine-tune job\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=openai_file.id, \n",
    "  validation_file=openai_file_val.id,\n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this I am working on fine tuning a model so that I can accurately extract the safety issues from the report.\n",
    "\n",
    "I am getting to the point of thinking that it might not be possible to extract them 100% accurately.\n",
    "\n",
    "Instead maybe I should just focus on making a good distinction between exact and inferred.\n",
    "\n",
    "False news!\n",
    "\n",
    "False news!\n",
    "\n",
    "I had made a mistake and moved over the wrong query request to the new ft model.\n",
    "\n",
    "After checking it on the training data it is about 50% accurate which is pretty good.\n",
    "I will now check it on some new data that the training hasn't seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fine-tuned model\n",
    "\n",
    "I will now test the fine-tuned model and see how it fares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine.OpenAICaller as OpenAICaller\n",
    "from engine.OpenAICaller import openAICaller\n",
    "\n",
    "def compare_safety_issues(safety_issues, inferred_safety_issues):\n",
    "    \"\"\"\n",
    "    This will receive two lists of safety issues and then ask the LLM to compare each safety issue and see if they are the same or different.\n",
    "    \"\"\"\n",
    "\n",
    "    pairs = [(i, j) for i, _ in enumerate(safety_issues) for j, _ in enumerate(inferred_safety_issues)]\n",
    "\n",
    "    pairs_comparison_results = []\n",
    "\n",
    "    for pair in pairs:\n",
    "        response = openAICaller.query(\n",
    "            \"\"\"\n",
    "You are going to help me compare if safety issues are the same.\n",
    "\n",
    "You will be given two safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that they are the same safety issue and maybe just worded differently. Whereas different will mean that they are fundamentally different safety issues.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "Here is the first safety issues:\n",
    "{safety_issues}\n",
    "\n",
    "Here is the second safety issues:\n",
    "{inferred_safety_issues}\n",
    "    \"\"\",\n",
    "            temp =  0,\n",
    "            model = \"gpt-4\"\n",
    "        ).lower()\n",
    "\n",
    "        if response != 'yes' and response != 'no':\n",
    "            print(f\"Error response in the wrong format {response}\")\n",
    "            response = 'undetermined'\n",
    "    \n",
    "        pairs_comparison_results.append({\n",
    "            'pair': pair,\n",
    "            'result': response\n",
    "        })\n",
    "    return pairs_comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt used to compare two lists of safety themes.\n",
    "```\n",
    "You are going to help me compare if these listed safety issues are the same.\n",
    "\n",
    "You will be given two lists of safety issues that have been retrieved from the same transport investigation report in different ways.\n",
    "\n",
    "I will need to know if they are either the same or different. Same will mean that for each safety issue in the set it will match a safety issue found in the other set albeit if worded slightly differently.\n",
    "\n",
    "Your response should just be \"yes\" or \"no\".\n",
    "        \"\"\",\n",
    "        f\"\"\"\n",
    "Here is the first set of safety issues:\n",
    "{safety_issues}\n",
    "\n",
    "Here is the second set of safety issues:\n",
    "{inferred_safety_issues}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_safety_issues_df(df):\n",
    "    # Extract the safety issues from the data structured returned from the inferences extraction function.\n",
    "    df['inferred_safety_issues'] = df['inferred_safety_issues'].apply(lambda x: [issue['safety_issue'] for issue in x] if (x is not None and isinstance(x[0], dict)) else None if x is None else x)\n",
    "\n",
    "    # # Rule out matches that dont have the same number of safety issues\n",
    "    # df['same'] = df.apply(lambda x: \"no\" if x['number_extracted'] != x['number_inferred'] else 'yes', axis=1)\n",
    "\n",
    "    # # Rule out matches that dont have safety issues as all reports have safety issues\n",
    "    # df['same'] = df.apply(lambda x: \"no\" if x['inferred_safety_issues'] is None else x['same'], axis=1)\n",
    "\n",
    "    # Compare safety issues with the LLM\n",
    "    df['same_comparison_results'] = df.apply(\n",
    "        lambda x: compare_safety_issues(x['safety_issues'],\n",
    "                                        x['inferred_safety_issues']),\n",
    "        axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_safety_issues_from_cleaned_reports(df):\n",
    "    \"\"\"\n",
    "    This will call the inference function and extract the safety issues from the cleaned reports.\n",
    "    \"\"\"\n",
    "    df['inferred_safety_issues'] = df.apply(lambda x: ReportExtracting.SafetyIssueExtractor(x['cleaned_report'], x['file'])._extract_safety_issues_with_inference(), axis=1)\n",
    "\n",
    "    df['number_inferred'] = df['inferred_safety_issues'].apply(lambda x: len(x) if x is not None else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10, 11]\n",
      "  I am going to be reading these pages: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n",
      "  I am going to be reading these pages: [13, 14, 15, 16, 17, 18]\n",
      "  I am going to be reading these pages: [6, 7, 8, 9, 10]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [17, 18, 19, 20, 21, 22]\n",
      "  I am going to be reading these pages: [10, 11, 12, 13, 14, 15, 16, 17]\n",
      "  I am going to be reading these pages: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "  Found multiple matches for text between pages 27 and 28\n",
      "  Could not extract text from page 27\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12, 13, 14]\n",
      "  I am going to be reading these pages: [5, 6, 7, 8, 9]\n",
      "  I am going to be reading these pages: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "  I am going to be reading these pages: [8, 9, 10, 11, 12]\n",
      "  Could not find text between pages 8 and 9\n",
      "  Could not extract text from page 8\n",
      "  I am going to be reading these pages: [9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ReportExtracting)\n",
    "importlib.reload(OpenAICaller)\n",
    "\n",
    "validation_df_with_SI_Inference = extract_safety_issues_from_cleaned_reports(validation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df_with_SI_Inference = compare_safety_issues_df(validation_df_with_SI_Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (0, 1), 'result': 'yes'}, {'pair': (0, 2), 'result': 'yes'}, {'pair': (1, 0), 'result': 'yes'}, {'pair': (1, 1), 'result': 'yes'}, {'pair': (1, 2), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (1, 0), 'result': 'no'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (0, 1), 'result': 'no'}, {'pair': (1, 0), 'result': 'yes'}, {'pair': (1, 1), 'result': 'yes'}, {'pair': (2, 0), 'result': 'no'}, {'pair': (2, 1), 'result': 'no'}, {'pair': (3, 0), 'result': 'yes'}, {'pair': (3, 1), 'result': 'yes'}, {'pair': (4, 0), 'result': 'yes'}, {'pair': (4, 1), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (1, 0), 'result': 'no'}]\n",
      "[{'pair': (0, 0), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'yes'}, {'pair': (0, 1), 'result': 'yes'}, {'pair': (1, 0), 'result': 'yes'}, {'pair': (1, 1), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (1, 0), 'result': 'no'}]\n",
      "[{'pair': (0, 0), 'result': 'yes'}, {'pair': (1, 0), 'result': 'yes'}, {'pair': (2, 0), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'yes'}, {'pair': (0, 1), 'result': 'yes'}, {'pair': (1, 0), 'result': 'yes'}, {'pair': (1, 1), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (0, 1), 'result': 'no'}, {'pair': (0, 2), 'result': 'yes'}, {'pair': (0, 3), 'result': 'yes'}, {'pair': (1, 0), 'result': 'no'}, {'pair': (1, 1), 'result': 'no'}, {'pair': (1, 2), 'result': 'no'}, {'pair': (1, 3), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (1, 0), 'result': 'no'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}]\n",
      "[{'pair': (0, 0), 'result': 'no'}, {'pair': (0, 1), 'result': 'no'}, {'pair': (0, 2), 'result': 'no'}, {'pair': (0, 3), 'result': 'no'}, {'pair': (1, 0), 'result': 'no'}, {'pair': (1, 1), 'result': 'no'}, {'pair': (1, 2), 'result': 'no'}, {'pair': (1, 3), 'result': 'no'}, {'pair': (2, 0), 'result': 'no'}, {'pair': (2, 1), 'result': 'no'}, {'pair': (2, 2), 'result': 'no'}, {'pair': (2, 3), 'result': 'no'}]\n",
      "[{'pair': (0, 0), 'result': 'yes'}]\n",
      "[{'pair': (0, 0), 'result': 'yes'}, {'pair': (0, 1), 'result': 'no'}, {'pair': (0, 2), 'result': 'yes'}, {'pair': (1, 0), 'result': 'yes'}, {'pair': (1, 1), 'result': 'yes'}, {'pair': (1, 2), 'result': 'yes'}, {'pair': (2, 0), 'result': 'yes'}, {'pair': (2, 1), 'result': 'yes'}, {'pair': (2, 2), 'result': 'yes'}, {'pair': (3, 0), 'result': 'yes'}, {'pair': (3, 1), 'result': 'yes'}, {'pair': (3, 2), 'result': 'yes'}]\n"
     ]
    }
   ],
   "source": [
    "# Understanding comparison results\n",
    "\n",
    "# Loop throufgh each reuslt\n",
    "\n",
    "for index, row in validation_df_with_SI_Inference.iterrows():\n",
    "    result = row['same_comparison_results']\n",
    "\n",
    "    # Interpret the results\n",
    "    # Make sure that each exact safety issues is matched to only one inferred safety issue\n",
    "\n",
    "    inferred_safety_issues_matched = [False] * len(inferred_safety_issues)\n",
    "    safety_issues_matched = [False] * len(safety_issues)\n",
    "\n",
    "    for pair_comparison in pairs_comparison_results:\n",
    "        if pair_comparison['result'] == 'yes':\n",
    "            inferred_safety_issues_matched[pair_comparison['pair'][1]] = True\n",
    "            safety_issues_matched[pair_comparison['pair'][0]] = True\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ba54e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ba54e_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n",
       "      <th id=\"T_ba54e_level0_col1\" class=\"col_heading level0 col1\" >statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_ba54e_row0_col0\" class=\"data row0 col0\" >same number of extracted and inferred safety issues</td>\n",
       "      <td id=\"T_ba54e_row0_col1\" class=\"data row0 col1\" >0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ba54e_row1_col0\" class=\"data row1 col0\" >same extracted and inferred safety issues</td>\n",
       "      <td id=\"T_ba54e_row1_col1\" class=\"data row1 col1\" >0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ba54e_row2_col0\" class=\"data row2 col0\" >number of same safety issues with different inferred safety issues</td>\n",
       "      <td id=\"T_ba54e_row2_col1\" class=\"data row2 col1\" >0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x78f84e1cb880>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get percentage of rows that 'number_extracted' == 'number_inferred'\n",
    "percent_matching_SI_count = validation_df_with_SI_Inference[validation_df_with_SI_Inference['same'] == 'yes'].shape[0] / validation_df_with_SI_Inference.shape[0]\n",
    "\n",
    "# percentage of same\n",
    "percent_same = validation_df_with_SI_Inference.value_counts('same', normalize=True)['yes']\n",
    "\n",
    "pd.DataFrame([\n",
    "    ['same number of extracted and inferred safety issues', round(percent_matching_SI_count, 2)],\n",
    "    ['same extracted and inferred safety issues', round(percent_same, 2)],\n",
    "    ['number of same safety issues with different inferred safety issues', round(percent_matching_SI_count - percent_same, 2)]\n",
    "], columns=[\"description\", \"statistic\"]).style \\\n",
    "    .format(precision=2) \\\n",
    "    .hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see good enough performance on the validation set of about 33%.\n",
    "\n",
    "I need to still do 2 things before the completion of the safety issue extraction upgrade:\n",
    "\n",
    "- [ ] Make sure that the comparison of the extracted and inferred safety issues are reasonable.\n",
    "- [ ] Check whether it does a good job getting the difference between inferred and exact safety issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of safety issues reasonable\n",
    "\n",
    "I will start by just looking at the validation_df and the 15 examples there and see if I agree with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly will go through and make manual comparisons of the extracted and inferred safety issues\n",
    "\n",
    "with open('manual_comparison.txt', 'w') as file:\n",
    "    for index, row in validation_df_with_SI_Inference.iterrows():\n",
    "        file.write(f\"\"\"\n",
    "=============================================================================\n",
    "                   Report: {row['file']}\n",
    "=============================================================================\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Extracted Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")\n",
    "\n",
    "        file.write(f\"\"\"\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "        Inferred Safety Issues:\n",
    "- - - - - - - - - - - - - - - - - - -\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "        for issue in enumerate(row['inferred_safety_issues']):\n",
    "            file.write(f\"{issue[0] + 1}. {issue[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_comparison = [\n",
    "    {\n",
    "        \"file\": \"2013_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count extra\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_204\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Here the extracted safety issues have also caputred a summarty of hte safety issues at the top of the analysis. This means that there are 5 where there should only be 4 and the first one is curently a list of all of them.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_005\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The wording is different but it is similar enough. However the inferred one does not 'aviation industry practice'\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_108\",\n",
    "        \"same\": \"yes\",\n",
    "        \"notes\": \"The first ones match however the inferred one is much longer and even gives a reference to a document.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_006\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        'notes': \"Completely different safety issues\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2017_203\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"The one inferred one is an exact match and I wonder if it was not removed prior to the inference\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2014_101\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"They are similar but not the same\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_003\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred safety issues are actually correct as there is a summary which it has read from.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2013_202\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2019_007\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"different\",\n",
    "        \"notes\": \"The inferred is more specific about what equipment but is more vague when it comes to defining what the problem to be mitigated was.\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2011_106\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count more\",\n",
    "        \"notes\": \"The inferred are actaully exact ones lifted from the report\"\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2012_103\",\n",
    "        \"same\": \"yes\",\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"2016_102\",\n",
    "        \"same\": \"no\",\n",
    "        \"reason\": \"count less\",\n",
    "        \"notes\": \"Two of the safety issues are exact however it should of exracted 4 exact issues.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made me noted that actaully I should have a look at which ones are be extracted exactly or inferred. But first I need to compare this with what my comparision function says"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>safety_issues</th>\n",
       "      <th>cleaned_report</th>\n",
       "      <th>number_extracted</th>\n",
       "      <th>split</th>\n",
       "      <th>inferred_safety_issues</th>\n",
       "      <th>number_inferred</th>\n",
       "      <th>same</th>\n",
       "      <th>manual_same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2014_101</td>\n",
       "      <td>[The sighting distance available to drivers of...</td>\n",
       "      <td>﻿ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>validation</td>\n",
       "      <td>[There was  an insufficient sighting distance ...</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                      safety_issues  \\\n",
       "60  2014_101  [The sighting distance available to drivers of...   \n",
       "\n",
       "                                       cleaned_report  number_extracted  \\\n",
       "60  ﻿ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...                 2   \n",
       "\n",
       "         split                             inferred_safety_issues  \\\n",
       "60  validation  [There was  an insufficient sighting distance ...   \n",
       "\n",
       "    number_inferred same manual_same  \n",
       "60                2  yes          no  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df_with_SI_Inference['manual_same'] = validation_df_with_SI_Inference['file'].apply(lambda x: next((item['same'] for item in manual_comparison if item[\"file\"] == x), None))\n",
    "\n",
    "# Show cases where the manual comparison is different to the LLM comparison\n",
    "validation_df_with_SI_Inference[validation_df_with_SI_Inference['same'] != validation_df_with_SI_Inference['manual_same']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taic-report-engine-vkGeZcZ8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
